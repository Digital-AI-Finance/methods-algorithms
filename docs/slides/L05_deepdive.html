<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="pandoc + custom post-processor">
    <meta name="author" content="Methods and Algorithms – MSc Data Science">
    <title>L05: PCA & t-SNE</title>

    <!-- Reveal.js core -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">

    <!-- Custom ML theme -->
    <link rel="stylesheet" href="css/ml-theme.css">

    <!-- Menu plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-menu@2.1.0/menu.css">

    <!-- Chalkboard plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-plugins@latest/chalkboard/style.css">

    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        /* Additional inline styles */
        .reveal .sourceCode {
            overflow: visible;
        }
        code {
            white-space: pre-wrap;
        }
        span.smallcaps {
            font-variant: small-caps;
        }
        div.columns {
            display: flex;
            gap: min(4vw, 1.5em);
        }
        div.column {
            flex: auto;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section id="title-slide">
<h1 class="title">L05: PCA &amp; t-SNE</h1>
<p class="subtitle">Deep Dive: Theory, Implementation, and
Applications</p>
<p class="author">Methods and Algorithms – MSc Data Science</p>
</section>

<section>
<h2>Part 1: PCA Foundations</h2>

<p> <strong>Principal Component
Analysis (PCA)</strong></p>
<ul>
<li class="fragment"><p>Find orthogonal directions of maximum variance</p></li>
<li class="fragment"><p>Project data onto these directions</p></li>
<li class="fragment"><p>Reduce dimensions while preserving information</p></li>
</ul>
<p><strong>Key Properties:</strong></p>
<ul>
<li class="fragment"><p>Linear transformation</p></li>
<li class="fragment"><p>Components are uncorrelated</p></li>
<li class="fragment"><p>Reversible (can reconstruct original data)</p></li>
</ul>
<p><span style="color: gray">PCA: one of the most fundamental tools in
data science</span></p>

</section>

<section>
<h2>Mathematical Foundation</h2>

<p> <strong>Covariance
Matrix:</strong> <span class="math display">\[\Sigma = \frac{1}{n-1} X^T
X \quad \text{(centered data)}\]</span></p>
<p><strong>Eigendecomposition:</strong> <span class="math display">\[\Sigma v = \lambda v\]</span> where <span class="math inline">\(v\)</span> = eigenvector (principal direction),
<span class="math inline">\(\lambda\)</span> = eigenvalue (variance)</p>
<p><strong>Projection:</strong> <span class="math display">\[Z = X W_k
\quad \text{where } W_k = [v_1, v_2, \ldots, v_k]\]</span> <span style="color: gray">Eigenvalues tell us how much variance each component
captures</span></p>

</section>

<section>
<h2>Variance Explained</h2>

<p> <strong>Proportion of
Variance:</strong> <span class="math display">\[\text{Explained Variance
Ratio}_i = \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}\]</span></p>
<p><strong>Cumulative Variance:</strong> <span class="math display">\[\text{Cumulative}_k = \sum_{i=1}^{k}
\frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}\]</span> <strong>Rules of
thumb for choosing k:</strong></p>
<ul>
<li class="fragment"><p>Keep 80-95% of total variance</p></li>
<li class="fragment"><p>Use scree plot “elbow” method</p></li>
<li class="fragment"><p>Kaiser criterion: keep components with <span class="math inline">\(\lambda &gt; 1\)</span></p></li>
</ul>
<p><span style="color: gray">Balance dimensionality reduction with
information preservation</span></p>

</section>

<section>
<h2>Scree Plot</h2>


<div class="center">
<p><embed data-src="01_scree_plot/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">Look for the “elbow” where variance
explained drops off</span></p>

</section>

<section>
<h2>Principal Components Visualization</h2>


<div class="center">
<p><embed data-src="02_principal_components/chart.pdf" style="width:50.0%"/></p>
</div>
<p><span style="color: gray">PC1 captures the dominant trend, PC2 the
residual variation</span></p>

</section>

<section>
<h2>Reconstruction</h2>

<p> <strong>From k components back to
original space:</strong> <span class="math display">\[\hat{X} = Z W_k^T
= X W_k W_k^T\]</span></p>
<p><strong>Reconstruction Error:</strong> <span class="math display">\[\text{Error} = ||X - \hat{X}||_F^2 =
\sum_{i=k+1}^{p} \lambda_i\]</span> <span style="color: gray">Reconstruction error = sum of discarded
eigenvalues</span></p>

</section>

<section>
<h2>Reconstruction Error vs Components</h2>


<div class="center">
<p><embed data-src="03_reconstruction/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">Adding more components always reduces error
(but diminishing returns)</span></p>

</section>

<section>
<h2>Part 2: PCA in Finance</h2>

<p> <strong>Portfolio Risk
Decomposition:</strong></p>
<ul>
<li class="fragment"><p>PC1 often represents “market factor”</p></li>
<li class="fragment"><p>PC2-3 may capture sector/size factors</p></li>
<li class="fragment"><p>Higher PCs: idiosyncratic risk</p></li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li class="fragment"><p>Risk factor modeling</p></li>
<li class="fragment"><p>Dimensionality reduction for trading signals</p></li>
<li class="fragment"><p>Noise reduction in time series</p></li>
<li class="fragment"><p>Feature extraction for ML models</p></li>
</ul>
<p><span style="color: gray">PCA reveals latent structure in financial
data</span></p>

</section>

<section>
<h2>PCA Limitations</h2>

<p> <strong>When PCA Falls
Short:</strong></p>
<ul>
<li class="fragment"><p>Non-linear relationships (curved manifolds)</p></li>
<li class="fragment"><p>Cluster structure not aligned with variance</p></li>
<li class="fragment"><p>Discrete or categorical data</p></li>
<li class="fragment"><p>Outliers heavily influence results</p></li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li class="fragment"><p>Kernel PCA (non-linear)</p></li>
<li class="fragment"><p>Robust PCA (outlier-resistant)</p></li>
<li class="fragment"><p>t-SNE/UMAP (for visualization)</p></li>
</ul>
<p><span style="color: gray">PCA assumes linear structure and
Gaussian-like distributions</span></p>

</section>

<section>
<h2>Part 3: t-SNE Introduction</h2>

<p> <strong>t-Distributed
Stochastic Neighbor Embedding</strong></p>
<ul>
<li class="fragment"><p>Non-linear dimensionality reduction</p></li>
<li class="fragment"><p>Optimized for visualization (2D/3D)</p></li>
<li class="fragment"><p>Preserves local neighborhood structure</p></li>
</ul>
<p><strong>Key Idea:</strong></p>
<ul>
<li class="fragment"><p>Convert distances to probabilities</p></li>
<li class="fragment"><p>In high-D: Gaussian similarities</p></li>
<li class="fragment"><p>In low-D: t-distribution similarities</p></li>
<li class="fragment"><p>Minimize KL divergence between distributions</p></li>
</ul>
<p><span style="color: gray">t-SNE: visualization method, NOT for
preprocessing</span></p>

</section>

<section>
<h2>t-SNE: Mathematical Formulation</h2>

<p> <strong>High-dimensional
similarity:</strong> <span class="math display">\[p_{j|i} =
\frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i
- x_k||^2 / 2\sigma_i^2)}\]</span></p>
<p><strong>Low-dimensional similarity (t-distribution):</strong> <span class="math display">\[q_{ij} = \frac{(1 + ||y_i -
y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}\]</span></p>
<p><strong>Objective: Minimize KL divergence</strong> <span class="math display">\[KL(P||Q) = \sum_{i \neq j} p_{ij} \log
\frac{p_{ij}}{q_{ij}}\]</span> <span style="color: gray">t-distribution
has heavier tails, allowing better separation in low-D</span></p>

</section>

<section>
<h2>Perplexity Parameter</h2>


<div class="center">
<p><embed data-src="04_tsne_perplexity/chart.pdf" style="width:65.0%"/></p>
</div>
<p><span style="color: gray">Perplexity <span class="math inline">\(\approx\)</span> effective number of neighbors
(try 5-50)</span></p>

</section>

<section>
<h2>Perplexity Guidelines</h2>

<p> <strong>Perplexity</strong>
controls the balance between local and global structure:</p>
<ul>
<li class="fragment"><p>Low perplexity (5-10): Focus on very local structure</p></li>
<li class="fragment"><p>Medium perplexity (30-50): Balanced (default)</p></li>
<li class="fragment"><p>High perplexity (100+): More global structure</p></li>
</ul>
<p><strong>Guidelines:</strong></p>
<ul>
<li class="fragment"><p>Should be smaller than number of points</p></li>
<li class="fragment"><p>Larger datasets can use higher perplexity</p></li>
<li class="fragment"><p>Run multiple perplexities to validate findings</p></li>
</ul>
<p><span style="color: gray">Results can vary significantly with
perplexity choice</span></p>

</section>

<section>
<h2>t-SNE Caveats</h2>

<p> <strong>Important
Limitations:</strong></p>
<ul>
<li class="fragment"><p>Non-deterministic (run multiple times)</p></li>
<li class="fragment"><p>Cluster sizes are not meaningful</p></li>
<li class="fragment"><p>Distances between clusters are not meaningful</p></li>
<li class="fragment"><p>Slow for large datasets (O(<span class="math inline">\(n^2\)</span>))</p></li>
</ul>
<p><strong>Best Practices:</strong></p>
<ul>
<li class="fragment"><p>Use PCA first to reduce to 30-50 dims</p></li>
<li class="fragment"><p>Run multiple times with different seeds</p></li>
<li class="fragment"><p>Don’t over-interpret cluster sizes/distances</p></li>
<li class="fragment"><p>Use for exploration, not final conclusions</p></li>
</ul>
<p><span style="color: gray">t-SNE shows IF clusters exist, not HOW they
relate</span></p>

</section>

<section>
<h2>Part 4: PCA vs t-SNE</h2>


<div class="center">
<p><embed data-src="05_pca_vs_tsne/chart.pdf" style="width:65.0%"/></p>
</div>
<p><span style="color: gray">PCA: global structure, linear. t-SNE: local
structure, non-linear</span></p>

</section>

<section>
<h2>Comparison Table</h2>


<div class="center">
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: center;"><strong>PCA</strong></th>
<th style="text-align: center;"><strong>t-SNE</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Type</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">Non-linear</td>
</tr>
<tr>
<td style="text-align: left;">Speed</td>
<td style="text-align: center;">Fast <span class="math inline">\(O(np^2)\)</span></td>
<td style="text-align: center;">Slow <span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Deterministic</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">Preserves</td>
<td style="text-align: center;">Global variance</td>
<td style="text-align: center;">Local neighbors</td>
</tr>
<tr>
<td style="text-align: left;">Reversible</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">Use for ML</td>
<td style="text-align: center;">Yes (preprocessing)</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">Visualization</td>
<td style="text-align: center;">Okay</td>
<td style="text-align: center;">Excellent</td>
</tr>
</tbody>
</table>
</div>
<p><span style="color: gray">Use PCA for preprocessing, t-SNE for
visualization only</span></p>

</section>

<section>
<h2>Cluster Preservation</h2>


<div class="center">
<p><embed data-src="06_cluster_preservation/chart.pdf" style="width:65.0%"/></p>
</div>
<p><span style="color: gray">t-SNE often reveals cluster structure
better than PCA</span></p>

</section>

<section>
<h2>When to Use Which</h2>

<p> <strong>Use PCA When:</strong></p>
<ul>
<li class="fragment"><p>Preprocessing for ML (reduce features)</p></li>
<li class="fragment"><p>Linear relationships expected</p></li>
<li class="fragment"><p>Need reversibility (reconstruction)</p></li>
<li class="fragment"><p>Speed matters</p></li>
</ul>
<p><strong>Use t-SNE When:</strong></p>
<ul>
<li class="fragment"><p>Visualizing high-dimensional data</p></li>
<li class="fragment"><p>Looking for cluster structure</p></li>
<li class="fragment"><p>Non-linear manifolds expected</p></li>
<li class="fragment"><p>Exploratory analysis</p></li>
</ul>
<p><span style="color: gray">Often use both: PCA first to 30-50 dims,
then t-SNE for visualization</span></p>

</section>

<section>
<h2>Decision Framework</h2>


<div class="center">
<p><embed data-src="07_decision_flowchart/chart.pdf" style="width:50.0%"/></p>
</div>
<p><span style="color: gray">Consider purpose: preprocessing (PCA) vs
visualization (t-SNE)</span></p>

</section>

<section>
<h2>Part 5: Implementation</h2>

<p> <strong>PCA in
scikit-learn:</strong></p>
<ul>
<li class="fragment"><p><code>PCA(n_components=k)</code>: Keep k components</p></li>
<li class="fragment"><p><code>PCA(n_components=0.95)</code>: Keep 95% variance</p></li>
<li class="fragment"><p><code>pca.explained_variance_ratio_</code>: Variance per
component</p></li>
<li class="fragment"><p><code>pca.inverse_transform()</code>: Reconstruct
original</p></li>
</ul>
<p><strong>t-SNE in scikit-learn:</strong></p>
<ul>
<li class="fragment"><p><code>TSNE(n_components=2, perplexity=30)</code></p></li>
<li class="fragment"><p>Always normalize data first</p></li>
<li class="fragment"><p>Consider PCA preprocessing for speed</p></li>
</ul>
<p><span style="color: gray">Standardize data before PCA; normalize
before t-SNE</span></p>

</section>

<section>
<h2>UMAP: Modern Alternative</h2>

<p> <strong>Uniform Manifold
Approximation and Projection</strong></p>
<ul>
<li class="fragment"><p>Faster than t-SNE</p></li>
<li class="fragment"><p>Better preserves global structure</p></li>
<li class="fragment"><p>Can embed new points (unlike t-SNE)</p></li>
<li class="fragment"><p>Hyperparameters: n_neighbors, min_dist</p></li>
</ul>
<p><strong>When to use UMAP:</strong></p>
<ul>
<li class="fragment"><p>Large datasets (faster than t-SNE)</p></li>
<li class="fragment"><p>Need to embed new data points</p></li>
<li class="fragment"><p>Want more preserved global structure</p></li>
</ul>
<p><span style="color: gray">UMAP often preferred over t-SNE in modern
practice</span></p>

</section>

<section>
<h2>Summary</h2>

<p> <strong>PCA:</strong></p>
<ul>
<li class="fragment"><p>Linear, fast, reversible</p></li>
<li class="fragment"><p>Use for preprocessing and feature extraction</p></li>
<li class="fragment"><p>Choose k by variance explained or elbow</p></li>
</ul>
<p><strong>t-SNE:</strong></p>
<ul>
<li class="fragment"><p>Non-linear, slow, visualization-only</p></li>
<li class="fragment"><p>Excellent for exploring cluster structure</p></li>
<li class="fragment"><p>Don’t interpret distances or sizes literally</p></li>
</ul>
<p><strong>Common Pipeline:</strong> Standardize <span class="math inline">\(\rightarrow\)</span> PCA (30-50) <span class="math inline">\(\rightarrow\)</span> t-SNE (2D) <span style="color: gray">Next: Embeddings and Reinforcement
Learning</span></p>

</section>

<section>
<h2>References</h2>

<p> <strong>Textbooks:</strong></p>
<ul>
<li class="fragment"><p>James et al. (2021). <em>ISLR</em>, Chapter 12: Unsupervised
Learning</p></li>
<li class="fragment"><p>Hastie et al. (2009). <em>ESL</em>, Chapter 14: Unsupervised
Learning</p></li>
</ul>
<p><strong>Original Papers:</strong></p>
<ul>
<li class="fragment"><p>Pearson (1901). On Lines and Planes of Closest Fit</p></li>
<li class="fragment"><p>van der Maaten &amp; Hinton (2008). Visualizing Data using
t-SNE</p></li>
<li class="fragment"><p>McInnes et al. (2018). UMAP</p></li>
</ul>
<p><strong>Documentation:</strong></p>
<ul>
<li class="fragment"><p>scikit-learn: <code>sklearn.decomposition.PCA</code></p></li>
<li class="fragment"><p>scikit-learn: <code>sklearn.manifold.TSNE</code></p></li>
</ul>
<p><span style="color: gray">t-SNE paper: one of the most influential
visualization papers</span></p>

</section>
        </div>

        <!-- Custom footer -->
        <div class="slide-footer">
            <div class="footer-left">Methods and Algorithms</div>
            <div class="footer-center">MSc Data Science</div>
            <div class="footer-right"></div>
        </div>
    </div>

    <!-- Reveal.js core -->
    <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

    <!-- Reveal.js plugins -->
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>

    <!-- Menu plugin -->
    <script src="https://unpkg.com/reveal.js-menu@2.1.0/menu.js"></script>

    <!-- Chalkboard plugin -->
    <script src="https://unpkg.com/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

    <script>
        Reveal.initialize({
            // Display settings
            controls: true,
            controlsTutorial: true,
            controlsLayout: 'bottom-right',
            progress: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            hash: true,
            history: true,

            // Navigation
            keyboard: true,
            overview: true,
            touch: true,
            loop: false,
            navigationMode: 'default',

            // Transitions
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',

            // Sizing (16:9 aspect ratio)
            width: 1600,
            height: 900,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            center: false,

            // Fragments
            fragments: true,
            fragmentInURL: false,

            // Auto-slide (disabled)
            autoSlide: 0,

            // Math configuration
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                tex: {
                    inlineMath: [['\\(', '\\)']],
                    displayMath: [['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            },

            // Menu plugin configuration
            menu: {
                side: 'left',
                width: 'normal',
                numbers: true,
                titleSelector: 'h1, h2',
                useTextContentForMissingTitles: true,
                hideMissingTitles: false,
                markers: true,
                custom: false,
                themes: [
                    { name: 'ML Theme (Light)', theme: 'css/ml-theme.css' },
                    { name: 'Black', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/black.css' },
                    { name: 'White', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css' }
                ],
                transitions: true,
                openButton: true,
                openSlideNumber: false,
                keyboard: true,
                sticky: false,
                autoOpen: true
            },

            // Chalkboard plugin configuration
            chalkboard: {
                boardmarkerWidth: 3,
                chalkWidth: 4,
                chalkEffect: 0.5,
                storage: null,
                src: null,
                readOnly: false,
                toggleChalkboardButton: { left: "30px", bottom: "30px", top: "auto", right: "auto" },
                toggleNotesButton: { left: "70px", bottom: "30px", top: "auto", right: "auto" },
                colorButtons: true,
                boardHandle: true,
                transition: 800,
                theme: "chalkboard"
            },

            // Plugins to load
            plugins: [
                RevealMath,
                RevealNotes,
                RevealSearch,
                RevealZoom,
                RevealHighlight,
                RevealMenu,
                RevealChalkboard
            ]
        });

        // Update footer with slide number
        Reveal.on('slidechanged', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const current = event.indexh + 1;
                const total = Reveal.getTotalSlides();
                footer.textContent = `${current} / ${total}`;
            }
        });

        // Initial footer update
        Reveal.on('ready', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const total = Reveal.getTotalSlides();
                footer.textContent = `1 / ${total}`;
            }
        });

        // Keyboard shortcuts info
        console.log('Keyboard shortcuts:');
        console.log('  S - Speaker notes');
        console.log('  B - Chalkboard');
        console.log('  C - Canvas (draw on slide)');
        console.log('  M - Menu');
        console.log('  O - Overview');
        console.log('  F - Fullscreen');
        console.log('  ? - Help');
    </script>
</body>
</html>
