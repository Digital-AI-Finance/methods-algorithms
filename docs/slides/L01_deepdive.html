<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="pandoc + custom post-processor">
    <meta name="author" content="Methods and Algorithms">
    <title>Introduction & Linear Regression</title>

    <!-- Reveal.js core -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">

    <!-- Custom ML theme -->
    <link rel="stylesheet" href="css/ml-theme.css">

    <!-- Menu plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-menu@2.1.0/menu.css">

    <!-- Chalkboard plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-plugins@latest/chalkboard/style.css">

    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;600;700&display=swap" rel="stylesheet">

    <style>
        /* Additional inline styles */
        .reveal .sourceCode {
            overflow: visible;
        }
        code {
            white-space: pre-wrap;
        }
        span.smallcaps {
            font-variant: small-caps;
        }
        div.columns {
            display: flex;
            gap: min(4vw, 1.5em);
        }
        div.column {
            flex: auto;
            overflow-x: auto;
        }

        /* Spotlight cursor */
        .spotlight-cursor {
            position: fixed;
            pointer-events: none;
            z-index: 9999;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(255,0,0,0.3) 0%, transparent 70%);
            transform: translate(-50%, -50%);
        }

        /* PDF export button */
        .pdf-export-btn {
            position: fixed;
            bottom: 20px;
            right: 20px;
            z-index: 100;
            background: var(--ml-purple, #3333B2);
            color: white;
            border: none;
            padding: 10px 15px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            display: flex;
            align-items: center;
            gap: 8px;
            opacity: 0.8;
            transition: opacity 0.2s;
        }
        .pdf-export-btn:hover {
            opacity: 1;
        }

        /* Hide export button in print mode */
        @media print {
            .pdf-export-btn {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section id="title-slide" data-background="linear-gradient(135deg, #3333B2 0%, #0066CC 100%)">
    <div style="color: white; text-align: center;">
        <h1 style="color: white; font-size: 2.5em; margin-bottom: 0.3em; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);">Introduction & Linear Regression</h1>
        <p style="font-size: 1.4em; color: #ADADE0; margin-bottom: 1.5em;">Deep Dive: Mathematics and Implementation</p>
        <p style="font-size: 1em; color: rgba(255,255,255,0.9);">Methods and Algorithms</p>
        <div style="margin-top: 2em;">
            <img src="images/qr_wiki.png" alt="QR Code" style="width: 120px; height: 120px; background: white; padding: 8px; border-radius: 8px;" onerror="this.style.display='none'">
            <p style="font-size: 0.7em; color: rgba(255,255,255,0.7); margin-top: 0.5em;">Scan for course materials</p>
        </div>
    </div>
</section>

<section>
<h2>Outline</h2>



</section>

<section>
<h2>Matrix Notation</h2>

<p> <strong>The Model in Matrix
Form</strong></p>
<p><span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\]</span></p>
<ul>
<li class="fragment"><p><span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>:
Response vector</p></li>
<li class="fragment"><p><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times
(p+1)}\)</span>: Design matrix (with intercept column)</p></li>
<li class="fragment"><p><span class="math inline">\(\boldsymbol{\beta} \in
\mathbb{R}^{p+1}\)</span>: Coefficient vector</p></li>
<li class="fragment"><p><span class="math inline">\(\boldsymbol{\varepsilon} \in
\mathbb{R}^n\)</span>: Error vector</p></li>
</ul>
<p><em>Matrix notation enables elegant derivations and efficient
computation</em></p>

</section>

<section>
<h2>Design Matrix Structure</h2>

<p> <strong>The Design Matrix <span class="math inline">\(\mathbf{X}\)</span></strong></p>
<p><span class="math display">\[\mathbf{X} = \begin{bmatrix}
      1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
      1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
      1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
    \end{bmatrix}\]</span></p>
<ul>
<li class="fragment"><p>First column of 1s for intercept <span class="math inline">\(\beta_0\)</span></p></li>
<li class="fragment"><p>Each row is one observation</p></li>
<li class="fragment"><p>Each column (after first) is one feature</p></li>
</ul>
<p><em><span class="math inline">\(n\)</span> observations, <span class="math inline">\(p\)</span> features, <span class="math inline">\(p+1\)</span> parameters</em></p>

</section>

<section>
<h2>OLS Assumptions</h2>

<p> <strong>Classical Assumptions for Valid
Inference</strong></p>
<ol>
<li class="fragment"><p><strong>Linearity</strong>: <span class="math inline">\(E[y|X] =
X\beta\)</span> (correct functional form)</p></li>
<li class="fragment"><p><strong>Exogeneity</strong>: <span class="math inline">\(E[\varepsilon|X] = 0\)</span> (no omitted
variables)</p></li>
<li class="fragment"><p><strong>Homoscedasticity</strong>: <span class="math inline">\(\text{Var}(\varepsilon|X) = \sigma^2 I\)</span>
(constant variance)</p></li>
<li class="fragment"><p><strong>No multicollinearity</strong>: <span class="math inline">\(\text{rank}(X) = p+1\)</span> (full rank)</p></li>
<li class="fragment"><p><strong>Normality</strong> (for inference): <span class="math inline">\(\varepsilon \sim N(0, \sigma^2
I)\)</span></p></li>
</ol>
<p><strong>Violations?</strong> Robust standard errors, transformations,
regularization</p>
<p><em>Assumptions 1-4 needed for unbiased estimates; 5 for t-tests and
CIs</em></p>

</section>

<section>
<h2>The Loss Function</h2>

<p> <strong>Sum of Squared Residuals
(SSR)</strong></p>
<p><span class="math display">\[L(\boldsymbol{\beta}) = \sum_{i=1}^{n}
(y_i - \hat{y}_i)^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\]</span></p>
<p><strong>Expanding:</strong> <span class="math display">\[L(\boldsymbol{\beta}) = \mathbf{y}^\top\mathbf{y}
- 2\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} +
\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p><em>Quadratic function in <span class="math inline">\(\boldsymbol{\beta}\)</span> – has unique minimum
(if <span class="math inline">\(X\)</span> full rank)</em></p>

</section>

<section>
<h2>Deriving the Normal Equation</h2>

<p> <strong>Taking the
Derivative</strong></p>
<p><span class="math display">\[\frac{\partial L}{\partial
\boldsymbol{\beta}} = -2\mathbf{X}^\top\mathbf{y} +
2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p><strong>Setting to Zero:</strong> <span class="math display">\[\begin{aligned}
    \mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} &amp;=
\mathbf{X}^\top\mathbf{y} \\
    \hat{\boldsymbol{\beta}} &amp;=
(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}
  
\end{aligned}\]</span></p>
<p><em>This is the closed-form OLS solution – the “normal
equation”</em></p>

</section>

<section>
<h2>Simple Regression Visualization</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/01_simple_regression.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>The fitted line minimizes vertical distances squared</em></p>

</section>

<section>
<h2>Multiple Regression Surface</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/02_multiple_regression_3d.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>With 2 features, we fit a plane; with <span class="math inline">\(p\)</span> features, a hyperplane</em></p>

</section>

<section>
<h2>Why Gradient Descent?</h2>

<p> <strong>Normal Equation
Limitations</strong></p>
<ul>
<li class="fragment"><p>Computing <span class="math inline">\((\mathbf{X}^\top\mathbf{X})^{-1}\)</span> is <span class="math inline">\(O(p^3)\)</span></p></li>
<li class="fragment"><p>Memory: Need to store <span class="math inline">\(p \times
p\)</span> matrix</p></li>
<li class="fragment"><p>For large <span class="math inline">\(p\)</span> (millions of
features): infeasible</p></li>
</ul>
<p><strong>Gradient Descent Advantages</strong></p>
<ul>
<li class="fragment"><p>Memory efficient: process one sample at a time</p></li>
<li class="fragment"><p>Scales to big data (SGD)</p></li>
<li class="fragment"><p>Generalizes to non-linear models</p></li>
</ul>
<p><em>For <span class="math inline">\(p &gt; 10{,}000\)</span>,
gradient descent usually faster</em></p>

</section>

<section>
<h2>The Gradient</h2>

<p> <strong>Gradient of the Loss
Function</strong></p>
<p><span class="math display">\[\nabla L(\boldsymbol{\beta}) =
-2\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) =
-2\mathbf{X}^\top\mathbf{r}\]</span></p>
<p>where <span class="math inline">\(\mathbf{r} = \mathbf{y} -
\mathbf{X}\boldsymbol{\beta}\)</span> is the residual vector.</p>
<p><strong>Intuition:</strong></p>
<ul>
<li class="fragment"><p>Gradient points in direction of steepest ascent</p></li>
<li class="fragment"><p>We move opposite to gradient (steepest descent)</p></li>
<li class="fragment"><p>Scale by learning rate <span class="math inline">\(\alpha\)</span></p></li>
</ul>
<p><em>Gradient is a <span class="math inline">\(p+1\)</span>
dimensional vector</em></p>

</section>

<section>
<h2>Gradient Descent Algorithm</h2>

<p> <strong>Update
Rule</strong></p>
<p><span class="math display">\[\boldsymbol{\beta}^{(t+1)} =
\boldsymbol{\beta}^{(t)} - \alpha \nabla
L(\boldsymbol{\beta}^{(t)})\]</span></p>
<p><strong>Algorithm:</strong></p>
<ol>
<li class="fragment"><p>Initialize <span class="math inline">\(\boldsymbol{\beta}^{(0)}\)</span> (often zeros or
random)</p></li>
<li class="fragment"><p>Compute gradient <span class="math inline">\(\nabla
L(\boldsymbol{\beta}^{(t)})\)</span></p></li>
<li class="fragment"><p>Update: <span class="math inline">\(\boldsymbol{\beta}^{(t+1)} =
\boldsymbol{\beta}^{(t)} - \alpha \nabla L\)</span></p></li>
<li class="fragment"><p>Repeat until convergence</p></li>
</ol>
<p><em>Convergence: <span class="math inline">\(\|\boldsymbol{\beta}^{(t+1)} -
\boldsymbol{\beta}^{(t)}\| &lt; \epsilon\)</span> or max
iterations</em></p>

</section>

<section>
<h2>Gradient Descent Visualization</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/04_gradient_descent.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>Contours show loss surface; path shows optimization
trajectory</em></p>

</section>

<section>
<h2>Learning Rate Selection</h2>

<p> <strong>The Critical
Hyperparameter</strong></p>
<ul>
<li class="fragment"><p><strong>Too small</strong>: Slow convergence, many
iterations</p></li>
<li class="fragment"><p><strong>Too large</strong>: Divergence, oscillation</p></li>
<li class="fragment"><p><strong>Just right</strong>: Fast, stable convergence</p></li>
</ul>
<p><strong>Practical Approaches:</strong></p>
<ul>
<li class="fragment"><p>Start with <span class="math inline">\(\alpha = 0.01\)</span> or
<span class="math inline">\(0.001\)</span></p></li>
<li class="fragment"><p>Learning rate schedules (decay over time)</p></li>
<li class="fragment"><p>Adaptive methods: Adam, AdaGrad, RMSprop</p></li>
</ul>
<p><em>For OLS, optimal <span class="math inline">\(\alpha =
1/\lambda_{\max}(X^\top X)\)</span></em></p>

</section>

<section>
<h2>Stochastic Gradient Descent (SGD)</h2>

<p> <strong>Mini-Batch
Gradient Descent</strong></p>
<p>Instead of full gradient: <span class="math display">\[\nabla
L(\boldsymbol{\beta}) =
-\frac{2}{n}\mathbf{X}^\top\mathbf{r}\]</span></p>
<p>Use mini-batch of size <span class="math inline">\(m\)</span>: <span class="math display">\[\nabla L_B(\boldsymbol{\beta}) =
-\frac{2}{m}\mathbf{X}_B^\top\mathbf{r}_B\]</span></p>
<ul>
<li class="fragment"><p><span class="math inline">\(m = 1\)</span>: Stochastic GD (noisy
but fast)</p></li>
<li class="fragment"><p><span class="math inline">\(m = n\)</span>: Batch GD (stable but
slow)</p></li>
<li class="fragment"><p><span class="math inline">\(m \in [32, 256]\)</span>: Mini-batch
(good tradeoff)</p></li>
</ul>
<p><em>SGD: Process data once per epoch, update many times</em></p>

</section>

<section>
<h2>R-Squared (\(R^2\))</h2>

<p>
<strong>Coefficient of Determination</strong></p>
<p><span class="math display">\[R^2 = 1 -
\frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} = 1 -
\frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li class="fragment"><p>Proportion of variance explained by model</p></li>
<li class="fragment"><p><span class="math inline">\(R^2 = 0\)</span>: Model no better
than mean</p></li>
<li class="fragment"><p><span class="math inline">\(R^2 = 1\)</span>: Perfect
fit</p></li>
<li class="fragment"><p><span class="math inline">\(R^2 = 0.7\)</span>: 70% of variance
explained</p></li>
</ul>
<p><em><span class="math inline">\(R^2\)</span> always increases with
more features – use Adjusted <span class="math inline">\(R^2\)</span></em></p>

</section>

<section>
<h2>Adjusted R-Squared</h2>

<p> <strong>Penalizing Model
Complexity</strong></p>
<p><span class="math display">\[R^2_{\text{adj}} = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span></p>
<p><strong>Properties:</strong></p>
<ul>
<li class="fragment"><p>Adjusts for number of predictors <span class="math inline">\(p\)</span></p></li>
<li class="fragment"><p>Can decrease when adding irrelevant features</p></li>
<li class="fragment"><p>Better for model comparison</p></li>
</ul>
<p><em>Use <span class="math inline">\(R^2_{\text{adj}}\)</span> when
comparing models with different <span class="math inline">\(p\)</span></em></p>

</section>

<section>
<h2>RMSE and MAE</h2>

<p> <strong>Error Metrics in Original
Units</strong></p>
<p><span class="math display">\[\begin{aligned}
    \text{RMSE} &amp;= \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2} \\
    \text{MAE} &amp;= \frac{1}{n}\sum|y_i - \hat{y}_i|
  
\end{aligned}\]</span></p>
<p><strong>Comparison:</strong></p>
<ul>
<li class="fragment"><p>RMSE: Penalizes large errors more (sensitive to
outliers)</p></li>
<li class="fragment"><p>MAE: More robust, easier to interpret</p></li>
<li class="fragment"><p>Units: Same as target variable (e.g., dollars)</p></li>
</ul>
<p><em>Report both for comprehensive evaluation</em></p>

</section>

<section>
<h2>Residual Analysis</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/03_residual_plots.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>Good: random scatter. Bad: patterns indicate model
misspecification</em></p>

</section>

<section>
<h2>Train-Test Split</h2>

<p> <strong>Evaluating
Generalization</strong></p>
<ul>
<li class="fragment"><p>Never evaluate on training data alone</p></li>
<li class="fragment"><p>Split: 70-80% train, 20-30% test</p></li>
<li class="fragment"><p>Report test set metrics</p></li>
</ul>
<p><strong>Cross-Validation (K-Fold):</strong></p>
<ul>
<li class="fragment"><p>Split into <span class="math inline">\(K\)</span> folds
(typically <span class="math inline">\(K=5\)</span> or <span class="math inline">\(10\)</span>)</p></li>
<li class="fragment"><p>Train on <span class="math inline">\(K-1\)</span> folds, validate
on 1</p></li>
<li class="fragment"><p>Repeat <span class="math inline">\(K\)</span> times, average
results</p></li>
</ul>
<p><em>CV gives more reliable estimate with limited data</em></p>

</section>

<section>
<h2>Learning Curves</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/05_learning_curves.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>Gap between curves indicates overfitting; convergence shows
saturation</em></p>

</section>

<section>
<h2>The Overfitting Problem</h2>

<p> <strong>When Models Memorize
Instead of Learn</strong></p>
<ul>
<li class="fragment"><p>High-dimensional data (<span class="math inline">\(p \approx
n\)</span> or <span class="math inline">\(p &gt; n\)</span>)</p></li>
<li class="fragment"><p>Coefficients become very large</p></li>
<li class="fragment"><p>Perfect fit on training data, poor generalization</p></li>
</ul>
<p><strong>Solution: Add Penalty to Loss Function</strong> <span class="math display">\[L_{\text{reg}}(\boldsymbol{\beta}) = \|\mathbf{y}
- \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \cdot
\text{penalty}(\boldsymbol{\beta})\]</span></p>
<p><em><span class="math inline">\(\lambda\)</span> controls strength of
regularization</em></p>

</section>

<section>
<h2>Ridge Regression (L2)</h2>

<p> <strong>L2 Penalty: Sum of Squared
Coefficients</strong></p>
<p><span class="math display">\[L_{\text{ridge}}(\boldsymbol{\beta}) =
\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda
\|\boldsymbol{\beta}\|_2^2\]</span></p>
<p><strong>Closed-Form Solution:</strong> <span class="math display">\[\hat{\boldsymbol{\beta}}_{\text{ridge}} =
(\mathbf{X}^\top\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y}\]</span></p>
<ul>
<li class="fragment"><p>Shrinks all coefficients toward zero</p></li>
<li class="fragment"><p>Never sets coefficients exactly to zero</p></li>
<li class="fragment"><p>Always invertible (even when <span class="math inline">\(p &gt;
n\)</span>)</p></li>
</ul>
<p><em>Ridge adds <span class="math inline">\(\lambda\)</span> to
diagonal – stabilizes inversion</em></p>

</section>

<section>
<h2>Lasso Regression (L1)</h2>

<p> <strong>L1 Penalty: Sum of
Absolute Coefficients</strong></p>
<p><span class="math display">\[L_{\text{lasso}}(\boldsymbol{\beta}) =
\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda
\|\boldsymbol{\beta}\|_1\]</span></p>
<p><strong>Properties:</strong></p>
<ul>
<li class="fragment"><p>Produces sparse solutions (some <span class="math inline">\(\beta_j = 0\)</span>)</p></li>
<li class="fragment"><p>Automatic feature selection</p></li>
<li class="fragment"><p>No closed-form solution (use coordinate descent)</p></li>
</ul>
<p><em>Lasso: Least Absolute Shrinkage and Selection Operator</em></p>

</section>

<section>
<h2>Ridge vs Lasso Comparison</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/06_regularization_comparison.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>Ridge: smooth shrinkage. Lasso: sparse (feature
selection)</em></p>

</section>

<section>
<h2>Elastic Net</h2>

<p> <strong>Combining L1 and L2
Penalties</strong></p>
<p><span class="math display">\[L_{\text{elastic}}(\boldsymbol{\beta}) =
\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda_1
\|\boldsymbol{\beta}\|_1 + \lambda_2
\|\boldsymbol{\beta}\|_2^2\]</span></p>
<p><strong>Benefits:</strong></p>
<ul>
<li class="fragment"><p>Sparsity from L1</p></li>
<li class="fragment"><p>Stability from L2 (handles correlated features)</p></li>
<li class="fragment"><p>Two hyperparameters to tune</p></li>
</ul>
<p><em>Often best of both worlds for correlated features</em></p>

</section>

<section>
<h2>Choosing Lambda</h2>

<p> <strong>Cross-Validation for
Hyperparameter Tuning</strong></p>
<ol>
<li class="fragment"><p>Define grid of <span class="math inline">\(\lambda\)</span>
values (e.g., <span class="math inline">\(10^{-4}\)</span> to <span class="math inline">\(10^{4}\)</span>)</p></li>
<li class="fragment"><p>For each <span class="math inline">\(\lambda\)</span>, perform
K-fold CV</p></li>
<li class="fragment"><p>Select <span class="math inline">\(\lambda\)</span> with lowest
CV error</p></li>
<li class="fragment"><p>Refit on full training data</p></li>
</ol>
<p><strong>In Practice:</strong></p>
<ul>
<li class="fragment"><p><code>sklearn.linear_model.RidgeCV</code></p></li>
<li class="fragment"><p><code>sklearn.linear_model.LassoCV</code></p></li>
</ul>
<p><em>Larger <span class="math inline">\(\lambda\)</span> = more
regularization = simpler model</em></p>

</section>

<section>
<h2>Decomposing Prediction Error</h2>

<p> <strong>Expected Prediction
Error</strong></p>
<p><span class="math display">\[E[(y - \hat{f}(x))^2] =
\text{Bias}^2(\hat{f}) + \text{Var}(\hat{f}) + \sigma^2\]</span></p>
<ul>
<li class="fragment"><p><strong>Bias</strong>: Error from wrong assumptions
(underfitting)</p></li>
<li class="fragment"><p><strong>Variance</strong>: Error from sensitivity to training
data (overfitting)</p></li>
<li class="fragment"><p><strong><span class="math inline">\(\sigma^2\)</span></strong>:
Irreducible noise in data</p></li>
</ul>
<p><em>We can’t reduce irreducible error – focus on bias and
variance</em></p>

</section>

<section>
<h2>The Tradeoff Illustrated</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/07_bias_variance.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>Optimal complexity minimizes total error</em></p>

</section>

<section>
<h2>Regularization and Bias-Variance</h2>

<p> <strong>How
Regularization Helps</strong></p>
<ul>
<li class="fragment"><p>Increasing <span class="math inline">\(\lambda\)</span>: <span style="color: #FF7F0E"><strong>increases bias</strong></span>, <span style="color: #FF7F0E"><strong>decreases
variance</strong></span></p></li>
<li class="fragment"><p>Decreasing <span class="math inline">\(\lambda\)</span>:
decreases bias, increases variance</p></li>
<li class="fragment"><p>Optimal <span class="math inline">\(\lambda\)</span>: minimizes
total error</p></li>
</ul>
<p><strong>In Practice:</strong></p>
<ul>
<li class="fragment"><p>Use CV to find optimal <span class="math inline">\(\lambda\)</span></p></li>
<li class="fragment"><p>Regularization almost always helps when <span class="math inline">\(p\)</span> is large</p></li>
</ul>
<p><em>Regularization trades a little bias for a lot of variance
reduction</em></p>

</section>

<section>
<h2>Algorithm Selection Guide</h2>


<div class="center">
<p><img src="images/L01_Introduction_Linear_Regression/08_decision_flowchart.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><em>Use this framework when choosing regression methods</em></p>

</section>

<section>
<h2>Linear Regression: When and Why</h2>


<div class="columns">
<div class="column">
<p> <strong><span style="color: #2CA02C">Use
When:</span></strong></p>
<ul>
<li class="fragment"><p>Continuous target variable</p></li>
<li class="fragment"><p>Approximate linear relationships</p></li>
<li class="fragment"><p>Interpretability is critical</p></li>
<li class="fragment"><p>Inference on coefficients needed</p></li>
<li class="fragment"><p>Fast prediction required</p></li>
</ul>
</div><div class="column">
<p> <strong><span style="color: #D62728">Avoid
When:</span></strong></p>
<ul>
<li class="fragment"><p>Target is categorical</p></li>
<li class="fragment"><p>Strong non-linear patterns</p></li>
<li class="fragment"><p>Many outliers present</p></li>
<li class="fragment"><p>Features highly correlated</p></li>
<li class="fragment"><p>Prediction accuracy paramount</p></li>
</ul>
</div>
</div>
<p><em>When in doubt, linear regression is a strong baseline</em></p>

</section>

<section>
<h2>Key Equations Summary</h2>

<p> <span class="math display">\[\begin{aligned}
    \text{Model:} \quad &amp; \mathbf{y} = \mathbf{X}\boldsymbol{\beta}
+ \boldsymbol{\varepsilon} \\
    \text{OLS Solution:} \quad &amp; \hat{\boldsymbol{\beta}} =
(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y} \\
    \text{Gradient:} \quad &amp; \nabla L = -2\mathbf{X}^\top(\mathbf{y}
- \mathbf{X}\boldsymbol{\beta}) \\
    \text{GD Update:} \quad &amp; \boldsymbol{\beta}^{(t+1)} =
\boldsymbol{\beta}^{(t)} - \alpha \nabla L \\
    \text{Ridge:} \quad &amp; \hat{\boldsymbol{\beta}} =
(\mathbf{X}^\top\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y} \\
    R^2: \quad &amp; 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i -
\bar{y})^2}
  
\end{aligned}\]</span></p>

</section>

<section>
<h2>Key Takeaways</h2>


<ol>
<li class="fragment"><p>Linear regression minimizes squared error – closed form or
GD</p></li>
<li class="fragment"><p>Matrix notation enables efficient computation</p></li>
<li class="fragment"><p>Gradient descent scales to large datasets</p></li>
<li class="fragment"><p>Regularization (Ridge/Lasso) prevents overfitting</p></li>
<li class="fragment"><p>The bias-variance tradeoff guides model complexity</p></li>
<li class="fragment"><p>Always evaluate on held-out test data</p></li>
</ol>
<p><strong>Next Session:</strong> Logistic Regression for
Classification</p>

</section>

<section>
<h2>References</h2>


<ul>
<li class="fragment"><p>James, Witten, Hastie, Tibshirani (2021). <em>Introduction to
Statistical Learning</em>. Chapter 3.</p></li>
<li class="fragment"><p>Hastie, Tibshirani, Friedman (2009). <em>Elements of Statistical
Learning</em>. Chapter 3.</p></li>
<li class="fragment"><p>Bishop (2006). <em>Pattern Recognition and Machine Learning</em>.
Chapter 3.</p></li>
</ul>
<p><strong>Online Resources:</strong></p>
<ul>
<li class="fragment"><p>scikit-learn: <a class="uri" href="https://scikit-learn.org/stable/modules/linear_model.html">https://scikit-learn.org/stable/modules/linear_model.html</a></p></li>
<li class="fragment"><p>Stanford CS229: <a class="uri" href="https://cs229.stanford.edu/">https://cs229.stanford.edu/</a></p></li>
</ul>

</section>
        </div>

        <!-- Custom footer -->
        <div class="slide-footer">
            <div class="footer-left">Methods and Algorithms</div>
            <div class="footer-center">MSc Data Science</div>
            <div class="footer-right"></div>
        </div>
    </div>

    <!-- PDF Export Button -->
    <button class="pdf-export-btn" onclick="exportPDF()" title="Export to PDF">
        <i class="fas fa-file-pdf"></i> PDF
    </button>

    <!-- Reveal.js core -->
    <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

    <!-- Reveal.js plugins -->
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>

    <!-- Menu plugin -->
    <script src="https://unpkg.com/reveal.js-menu@2.1.0/menu.js"></script>

    <!-- Chalkboard plugin -->
    <script src="https://unpkg.com/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

    <script>
        // Spotlight functionality
        let spotlightEnabled = false;
        let spotlightElement = null;

        function toggleSpotlight() {
            spotlightEnabled = !spotlightEnabled;
            if (spotlightEnabled) {
                spotlightElement = document.createElement('div');
                spotlightElement.className = 'spotlight-cursor';
                spotlightElement.style.width = '100px';
                spotlightElement.style.height = '100px';
                document.body.appendChild(spotlightElement);
                document.addEventListener('mousemove', updateSpotlight);
            } else if (spotlightElement) {
                document.removeEventListener('mousemove', updateSpotlight);
                spotlightElement.remove();
                spotlightElement = null;
            }
        }

        function updateSpotlight(e) {
            if (spotlightElement) {
                spotlightElement.style.left = e.clientX + 'px';
                spotlightElement.style.top = e.clientY + 'px';
            }
        }

        // PDF Export function
        function exportPDF() {
            // Open print dialog with PDF-optimized settings
            const printUrl = window.location.href + '?print-pdf';
            window.open(printUrl, '_blank');
        }

        // Check if we're in print mode
        if (window.location.search.includes('print-pdf')) {
            document.querySelector('.pdf-export-btn').style.display = 'none';
        }

        Reveal.initialize({
            // Display settings
            controls: true,
            controlsTutorial: true,
            controlsLayout: 'bottom-right',
            progress: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            hash: true,
            history: true,

            // Navigation
            keyboard: true,
            overview: true,
            touch: true,
            loop: false,
            navigationMode: 'default',

            // Transitions
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',

            // Sizing (16:9 aspect ratio)
            width: 1600,
            height: 900,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            center: false,

            // Fragments
            fragments: true,
            fragmentInURL: false,

            // Auto-slide (disabled)
            autoSlide: 0,

            // Math configuration
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                tex: {
                    inlineMath: [['\\(', '\\)']],
                    displayMath: [['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            },

            // Menu plugin configuration
            menu: {
                side: 'left',
                width: 'normal',
                numbers: true,
                titleSelector: 'h1, h2',
                useTextContentForMissingTitles: true,
                hideMissingTitles: false,
                markers: true,
                custom: [
                    {
                        title: 'Tools',
                        icon: '<i class="fas fa-tools"></i>',
                        content: `
                            <ul class="slide-menu-items">
                                <li class="slide-menu-item" onclick="toggleSpotlight(); RevealMenu.toggle();">
                                    <i class="fas fa-bullseye"></i> Toggle Spotlight (L)
                                </li>
                                <li class="slide-menu-item" onclick="exportPDF();">
                                    <i class="fas fa-file-pdf"></i> Export PDF
                                </li>
                            </ul>
                        `
                    }
                ],
                themes: [
                    { name: 'ML Theme (Light)', theme: 'css/ml-theme.css' },
                    { name: 'Black', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/black.css' },
                    { name: 'White', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css' }
                ],
                transitions: true,
                openButton: true,
                openSlideNumber: false,
                keyboard: true,
                sticky: false,
                autoOpen: true
            },

            // Chalkboard plugin configuration
            chalkboard: {
                boardmarkerWidth: 3,
                chalkWidth: 4,
                chalkEffect: 0.5,
                storage: null,
                src: null,
                readOnly: false,
                toggleChalkboardButton: { left: "30px", bottom: "30px", top: "auto", right: "auto" },
                toggleNotesButton: { left: "70px", bottom: "30px", top: "auto", right: "auto" },
                colorButtons: true,
                boardHandle: true,
                transition: 800,
                theme: "chalkboard"
            },

            // Plugins to load
            plugins: [
                RevealMath,
                RevealNotes,
                RevealSearch,
                RevealZoom,
                RevealHighlight,
                RevealMenu,
                RevealChalkboard
            ]
        });

        // Custom keyboard shortcuts
        Reveal.addKeyBinding({ keyCode: 76, key: 'L' }, toggleSpotlight);  // L for spotlight

        // Update footer with slide number
        Reveal.on('slidechanged', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const current = event.indexh + 1;
                const total = Reveal.getTotalSlides();
                footer.textContent = `${current} / ${total}`;
            }
        });

        // Initial footer update
        Reveal.on('ready', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const total = Reveal.getTotalSlides();
                footer.textContent = `1 / ${total}`;
            }
        });

        // Keyboard shortcuts info
        console.log('Keyboard shortcuts:');
        console.log('  S - Speaker notes');
        console.log('  B - Chalkboard');
        console.log('  C - Canvas (draw on slide)');
        console.log('  L - Spotlight/Laser pointer');
        console.log('  M - Menu');
        console.log('  O - Overview');
        console.log('  F - Fullscreen');
        console.log('  ? - Help');
    </script>
</body>
</html>
