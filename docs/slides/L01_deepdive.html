<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Methods and Algorithms">
  <title>Introduction &amp; Linear Regression</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Introduction &amp; Linear Regression</h1>
  <p class="subtitle">Deep Dive: Mathematics and Implementation</p>
  <p class="author">Methods and Algorithms</p>
  <p class="date">Spring 2026</p>
</section>

<section class="slide level2">

<div class="frame">

</div>
<div class="frame">
<p><span>Outline</span></p>
</div>
</section>
<section id="mathematical-foundations" class="title-slide slide level1">
<h1>Mathematical Foundations</h1>
<div class="frame">
<p><span>Matrix Notation</span> <strong>The Model in Matrix
Form</strong></p>
<p><span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\]</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>:
Response vector</p></li>
<li><p><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times
(p+1)}\)</span>: Design matrix (with intercept column)</p></li>
<li><p><span class="math inline">\(\boldsymbol{\beta} \in
\mathbb{R}^{p+1}\)</span>: Coefficient vector</p></li>
<li><p><span class="math inline">\(\boldsymbol{\varepsilon} \in
\mathbb{R}^n\)</span>: Error vector</p></li>
</ul>
<p><em>Matrix notation enables elegant derivations and efficient
computation</em></p>
</div>
<div class="frame">
<p><span>Design Matrix Structure</span> <strong>The Design Matrix <span
class="math inline">\(\mathbf{X}\)</span></strong></p>
<p><span class="math display">\[\mathbf{X} = \begin{bmatrix}
      1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
      1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
      1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
    \end{bmatrix}\]</span></p>
<ul>
<li><p>First column of 1s for intercept <span
class="math inline">\(\beta_0\)</span></p></li>
<li><p>Each row is one observation</p></li>
<li><p>Each column (after first) is one feature</p></li>
</ul>
<p><em><span class="math inline">\(n\)</span> observations, <span
class="math inline">\(p\)</span> features, <span
class="math inline">\(p+1\)</span> parameters</em></p>
</div>
<div class="frame">
<p><span>OLS Assumptions</span> <strong>Classical Assumptions for Valid
Inference</strong></p>
<ol>
<li><p><strong>Linearity</strong>: <span class="math inline">\(E[y|X] =
X\beta\)</span> (correct functional form)</p></li>
<li><p><strong>Exogeneity</strong>: <span
class="math inline">\(E[\varepsilon|X] = 0\)</span> (no omitted
variables)</p></li>
<li><p><strong>Homoscedasticity</strong>: <span
class="math inline">\(\text{Var}(\varepsilon|X) = \sigma^2 I\)</span>
(constant variance)</p></li>
<li><p><strong>No multicollinearity</strong>: <span
class="math inline">\(\text{rank}(X) = p+1\)</span> (full rank)</p></li>
<li><p><strong>Normality</strong> (for inference): <span
class="math inline">\(\varepsilon \sim N(0, \sigma^2
I)\)</span></p></li>
</ol>
<p><strong>Violations?</strong> Robust standard errors, transformations,
regularization</p>
<p><em>Assumptions 1-4 needed for unbiased estimates; 5 for t-tests and
CIs</em></p>
</div>
<div class="frame">
<p><span>The Loss Function</span> <strong>Sum of Squared Residuals
(SSR)</strong></p>
<p><span class="math display">\[L(\boldsymbol{\beta}) = \sum_{i=1}^{n}
(y_i - \hat{y}_i)^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\]</span></p>
<p><strong>Expanding:</strong> <span
class="math display">\[L(\boldsymbol{\beta}) = \mathbf{y}^\top\mathbf{y}
- 2\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} +
\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p><em>Quadratic function in <span
class="math inline">\(\boldsymbol{\beta}\)</span> – has unique minimum
(if <span class="math inline">\(X\)</span> full rank)</em></p>
</div>
<div class="frame">
<p><span>Deriving the Normal Equation</span> <strong>Taking the
Derivative</strong></p>
<p><span class="math display">\[\frac{\partial L}{\partial
\boldsymbol{\beta}} = -2\mathbf{X}^\top\mathbf{y} +
2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p><strong>Setting to Zero:</strong> <span
class="math display">\[\begin{aligned}
    \mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} &amp;=
\mathbf{X}^\top\mathbf{y} \\
    \hat{\boldsymbol{\beta}} &amp;=
(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}
  
\end{aligned}\]</span></p>
<p><em>This is the closed-form OLS solution – the “normal
equation”</em></p>
</div>
<div class="frame">
<p><span>Simple Regression Visualization</span></p>
<div class="center">
<p><embed data-src="01_simple_regression/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>The fitted line minimizes vertical distances squared</em></p>
</div>
<div class="frame">
<p><span>Multiple Regression Surface</span></p>
<div class="center">
<p><embed data-src="02_multiple_regression_3d/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>With 2 features, we fit a plane; with <span
class="math inline">\(p\)</span> features, a hyperplane</em></p>
</div>
</section>

<section id="gradient-descent" class="title-slide slide level1">
<h1>Gradient Descent</h1>
<div class="frame">
<p><span>Why Gradient Descent?</span> <strong>Normal Equation
Limitations</strong></p>
<ul>
<li><p>Computing <span
class="math inline">\((\mathbf{X}^\top\mathbf{X})^{-1}\)</span> is <span
class="math inline">\(O(p^3)\)</span></p></li>
<li><p>Memory: Need to store <span class="math inline">\(p \times
p\)</span> matrix</p></li>
<li><p>For large <span class="math inline">\(p\)</span> (millions of
features): infeasible</p></li>
</ul>
<p><strong>Gradient Descent Advantages</strong></p>
<ul>
<li><p>Memory efficient: process one sample at a time</p></li>
<li><p>Scales to big data (SGD)</p></li>
<li><p>Generalizes to non-linear models</p></li>
</ul>
<p><em>For <span class="math inline">\(p &gt; 10{,}000\)</span>,
gradient descent usually faster</em></p>
</div>
<div class="frame">
<p><span>The Gradient</span> <strong>Gradient of the Loss
Function</strong></p>
<p><span class="math display">\[\nabla L(\boldsymbol{\beta}) =
-2\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) =
-2\mathbf{X}^\top\mathbf{r}\]</span></p>
<p>where <span class="math inline">\(\mathbf{r} = \mathbf{y} -
\mathbf{X}\boldsymbol{\beta}\)</span> is the residual vector.</p>
<p><strong>Intuition:</strong></p>
<ul>
<li><p>Gradient points in direction of steepest ascent</p></li>
<li><p>We move opposite to gradient (steepest descent)</p></li>
<li><p>Scale by learning rate <span
class="math inline">\(\alpha\)</span></p></li>
</ul>
<p><em>Gradient is a <span class="math inline">\(p+1\)</span>
dimensional vector</em></p>
</div>
<div class="frame">
<p><span>Gradient Descent Algorithm</span> <strong>Update
Rule</strong></p>
<p><span class="math display">\[\boldsymbol{\beta}^{(t+1)} =
\boldsymbol{\beta}^{(t)} - \alpha \nabla
L(\boldsymbol{\beta}^{(t)})\]</span></p>
<p><strong>Algorithm:</strong></p>
<ol>
<li><p>Initialize <span
class="math inline">\(\boldsymbol{\beta}^{(0)}\)</span> (often zeros or
random)</p></li>
<li><p>Compute gradient <span class="math inline">\(\nabla
L(\boldsymbol{\beta}^{(t)})\)</span></p></li>
<li><p>Update: <span class="math inline">\(\boldsymbol{\beta}^{(t+1)} =
\boldsymbol{\beta}^{(t)} - \alpha \nabla L\)</span></p></li>
<li><p>Repeat until convergence</p></li>
</ol>
<p><em>Convergence: <span
class="math inline">\(\|\boldsymbol{\beta}^{(t+1)} -
\boldsymbol{\beta}^{(t)}\| &lt; \epsilon\)</span> or max
iterations</em></p>
</div>
<div class="frame">
<p><span>Gradient Descent Visualization</span></p>
<div class="center">
<p><embed data-src="04_gradient_descent/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>Contours show loss surface; path shows optimization
trajectory</em></p>
</div>
<div class="frame">
<p><span>Learning Rate Selection</span> <strong>The Critical
Hyperparameter</strong></p>
<ul>
<li><p><strong>Too small</strong>: Slow convergence, many
iterations</p></li>
<li><p><strong>Too large</strong>: Divergence, oscillation</p></li>
<li><p><strong>Just right</strong>: Fast, stable convergence</p></li>
</ul>
<p><strong>Practical Approaches:</strong></p>
<ul>
<li><p>Start with <span class="math inline">\(\alpha = 0.01\)</span> or
<span class="math inline">\(0.001\)</span></p></li>
<li><p>Learning rate schedules (decay over time)</p></li>
<li><p>Adaptive methods: Adam, AdaGrad, RMSprop</p></li>
</ul>
<p><em>For OLS, optimal <span class="math inline">\(\alpha =
1/\lambda_{\max}(X^\top X)\)</span></em></p>
</div>
<div class="frame">
<p><span>Stochastic Gradient Descent (SGD)</span> <strong>Mini-Batch
Gradient Descent</strong></p>
<p>Instead of full gradient: <span class="math display">\[\nabla
L(\boldsymbol{\beta}) =
-\frac{2}{n}\mathbf{X}^\top\mathbf{r}\]</span></p>
<p>Use mini-batch of size <span class="math inline">\(m\)</span>: <span
class="math display">\[\nabla L_B(\boldsymbol{\beta}) =
-\frac{2}{m}\mathbf{X}_B^\top\mathbf{r}_B\]</span></p>
<ul>
<li><p><span class="math inline">\(m = 1\)</span>: Stochastic GD (noisy
but fast)</p></li>
<li><p><span class="math inline">\(m = n\)</span>: Batch GD (stable but
slow)</p></li>
<li><p><span class="math inline">\(m \in [32, 256]\)</span>: Mini-batch
(good tradeoff)</p></li>
</ul>
<p><em>SGD: Process data once per epoch, update many times</em></p>
</div>
</section>

<section id="model-evaluation" class="title-slide slide level1">
<h1>Model Evaluation</h1>
<div class="frame">
<p><span>R-Squared (<span class="math inline">\(R^2\)</span>)</span>
<strong>Coefficient of Determination</strong></p>
<p><span class="math display">\[R^2 = 1 -
\frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} = 1 -
\frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><p>Proportion of variance explained by model</p></li>
<li><p><span class="math inline">\(R^2 = 0\)</span>: Model no better
than mean</p></li>
<li><p><span class="math inline">\(R^2 = 1\)</span>: Perfect
fit</p></li>
<li><p><span class="math inline">\(R^2 = 0.7\)</span>: 70% of variance
explained</p></li>
</ul>
<p><em><span class="math inline">\(R^2\)</span> always increases with
more features – use Adjusted <span
class="math inline">\(R^2\)</span></em></p>
</div>
<div class="frame">
<p><span>Adjusted R-Squared</span> <strong>Penalizing Model
Complexity</strong></p>
<p><span class="math display">\[R^2_{\text{adj}} = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span></p>
<p><strong>Properties:</strong></p>
<ul>
<li><p>Adjusts for number of predictors <span
class="math inline">\(p\)</span></p></li>
<li><p>Can decrease when adding irrelevant features</p></li>
<li><p>Better for model comparison</p></li>
</ul>
<p><em>Use <span class="math inline">\(R^2_{\text{adj}}\)</span> when
comparing models with different <span
class="math inline">\(p\)</span></em></p>
</div>
<div class="frame">
<p><span>RMSE and MAE</span> <strong>Error Metrics in Original
Units</strong></p>
<p><span class="math display">\[\begin{aligned}
    \text{RMSE} &amp;= \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2} \\
    \text{MAE} &amp;= \frac{1}{n}\sum|y_i - \hat{y}_i|
  
\end{aligned}\]</span></p>
<p><strong>Comparison:</strong></p>
<ul>
<li><p>RMSE: Penalizes large errors more (sensitive to
outliers)</p></li>
<li><p>MAE: More robust, easier to interpret</p></li>
<li><p>Units: Same as target variable (e.g., dollars)</p></li>
</ul>
<p><em>Report both for comprehensive evaluation</em></p>
</div>
<div class="frame">
<p><span>Residual Analysis</span></p>
<div class="center">
<p><embed data-src="03_residual_plots/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>Good: random scatter. Bad: patterns indicate model
misspecification</em></p>
</div>
<div class="frame">
<p><span>Train-Test Split</span> <strong>Evaluating
Generalization</strong></p>
<ul>
<li><p>Never evaluate on training data alone</p></li>
<li><p>Split: 70-80% train, 20-30% test</p></li>
<li><p>Report test set metrics</p></li>
</ul>
<p><strong>Cross-Validation (K-Fold):</strong></p>
<ul>
<li><p>Split into <span class="math inline">\(K\)</span> folds
(typically <span class="math inline">\(K=5\)</span> or <span
class="math inline">\(10\)</span>)</p></li>
<li><p>Train on <span class="math inline">\(K-1\)</span> folds, validate
on 1</p></li>
<li><p>Repeat <span class="math inline">\(K\)</span> times, average
results</p></li>
</ul>
<p><em>CV gives more reliable estimate with limited data</em></p>
</div>
<div class="frame">
<p><span>Learning Curves</span></p>
<div class="center">
<p><embed data-src="05_learning_curves/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>Gap between curves indicates overfitting; convergence shows
saturation</em></p>
</div>
</section>

<section id="regularization" class="title-slide slide level1">
<h1>Regularization</h1>
<div class="frame">
<p><span>The Overfitting Problem</span> <strong>When Models Memorize
Instead of Learn</strong></p>
<ul>
<li><p>High-dimensional data (<span class="math inline">\(p \approx
n\)</span> or <span class="math inline">\(p &gt; n\)</span>)</p></li>
<li><p>Coefficients become very large</p></li>
<li><p>Perfect fit on training data, poor generalization</p></li>
</ul>
<p><strong>Solution: Add Penalty to Loss Function</strong> <span
class="math display">\[L_{\text{reg}}(\boldsymbol{\beta}) = \|\mathbf{y}
- \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \cdot
\text{penalty}(\boldsymbol{\beta})\]</span></p>
<p><em><span class="math inline">\(\lambda\)</span> controls strength of
regularization</em></p>
</div>
<div class="frame">
<p><span>Ridge Regression (L2)</span> <strong>L2 Penalty: Sum of Squared
Coefficients</strong></p>
<p><span class="math display">\[L_{\text{ridge}}(\boldsymbol{\beta}) =
\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda
\|\boldsymbol{\beta}\|_2^2\]</span></p>
<p><strong>Closed-Form Solution:</strong> <span
class="math display">\[\hat{\boldsymbol{\beta}}_{\text{ridge}} =
(\mathbf{X}^\top\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y}\]</span></p>
<ul>
<li><p>Shrinks all coefficients toward zero</p></li>
<li><p>Never sets coefficients exactly to zero</p></li>
<li><p>Always invertible (even when <span class="math inline">\(p &gt;
n\)</span>)</p></li>
</ul>
<p><em>Ridge adds <span class="math inline">\(\lambda\)</span> to
diagonal – stabilizes inversion</em></p>
</div>
<div class="frame">
<p><span>Lasso Regression (L1)</span> <strong>L1 Penalty: Sum of
Absolute Coefficients</strong></p>
<p><span class="math display">\[L_{\text{lasso}}(\boldsymbol{\beta}) =
\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda
\|\boldsymbol{\beta}\|_1\]</span></p>
<p><strong>Properties:</strong></p>
<ul>
<li><p>Produces sparse solutions (some <span
class="math inline">\(\beta_j = 0\)</span>)</p></li>
<li><p>Automatic feature selection</p></li>
<li><p>No closed-form solution (use coordinate descent)</p></li>
</ul>
<p><em>Lasso: Least Absolute Shrinkage and Selection Operator</em></p>
</div>
<div class="frame">
<p><span>Ridge vs Lasso Comparison</span></p>
<div class="center">
<p><embed data-src="06_regularization_comparison/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>Ridge: smooth shrinkage. Lasso: sparse (feature
selection)</em></p>
</div>
<div class="frame">
<p><span>Elastic Net</span> <strong>Combining L1 and L2
Penalties</strong></p>
<p><span class="math display">\[L_{\text{elastic}}(\boldsymbol{\beta}) =
\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda_1
\|\boldsymbol{\beta}\|_1 + \lambda_2
\|\boldsymbol{\beta}\|_2^2\]</span></p>
<p><strong>Benefits:</strong></p>
<ul>
<li><p>Sparsity from L1</p></li>
<li><p>Stability from L2 (handles correlated features)</p></li>
<li><p>Two hyperparameters to tune</p></li>
</ul>
<p><em>Often best of both worlds for correlated features</em></p>
</div>
<div class="frame">
<p><span>Choosing Lambda</span> <strong>Cross-Validation for
Hyperparameter Tuning</strong></p>
<ol>
<li><p>Define grid of <span class="math inline">\(\lambda\)</span>
values (e.g., <span class="math inline">\(10^{-4}\)</span> to <span
class="math inline">\(10^{4}\)</span>)</p></li>
<li><p>For each <span class="math inline">\(\lambda\)</span>, perform
K-fold CV</p></li>
<li><p>Select <span class="math inline">\(\lambda\)</span> with lowest
CV error</p></li>
<li><p>Refit on full training data</p></li>
</ol>
<p><strong>In Practice:</strong></p>
<ul>
<li><p><code>sklearn.linear_model.RidgeCV</code></p></li>
<li><p><code>sklearn.linear_model.LassoCV</code></p></li>
</ul>
<p><em>Larger <span class="math inline">\(\lambda\)</span> = more
regularization = simpler model</em></p>
</div>
</section>

<section id="bias-variance-tradeoff" class="title-slide slide level1">
<h1>Bias-Variance Tradeoff</h1>
<div class="frame">
<p><span>Decomposing Prediction Error</span> <strong>Expected Prediction
Error</strong></p>
<p><span class="math display">\[E[(y - \hat{f}(x))^2] =
\text{Bias}^2(\hat{f}) + \text{Var}(\hat{f}) + \sigma^2\]</span></p>
<ul>
<li><p><strong>Bias</strong>: Error from wrong assumptions
(underfitting)</p></li>
<li><p><strong>Variance</strong>: Error from sensitivity to training
data (overfitting)</p></li>
<li><p><strong><span class="math inline">\(\sigma^2\)</span></strong>:
Irreducible noise in data</p></li>
</ul>
<p><em>We can’t reduce irreducible error – focus on bias and
variance</em></p>
</div>
<div class="frame">
<p><span>The Tradeoff Illustrated</span></p>
<div class="center">
<p><embed data-src="07_bias_variance/chart.pdf"
style="width:65.0%" /></p>
</div>
<p><em>Optimal complexity minimizes total error</em></p>
</div>
<div class="frame">
<p><span>Regularization and Bias-Variance</span> <strong>How
Regularization Helps</strong></p>
<ul>
<li><p>Increasing <span class="math inline">\(\lambda\)</span>: <span
style="color: MLOrange"><strong>increases bias</strong></span>, <span
style="color: MLOrange"><strong>decreases
variance</strong></span></p></li>
<li><p>Decreasing <span class="math inline">\(\lambda\)</span>:
decreases bias, increases variance</p></li>
<li><p>Optimal <span class="math inline">\(\lambda\)</span>: minimizes
total error</p></li>
</ul>
<p><strong>In Practice:</strong></p>
<ul>
<li><p>Use CV to find optimal <span
class="math inline">\(\lambda\)</span></p></li>
<li><p>Regularization almost always helps when <span
class="math inline">\(p\)</span> is large</p></li>
</ul>
<p><em>Regularization trades a little bias for a lot of variance
reduction</em></p>
</div>
</section>

<section id="decision-framework" class="title-slide slide level1">
<h1>Decision Framework</h1>
<div class="frame">
<p><span>Algorithm Selection Guide</span></p>
<div class="center">
<p><embed data-src="08_decision_flowchart/chart.pdf"
style="width:60.0%" /></p>
</div>
<p><em>Use this framework when choosing regression methods</em></p>
</div>
<div class="frame">
<p><span>Linear Regression: When and Why</span></p>
<div class="columns">
<div class="column">
<p><span>0.48</span> <strong><span style="color: MLGreen">Use
When:</span></strong></p>
<ul>
<li><p>Continuous target variable</p></li>
<li><p>Approximate linear relationships</p></li>
<li><p>Interpretability is critical</p></li>
<li><p>Inference on coefficients needed</p></li>
<li><p>Fast prediction required</p></li>
</ul>
</div><div class="column">
<p><span>0.48</span> <strong><span style="color: MLRed">Avoid
When:</span></strong></p>
<ul>
<li><p>Target is categorical</p></li>
<li><p>Strong non-linear patterns</p></li>
<li><p>Many outliers present</p></li>
<li><p>Features highly correlated</p></li>
<li><p>Prediction accuracy paramount</p></li>
</ul>
</div>
</div>
<p><em>When in doubt, linear regression is a strong baseline</em></p>
</div>
</section>

<section id="summary" class="title-slide slide level1">
<h1>Summary</h1>
<div class="frame">
<p><span>Key Equations Summary</span> <span
class="math display">\[\begin{aligned}
    \text{Model:} \quad &amp; \mathbf{y} = \mathbf{X}\boldsymbol{\beta}
+ \boldsymbol{\varepsilon} \\
    \text{OLS Solution:} \quad &amp; \hat{\boldsymbol{\beta}} =
(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y} \\
    \text{Gradient:} \quad &amp; \nabla L = -2\mathbf{X}^\top(\mathbf{y}
- \mathbf{X}\boldsymbol{\beta}) \\
    \text{GD Update:} \quad &amp; \boldsymbol{\beta}^{(t+1)} =
\boldsymbol{\beta}^{(t)} - \alpha \nabla L \\
    \text{Ridge:} \quad &amp; \hat{\boldsymbol{\beta}} =
(\mathbf{X}^\top\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y} \\
    R^2: \quad &amp; 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i -
\bar{y})^2}
  
\end{aligned}\]</span></p>
</div>
<div class="frame">
<p><span>Key Takeaways</span></p>
<ol>
<li><p>Linear regression minimizes squared error – closed form or
GD</p></li>
<li><p>Matrix notation enables efficient computation</p></li>
<li><p>Gradient descent scales to large datasets</p></li>
<li><p>Regularization (Ridge/Lasso) prevents overfitting</p></li>
<li><p>The bias-variance tradeoff guides model complexity</p></li>
<li><p>Always evaluate on held-out test data</p></li>
</ol>
<p><strong>Next Session:</strong> Logistic Regression for
Classification</p>
</div>
<div class="frame">
<p><span>References</span></p>
<ul>
<li><p>James, Witten, Hastie, Tibshirani (2021). <em>Introduction to
Statistical Learning</em>. Chapter 3.</p></li>
<li><p>Hastie, Tibshirani, Friedman (2009). <em>Elements of Statistical
Learning</em>. Chapter 3.</p></li>
<li><p>Bishop (2006). <em>Pattern Recognition and Machine Learning</em>.
Chapter 3.</p></li>
</ul>
<p><strong>Online Resources:</strong></p>
<ul>
<li><p>scikit-learn: <a
href="https://scikit-learn.org/stable/modules/linear_model.html"
class="uri">https://scikit-learn.org/stable/modules/linear_model.html</a></p></li>
<li><p>Stanford CS229: <a href="https://cs229.stanford.edu/"
class="uri">https://cs229.stanford.edu/</a></p></li>
</ul>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: true,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
