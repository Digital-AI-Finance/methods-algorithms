<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="pandoc + custom post-processor">
    <meta name="author" content="Methods and Algorithms – MSc Data Science">
    <title>L04: Random Forests</title>

    <!-- Reveal.js core -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">

    <!-- Custom ML theme -->
    <link rel="stylesheet" href="css/ml-theme.css">

    <!-- Menu plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-menu@2.1.0/menu.css">

    <!-- Chalkboard plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-plugins@latest/chalkboard/style.css">

    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        /* Additional inline styles */
        .reveal .sourceCode {
            overflow: visible;
        }
        code {
            white-space: pre-wrap;
        }
        span.smallcaps {
            font-variant: small-caps;
        }
        div.columns {
            display: flex;
            gap: min(4vw, 1.5em);
        }
        div.column {
            flex: auto;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section id="title-slide">
<h1 class="title">L04: Random Forests</h1>
<p class="subtitle">Deep Dive: Theory, Implementation, and
Applications</p>
<p class="author">Methods and Algorithms – MSc Data Science</p>
</section>

<section>
<h2>Part 1: Decision Tree Foundations</h2>

<p> <strong>From Rules to
Trees</strong></p>
<ul>
<li class="fragment"><p>Decision trees encode if-then-else rules</p></li>
<li class="fragment"><p>Each node splits data based on a feature threshold</p></li>
<li class="fragment"><p>Leaves contain predictions (class or value)</p></li>
</ul>
<p><strong>Key Questions</strong></p>
<ul>
<li class="fragment"><p>How to choose the best split?</p></li>
<li class="fragment"><p>When to stop splitting?</p></li>
<li class="fragment"><p>How to make predictions?</p></li>
</ul>
<p><span style="color: gray">Decision trees: the building blocks of
Random Forests</span></p>

</section>

<section>
<h2>Splitting Criteria: Gini Impurity</h2>

<p> <strong>Gini
Impurity</strong> measures class mixture at a node: <span class="math display">\[G = 1 - \sum_{k=1}^{K} p_k^2\]</span> where <span class="math inline">\(p_k\)</span> is the proportion of class <span class="math inline">\(k\)</span> samples.</p>
<p><strong>Properties:</strong></p>
<ul>
<li class="fragment"><p><span class="math inline">\(G = 0\)</span>: pure node (all
samples same class)</p></li>
<li class="fragment"><p><span class="math inline">\(G = 0.5\)</span>: maximum impurity
for binary classification</p></li>
<li class="fragment"><p>Lower Gini = better split</p></li>
</ul>
<p><span style="color: gray">Gini impurity: probability of
misclassifying a random sample</span></p>

</section>

<section>
<h2>Splitting Criteria: Information Gain</h2>

<p>
<strong>Entropy</strong> measures disorder: <span class="math display">\[H = -\sum_{k=1}^{K} p_k \log_2(p_k)\]</span></p>
<p><strong>Information Gain</strong>: <span class="math display">\[IG =
H(\text{parent}) - \sum_{j} \frac{n_j}{n} H(\text{child}_j)\]</span></p>
<p><strong>Comparison:</strong></p>
<ul>
<li class="fragment"><p>Gini: faster to compute, tends to isolate most frequent
class</p></li>
<li class="fragment"><p>Entropy: more balanced trees, slightly slower</p></li>
<li class="fragment"><p>In practice: similar performance</p></li>
</ul>
<p><span style="color: gray">Both criteria aim to create pure child
nodes</span></p>

</section>

<section>
<h2>Regression Trees: MSE Criterion</h2>

<p> <strong>For
regression</strong>, use Mean Squared Error: <span class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i -
\bar{y})^2\]</span></p>
<p><strong>Split quality</strong>: <span class="math display">\[\text{Reduction} = \text{MSE}(\text{parent}) -
\sum_{j} \frac{n_j}{n} \text{MSE}(\text{child}_j)\]</span></p>
<p><strong>Leaf prediction</strong>: mean of samples in leaf <span style="color: gray">Trees can handle both classification and regression
tasks</span></p>

</section>

<section>
<h2>Decision Tree Algorithm</h2>

<p> <strong>Recursive
Partitioning:</strong></p>
<ol>
<li class="fragment"><p>Start with all samples at root</p></li>
<li class="fragment"><p>For each feature and threshold:</p>
<ul>
<li class="fragment"><p>Calculate impurity reduction</p></li>
<li class="fragment"><p>Select split with maximum reduction</p></li>
</ul></li>
<li class="fragment"><p>Create child nodes with split samples</p></li>
<li class="fragment"><p>Recurse until stopping criterion met</p></li>
</ol>
<p><strong>Stopping Criteria:</strong></p>
<ul>
<li class="fragment"><p>Maximum depth reached</p></li>
<li class="fragment"><p>Minimum samples per leaf</p></li>
<li class="fragment"><p>No improvement in impurity</p></li>
</ul>
<p><span style="color: gray">Greedy algorithm: locally optimal splits at
each step</span></p>

</section>

<section>
<h2>Decision Tree: Fraud Detection Example</h2>


<div class="center">
<p><embed data-src="01_decision_tree/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">Each path through the tree represents a
fraud detection rule</span></p>

</section>

<section>
<h2>Part 2: Why Ensembles?</h2>

<p> <strong>Problem with Single
Trees:</strong></p>
<ul>
<li class="fragment"><p>High variance: small data changes <span class="math inline">\(\rightarrow\)</span> very different trees</p></li>
<li class="fragment"><p>Prone to overfitting</p></li>
<li class="fragment"><p>Unstable predictions</p></li>
</ul>
<p><strong>Ensemble Solution:</strong></p>
<ul>
<li class="fragment"><p>Train multiple diverse models</p></li>
<li class="fragment"><p>Combine predictions</p></li>
<li class="fragment"><p>Reduce variance while maintaining low bias</p></li>
</ul>
<p><span style="color: gray">“Wisdom of crowds”: aggregate many weak
learners into strong learner</span></p>

</section>

<section>
<h2>Bootstrap Sampling</h2>

<p> <strong>Bootstrap</strong>: sample
with replacement from original data</p>
<p><strong>Properties:</strong></p>
<ul>
<li class="fragment"><p>Each sample: same size as original (<span class="math inline">\(n\)</span> observations)</p></li>
<li class="fragment"><p>Expected unique samples: <span class="math inline">\(\approx
63.2\%\)</span> (probability <span class="math inline">\(1 -
(1-1/n)^n\)</span>)</p></li>
<li class="fragment"><p>Remaining <span class="math inline">\(\sim 37\%\)</span>:
out-of-bag (OOB) samples</p></li>
</ul>
<p><strong>Effect:</strong></p>
<ul>
<li class="fragment"><p>Each tree sees different data subset</p></li>
<li class="fragment"><p>Creates diversity among trees</p></li>
<li class="fragment"><p>OOB samples provide validation</p></li>
</ul>
<p><span style="color: gray">Bootstrap: key ingredient for reducing
variance through aggregation</span></p>

</section>

<section>
<h2>Bagging Visualization</h2>


<div class="center">
<p><embed data-src="03_bootstrap/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">Bootstrap Aggregating: train on random
samples, aggregate predictions</span></p>

</section>

<section>
<h2>Bagging: Mathematical Foundation</h2>

<p> <strong>Variance
Reduction by Averaging</strong></p>
<p>For <span class="math inline">\(B\)</span> independent predictions
with variance <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\text{Var}\left(\frac{1}{B}\sum_{b=1}^{B}
\hat{f}_b(x)\right) = \frac{\sigma^2}{B}\]</span></p>
<p><strong>With correlation <span class="math inline">\(\rho\)</span></strong>: <span class="math display">\[\text{Var} = \rho \sigma^2 +
\frac{1-\rho}{B}\sigma^2\]</span></p>
<p><strong>Key insight</strong>: Reduce correlation between trees to
maximize variance reduction <span style="color: gray">Lower correlation
between trees = greater ensemble benefit</span></p>

</section>

<section>
<h2>Part 3: Random Forests Algorithm</h2>

<p> <strong>Two Sources of
Randomness:</strong></p>
<ol>
<li class="fragment"><p><strong>Bootstrap sampling</strong>: each tree trained on random
sample</p></li>
<li class="fragment"><p><strong>Feature randomization</strong>: each split considers
random subset</p></li>
</ol>
<p><strong>Feature Subset Size</strong> (at each split):</p>
<ul>
<li class="fragment"><p>Classification: <span class="math inline">\(\sqrt{p}\)</span>
features (default)</p></li>
<li class="fragment"><p>Regression: <span class="math inline">\(p/3\)</span> features
(default)</p></li>
<li class="fragment"><p>Decorrelates trees more than bagging alone</p></li>
</ul>
<p><span style="color: gray">Feature randomization: Breiman’s key
innovation over bagging</span></p>

</section>

<section>
<h2>Random Forest Algorithm</h2>

<p> <strong>Training:</strong></p>
<ol>
<li class="fragment"><p>For <span class="math inline">\(b = 1\)</span> to <span class="math inline">\(B\)</span> trees:</p>
<ul>
<li class="fragment"><p>Draw bootstrap sample of size <span class="math inline">\(n\)</span></p></li>
<li class="fragment"><p>Grow tree:</p>
<ul>
<li class="fragment"><p>At each node, select <span class="math inline">\(m\)</span>
features randomly</p></li>
<li class="fragment"><p>Find best split among <span class="math inline">\(m\)</span>
features</p></li>
<li class="fragment"><p>Split until stopping criterion</p></li>
</ul></li>
</ul></li>
</ol>
<p><strong>Prediction:</strong></p>
<ul>
<li class="fragment"><p>Classification: majority vote across trees</p></li>
<li class="fragment"><p>Regression: average predictions</p></li>
</ul>
<p><span style="color: gray">Typical: 100-500 trees, but more trees
never hurts (just slower)</span></p>

</section>

<section>
<h2>Feature Importance</h2>


<div class="center">
<p><embed data-src="02_feature_importance/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">Mean Decrease in Impurity: sum of impurity
reductions from splits on feature</span></p>

</section>

<section>
<h2>Feature Importance Methods</h2>

<p> <strong>1. Mean Decrease in
Impurity (MDI):</strong></p>
<ul>
<li class="fragment"><p>Sum of Gini/entropy reductions from splits on feature</p></li>
<li class="fragment"><p>Fast to compute (comes free from training)</p></li>
<li class="fragment"><p>Bias toward high-cardinality features</p></li>
</ul>
<p><strong>2. Permutation Importance:</strong></p>
<ul>
<li class="fragment"><p>Permute feature values, measure accuracy drop</p></li>
<li class="fragment"><p>More reliable, less biased</p></li>
<li class="fragment"><p>Slower (requires re-evaluation)</p></li>
</ul>
<p><span style="color: gray">Permutation importance preferred for final
feature selection</span></p>

</section>

<section>
<h2>Out-of-Bag Error</h2>


<div class="center">
<p><embed data-src="04_oob_error/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">OOB error: free cross-validation using
samples not in bootstrap</span></p>

</section>

<section>
<h2>Out-of-Bag Error: How It Works</h2>

<p> <strong>For each
observation <span class="math inline">\(i\)</span>:</strong></p>
<ol>
<li class="fragment"><p>Identify trees where <span class="math inline">\(i\)</span> was
OOB (not in bootstrap sample)</p></li>
<li class="fragment"><p>Aggregate predictions from only those trees</p></li>
<li class="fragment"><p>Compare to true label</p></li>
</ol>
<p><strong>Benefits:</strong></p>
<ul>
<li class="fragment"><p>No separate validation set needed</p></li>
<li class="fragment"><p>Uses <span class="math inline">\(\sim 37\%\)</span> of trees per
sample</p></li>
<li class="fragment"><p>Unbiased estimate of generalization error</p></li>
</ul>
<p><span style="color: gray">OOB error converges to leave-one-out
cross-validation error</span></p>

</section>

<section>
<h2>Ensemble Voting</h2>


<div class="center">
<p><embed data-src="05_ensemble_voting/chart.pdf" style="width:55.0%"/></p>
</div>
<p><span style="color: gray">Classification: majority vote. Regression:
average prediction</span></p>

</section>

<section>
<h2>Part 4: Bias-Variance Decomposition</h2>

<p> <strong>Expected
Prediction Error:</strong> <span class="math display">\[\mathbb{E}[(y -
\hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible
Error}\]</span></p>
<p><strong>Single Tree:</strong></p>
<ul>
<li class="fragment"><p>Low bias (can fit complex patterns)</p></li>
<li class="fragment"><p>High variance (sensitive to training data)</p></li>
</ul>
<p><strong>Random Forest:</strong></p>
<ul>
<li class="fragment"><p>Bias: similar to single tree</p></li>
<li class="fragment"><p>Variance: reduced by <span class="math inline">\(\approx\)</span>
factor of <span class="math inline">\(1/B\)</span> (with
decorrelation)</p></li>
</ul>
<p><span style="color: gray">Ensembles reduce variance without
increasing bias</span></p>

</section>

<section>
<h2>Bias-Variance: Visualization</h2>


<div class="center">
<p><embed data-src="06_bias_variance/chart.pdf" style="width:65.0%"/></p>
</div>
<p><span style="color: gray">Averaging decorrelated trees dramatically
reduces prediction variance</span></p>

</section>

<section>
<h2>Hyperparameters: Number of Trees</h2>

<p>
<strong>n_estimators</strong> (number of trees):</p>
<ul>
<li class="fragment"><p>More trees = lower variance, never overfits</p></li>
<li class="fragment"><p>Diminishing returns after 100-500 trees</p></li>
<li class="fragment"><p>Cost: linear increase in training/prediction time</p></li>
</ul>
<p><strong>Guidelines:</strong></p>
<ul>
<li class="fragment"><p>Start with 100, increase if OOB error still decreasing</p></li>
<li class="fragment"><p>For production: balance accuracy vs. latency</p></li>
<li class="fragment"><p>More trees always better (if time permits)</p></li>
</ul>
<p><span style="color: gray">Unlike most hyperparameters, more trees
cannot hurt accuracy</span></p>

</section>

<section>
<h2>Hyperparameters: Tree Complexity</h2>

<p>
<strong>max_depth</strong>: Maximum tree depth</p>
<ul>
<li class="fragment"><p>Deeper = more complex patterns, higher variance</p></li>
<li class="fragment"><p>Default: unlimited (grow full trees)</p></li>
</ul>
<p><strong>min_samples_split</strong>: Minimum samples to split</p>
<ul>
<li class="fragment"><p>Higher = simpler trees, more regularization</p></li>
<li class="fragment"><p>Default: 2 (full trees)</p></li>
</ul>
<p><strong>min_samples_leaf</strong>: Minimum samples in leaf</p>
<ul>
<li class="fragment"><p>Higher = smoother predictions</p></li>
<li class="fragment"><p>Default: 1 (full trees)</p></li>
</ul>
<p><span style="color: gray">Full trees (default) often work well due to
bagging’s variance reduction</span></p>

</section>

<section>
<h2>Hyperparameters: Feature Randomization</h2>

<p>
<strong>max_features</strong>: Features considered at each split</p>
<ul>
<li class="fragment"><p>Lower = more decorrelated trees, higher bias</p></li>
<li class="fragment"><p>Higher = less decorrelated, lower bias</p></li>
</ul>
<p><strong>Defaults:</strong></p>
<ul>
<li class="fragment"><p>Classification: <span class="math inline">\(\sqrt{p}\)</span>
(e.g., 10 features <span class="math inline">\(\rightarrow\)</span>
3)</p></li>
<li class="fragment"><p>Regression: <span class="math inline">\(p/3\)</span> (e.g., 30
features <span class="math inline">\(\rightarrow\)</span> 10)</p></li>
</ul>
<p><strong>Tuning:</strong></p>
<ul>
<li class="fragment"><p>Try: <span class="math inline">\(\sqrt{p}\)</span>, <span class="math inline">\(\log_2(p)\)</span>, <span class="math inline">\(p/3\)</span></p></li>
<li class="fragment"><p>Cross-validate to find optimal</p></li>
</ul>
<p><span style="color: gray">Feature randomization is key differentiator
from bagged trees</span></p>

</section>

<section>
<h2>Part 5: Practical Considerations</h2>

<p>
<strong>Advantages:</strong></p>
<ul>
<li class="fragment"><p>Excellent accuracy out-of-the-box</p></li>
<li class="fragment"><p>Handles mixed feature types</p></li>
<li class="fragment"><p>Built-in feature importance</p></li>
<li class="fragment"><p>Robust to outliers and missing values</p></li>
<li class="fragment"><p>Parallelizable (trees independent)</p></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li class="fragment"><p>Less interpretable than single tree</p></li>
<li class="fragment"><p>Memory intensive (stores all trees)</p></li>
<li class="fragment"><p>Slower prediction than linear models</p></li>
<li class="fragment"><p>Cannot extrapolate beyond training range</p></li>
</ul>
<p><span style="color: gray">Random Forests: excellent default choice
for tabular data</span></p>

</section>

<section>
<h2>Comparison: Random Forest vs. Others</h2>


<div class="center">
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: center;"><strong>RF</strong></th>
<th style="text-align: center;"><strong>Single Tree</strong></th>
<th style="text-align: center;"><strong>Logistic</strong></th>
<th style="text-align: center;"><strong>KNN</strong></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Interpretability</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Speed</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Fast</td>
<td style="text-align: center;">Fast</td>
<td style="text-align: center;">Fast</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Feature Importance</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Non-linear</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p><span style="color: gray">RF trades some interpretability for
significant accuracy gains</span></p>

</section>

<section>
<h2>When to Use Random Forests</h2>

<p> <strong>Use
When:</strong></p>
<ul>
<li class="fragment"><p>Tabular data with mixed feature types</p></li>
<li class="fragment"><p>Non-linear relationships expected</p></li>
<li class="fragment"><p>Feature importance needed</p></li>
<li class="fragment"><p>Out-of-the-box performance matters</p></li>
</ul>
<p><strong>Consider Alternatives When:</strong></p>
<ul>
<li class="fragment"><p>Need fully interpretable model (use single tree)</p></li>
<li class="fragment"><p>Very high-dimensional sparse data (use linear models)</p></li>
<li class="fragment"><p>Extrapolation required (use parametric models)</p></li>
<li class="fragment"><p>Need fastest prediction (use linear/shallow tree)</p></li>
</ul>
<p><span style="color: gray">Random Forest: often the first model to try
on tabular data</span></p>

</section>

<section>
<h2>Decision Framework</h2>


<div class="center">
<p><embed data-src="07_decision_flowchart/chart.pdf" style="width:65.0%"/></p>
</div>
<p><span style="color: gray">Start with Random Forest; switch if
specific constraints require it</span></p>

</section>

<section>
<h2>Implementation: scikit-learn</h2>

<p>
<strong>Classification:</strong></p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, max_features='sqrt',
                            oob_score=True, random_state=42)
rf.fit(X_train, y_train)
print(f"OOB Score: {rf.oob_score_:.3f}")</code></pre>
<p><strong>Key Parameters:</strong></p>
<ul>
<li class="fragment"><p><code>n_estimators</code>: number of trees</p></li>
<li class="fragment"><p><code>max_features</code>: features per split</p></li>
<li class="fragment"><p><code>oob_score</code>: compute OOB error</p></li>
<li class="fragment"><p><code>n_jobs</code>: parallel trees (-1 for all cores)</p></li>
</ul>
<p><span style="color: gray">Set random_state for
reproducibility</span></p>

</section>

<section>
<h2>Summary: Random Forests</h2>

<p> <strong>Core
Concepts:</strong></p>
<ul>
<li class="fragment"><p>Ensemble of decision trees with bootstrap + feature
randomization</p></li>
<li class="fragment"><p>Reduces variance while maintaining low bias</p></li>
<li class="fragment"><p>OOB error provides free cross-validation</p></li>
</ul>
<p><strong>Practical Takeaways:</strong></p>
<ul>
<li class="fragment"><p>Excellent default for tabular data</p></li>
<li class="fragment"><p>Feature importance aids interpretation</p></li>
<li class="fragment"><p>More trees never hurts (just slower)</p></li>
<li class="fragment"><p>Hyperparameter tuning usually optional</p></li>
</ul>
<p><span style="color: gray">Next: PCA and t-SNE for dimensionality
reduction</span></p>

</section>

<section>
<h2>References</h2>

<p> <strong>Textbooks:</strong></p>
<ul>
<li class="fragment"><p>James et al. (2021). <em>ISLR</em>, Chapter 8: Tree-Based
Methods</p></li>
<li class="fragment"><p>Hastie et al. (2009). <em>ESL</em>, Chapter 15: Random
Forests</p></li>
</ul>
<p><strong>Original Papers:</strong></p>
<ul>
<li class="fragment"><p>Breiman, L. (2001). Random Forests. <em>Machine Learning</em>,
45(1), 5-32.</p></li>
<li class="fragment"><p>Breiman, L. (1996). Bagging Predictors. <em>Machine
Learning</em>, 24(2), 123-140.</p></li>
</ul>
<p><strong>Documentation:</strong></p>
<ul>
<li class="fragment"><p>scikit-learn:
<code>sklearn.ensemble.RandomForestClassifier</code></p></li>
</ul>
<p><span style="color: gray">Breiman’s 2001 paper: one of the most cited
in ML</span></p>

</section>
        </div>

        <!-- Custom footer -->
        <div class="slide-footer">
            <div class="footer-left">Methods and Algorithms</div>
            <div class="footer-center">MSc Data Science</div>
            <div class="footer-right"></div>
        </div>
    </div>

    <!-- Reveal.js core -->
    <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

    <!-- Reveal.js plugins -->
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>

    <!-- Menu plugin -->
    <script src="https://unpkg.com/reveal.js-menu@2.1.0/menu.js"></script>

    <!-- Chalkboard plugin -->
    <script src="https://unpkg.com/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

    <script>
        Reveal.initialize({
            // Display settings
            controls: true,
            controlsTutorial: true,
            controlsLayout: 'bottom-right',
            progress: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            hash: true,
            history: true,

            // Navigation
            keyboard: true,
            overview: true,
            touch: true,
            loop: false,
            navigationMode: 'default',

            // Transitions
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',

            // Sizing (16:9 aspect ratio)
            width: 1600,
            height: 900,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            center: false,

            // Fragments
            fragments: true,
            fragmentInURL: false,

            // Auto-slide (disabled)
            autoSlide: 0,

            // Math configuration
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                tex: {
                    inlineMath: [['\\(', '\\)']],
                    displayMath: [['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            },

            // Menu plugin configuration
            menu: {
                side: 'left',
                width: 'normal',
                numbers: true,
                titleSelector: 'h1, h2',
                useTextContentForMissingTitles: true,
                hideMissingTitles: false,
                markers: true,
                custom: false,
                themes: [
                    { name: 'ML Theme (Light)', theme: 'css/ml-theme.css' },
                    { name: 'Black', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/black.css' },
                    { name: 'White', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css' }
                ],
                transitions: true,
                openButton: true,
                openSlideNumber: false,
                keyboard: true,
                sticky: false,
                autoOpen: true
            },

            // Chalkboard plugin configuration
            chalkboard: {
                boardmarkerWidth: 3,
                chalkWidth: 4,
                chalkEffect: 0.5,
                storage: null,
                src: null,
                readOnly: false,
                toggleChalkboardButton: { left: "30px", bottom: "30px", top: "auto", right: "auto" },
                toggleNotesButton: { left: "70px", bottom: "30px", top: "auto", right: "auto" },
                colorButtons: true,
                boardHandle: true,
                transition: 800,
                theme: "chalkboard"
            },

            // Plugins to load
            plugins: [
                RevealMath,
                RevealNotes,
                RevealSearch,
                RevealZoom,
                RevealHighlight,
                RevealMenu,
                RevealChalkboard
            ]
        });

        // Update footer with slide number
        Reveal.on('slidechanged', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const current = event.indexh + 1;
                const total = Reveal.getTotalSlides();
                footer.textContent = `${current} / ${total}`;
            }
        });

        // Initial footer update
        Reveal.on('ready', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const total = Reveal.getTotalSlides();
                footer.textContent = `1 / ${total}`;
            }
        });

        // Keyboard shortcuts info
        console.log('Keyboard shortcuts:');
        console.log('  S - Speaker notes');
        console.log('  B - Chalkboard');
        console.log('  C - Canvas (draw on slide)');
        console.log('  M - Menu');
        console.log('  O - Overview');
        console.log('  F - Fullscreen');
        console.log('  ? - Help');
    </script>
</body>
</html>
