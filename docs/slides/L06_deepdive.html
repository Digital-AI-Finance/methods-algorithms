<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="pandoc + custom post-processor">
    <meta name="author" content="Methods and Algorithms – MSc Data Science">
    <title>L06: Embeddings & RL</title>

    <!-- Reveal.js core -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">

    <!-- Custom ML theme -->
    <link rel="stylesheet" href="css/ml-theme.css">

    <!-- Menu plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-menu@2.1.0/menu.css">

    <!-- Chalkboard plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-plugins@latest/chalkboard/style.css">

    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        /* Additional inline styles */
        .reveal .sourceCode {
            overflow: visible;
        }
        code {
            white-space: pre-wrap;
        }
        span.smallcaps {
            font-variant: small-caps;
        }
        div.columns {
            display: flex;
            gap: min(4vw, 1.5em);
        }
        div.column {
            flex: auto;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section id="title-slide">
<h1 class="title">L06: Embeddings &amp; RL</h1>
<p class="subtitle">Deep Dive: Theory, Implementation, and
Applications</p>
<p class="author">Methods and Algorithms – MSc Data Science</p>
</section>

<section>
<h2>Part 1: Word Embeddings Introduction</h2>

<p> <strong>The Problem
with One-Hot Encoding</strong></p>
<ul>
<li class="fragment"><p>Vocabulary of 10,000 words <span class="math inline">\(\rightarrow\)</span> 10,000-dim sparse
vectors</p></li>
<li class="fragment"><p>No semantic similarity: “king” and “queen” equally distant from
“car”</p></li>
<li class="fragment"><p>Curse of dimensionality</p></li>
</ul>
<p><strong>Solution: Dense Embeddings</strong></p>
<ul>
<li class="fragment"><p>Map words to dense vectors (50-300 dimensions)</p></li>
<li class="fragment"><p>Similar words <span class="math inline">\(\rightarrow\)</span>
similar vectors</p></li>
<li class="fragment"><p>Learn from context (distributional hypothesis)</p></li>
</ul>
<p><span style="color: gray">“You shall know a word by the company it
keeps” – Firth, 1957</span></p>

</section>

<section>
<h2>Word2Vec: Skip-gram</h2>

<p> <strong>Objective:</strong> Predict
context words given target word <span class="math display">\[P(w_{context} | w_{target}) =
\frac{\exp(v_{context}^T v_{target})}{\sum_{w \in V} \exp(v_w^T
v_{target})}\]</span></p>
<p><strong>Training:</strong></p>
<ul>
<li class="fragment"><p>Slide window over text corpus</p></li>
<li class="fragment"><p>For each word, predict surrounding words</p></li>
<li class="fragment"><p>Update embeddings via gradient descent</p></li>
</ul>
<p><span style="color: gray">Skip-gram works well for rare words; CBOW
better for frequent words</span></p>

</section>

<section>
<h2>Word Embedding Space</h2>


<div class="center">
<p><embed data-src="01_word_embedding_space/chart.pdf" style="width:55.0%"/></p>
</div>
<p><span style="color: gray">Finance terms cluster by semantic category
in embedding space</span></p>

</section>

<section>
<h2>Word Analogies</h2>

<p> <strong>Famous Example:</strong> <span class="math display">\[\vec{king} - \vec{man} + \vec{woman} \approx
\vec{queen}\]</span></p>
<p><strong>Finance Examples:</strong></p>
<ul>
<li class="fragment"><p><span class="math inline">\(\vec{stock} - \vec{equity} +
\vec{debt} \approx \vec{bond}\)</span></p></li>
<li class="fragment"><p><span class="math inline">\(\vec{CEO} - \vec{company} +
\vec{country} \approx \vec{president}\)</span></p></li>
</ul>
<p><strong>How it works:</strong></p>
<ul>
<li class="fragment"><p>Vector arithmetic in embedding space</p></li>
<li class="fragment"><p>Relationships encoded as directions</p></li>
</ul>
<p><span style="color: gray">Embeddings capture relational structure,
not just similarity</span></p>

</section>

<section>
<h2>Similarity Measures</h2>

<p> <strong>Cosine Similarity:</strong>
<span class="math display">\[\text{sim}(u, v) = \frac{u \cdot v}{||u||
\cdot ||v||} = \cos(\theta)\]</span></p>
<ul>
<li class="fragment"><p>Range: <span class="math inline">\([-1, 1]\)</span></p></li>
<li class="fragment"><p>1 = identical direction, 0 = orthogonal, -1 = opposite</p></li>
</ul>
<div class="center">
<p><embed data-src="02_similarity_heatmap/chart.pdf" style="width:35.0%"/></p>
</div>
<p><span style="color: gray">Cosine similarity ignores magnitude,
focuses on direction</span></p>

</section>

<section>
<h2>Pre-trained Embeddings</h2>

<p> <strong>Popular
Options:</strong></p>
<ul>
<li class="fragment"><p><strong>Word2Vec</strong>: Google, 300-dim, 3M words</p></li>
<li class="fragment"><p><strong>GloVe</strong>: Stanford, trained on Wikipedia + Common
Crawl</p></li>
<li class="fragment"><p><strong>FastText</strong>: Facebook, handles subwords (OOV
robust)</p></li>
</ul>
<p><strong>Domain-Specific:</strong></p>
<ul>
<li class="fragment"><p>FinBERT: Pre-trained on financial text</p></li>
<li class="fragment"><p>BioBERT: Biomedical domain</p></li>
</ul>
<p><span style="color: gray">Fine-tuning pre-trained embeddings usually
outperforms training from scratch</span></p>

</section>

<section>
<h2>Embeddings in Finance</h2>

<p> <strong>Applications:</strong></p>
<ul>
<li class="fragment"><p><strong>Sentiment Analysis</strong>: News <span class="math inline">\(\rightarrow\)</span> embedding <span class="math inline">\(\rightarrow\)</span> positive/negative</p></li>
<li class="fragment"><p><strong>Document Similarity</strong>: Find similar SEC
filings</p></li>
<li class="fragment"><p><strong>Named Entity Recognition</strong>: Extract company
names</p></li>
<li class="fragment"><p><strong>Event Detection</strong>: Identify earnings
announcements</p></li>
</ul>
<p><strong>Sentence Embeddings:</strong></p>
<ul>
<li class="fragment"><p>Average word vectors (simple baseline)</p></li>
<li class="fragment"><p>Doc2Vec (paragraph vectors)</p></li>
<li class="fragment"><p>Sentence-BERT (state-of-the-art)</p></li>
</ul>
<p><span style="color: gray">Aggregate word embeddings to represent
documents</span></p>

</section>

<section>
<h2>Part 2: Reinforcement Learning Framework</h2>

<p> <strong>Key
Components:</strong></p>
<ul>
<li class="fragment"><p><strong>Agent</strong>: Learner/decision-maker</p></li>
<li class="fragment"><p><strong>Environment</strong>: What agent interacts with</p></li>
<li class="fragment"><p><strong>State</strong> <span class="math inline">\(s\)</span>:
Current situation</p></li>
<li class="fragment"><p><strong>Action</strong> <span class="math inline">\(a\)</span>:
What agent can do</p></li>
<li class="fragment"><p><strong>Reward</strong> <span class="math inline">\(r\)</span>:
Feedback signal</p></li>
</ul>
<div class="center">
<p><embed data-src="03_rl_loop/chart.pdf" style="width:40.0%"/></p>
</div>
<p><span style="color: gray">RL: Learning from interaction, not from
labeled examples</span></p>

</section>

<section>
<h2>Markov Decision Process</h2>

<p> <strong>MDP Tuple:</strong>
<span class="math inline">\((S, A, P, R, \gamma)\)</span></p>
<ul>
<li class="fragment"><p><span class="math inline">\(S\)</span>: Set of states</p></li>
<li class="fragment"><p><span class="math inline">\(A\)</span>: Set of actions</p></li>
<li class="fragment"><p><span class="math inline">\(P(s'|s, a)\)</span>: Transition
probability</p></li>
<li class="fragment"><p><span class="math inline">\(R(s, a, s')\)</span>: Reward
function</p></li>
<li class="fragment"><p><span class="math inline">\(\gamma \in [0,1]\)</span>: Discount
factor</p></li>
</ul>
<p><strong>Markov Property:</strong> <span class="math display">\[P(s_{t+1} | s_t, a_t, s_{t-1}, ...) = P(s_{t+1} |
s_t, a_t)\]</span> <span style="color: gray">Future depends only on
current state, not history</span></p>

</section>

<section>
<h2>Policy and Value Functions</h2>

<p> <strong>Policy:</strong>
<span class="math inline">\(\pi(a|s) = P(A_t = a | S_t = s)\)</span></p>
<ul>
<li class="fragment"><p>Maps states to action probabilities</p></li>
<li class="fragment"><p>Goal: Find optimal policy <span class="math inline">\(\pi^*\)</span></p></li>
</ul>
<p><strong>Value Function:</strong> <span class="math display">\[V^\pi(s) = \mathbb{E}_\pi \left[
\sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s \right]\]</span></p>
<p><strong>Q-Function (Action-Value):</strong> <span class="math display">\[Q^\pi(s, a) = \mathbb{E}_\pi \left[
\sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s, A_0 = a \right]\]</span>
<span style="color: gray">Q-function: expected return starting from
state s, taking action a</span></p>

</section>

<section>
<h2>Bellman Equation</h2>

<p> <strong>Optimal Q-Function:</strong>
<span class="math display">\[Q^*(s, a) = \mathbb{E} \left[ R + \gamma
\max_{a'} Q^*(s', a') \right]\]</span>
<strong>Interpretation:</strong></p>
<ul>
<li class="fragment"><p>Value = immediate reward + discounted future value</p></li>
<li class="fragment"><p>Recursive definition enables dynamic programming</p></li>
</ul>
<p><strong>Optimal Policy:</strong> <span class="math display">\[\pi^*(s) = \arg\max_a Q^*(s, a)\]</span> <span style="color: gray">Bellman equation: foundation of all value-based RL
methods</span></p>

</section>

<section>
<h2>Q-Learning Algorithm</h2>

<p> <strong>Update Rule:</strong> <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r +
\gamma \max_{a'} Q(s', a') - Q(s, a) \right]\]</span>
<strong>Algorithm:</strong></p>
<ol>
<li class="fragment"><p>Initialize <span class="math inline">\(Q(s,a)\)</span>
arbitrarily</p></li>
<li class="fragment"><p>For each episode:</p>
<ul>
<li class="fragment"><p>Observe state <span class="math inline">\(s\)</span></p></li>
<li class="fragment"><p>Choose action <span class="math inline">\(a\)</span> (<span class="math inline">\(\epsilon\)</span>-greedy)</p></li>
<li class="fragment"><p>Execute <span class="math inline">\(a\)</span>, observe <span class="math inline">\(r\)</span>, <span class="math inline">\(s'\)</span></p></li>
<li class="fragment"><p>Update <span class="math inline">\(Q(s, a)\)</span></p></li>
</ul></li>
</ol>
<p><span style="color: gray">Q-learning is off-policy: learns optimal Q
regardless of policy followed</span></p>

</section>

<section>
<h2>Q-Values Visualization</h2>


<div class="center">
<p><embed data-src="04_q_learning_grid/chart.pdf" style="width:45.0%"/></p>
</div>
<p><span style="color: gray">Arrows show policy; colors show Q-values
(green=high, red=negative)</span></p>

</section>

<section>
<h2>Exploration vs Exploitation</h2>

<p> <strong>The
Dilemma:</strong></p>
<ul>
<li class="fragment"><p><strong>Exploit</strong>: Choose best known action
(greedy)</p></li>
<li class="fragment"><p><strong>Explore</strong>: Try new actions (discover better
options)</p></li>
</ul>
<p><strong><span class="math inline">\(\epsilon\)</span>-Greedy
Strategy:</strong> <span class="math display">\[a = \begin{cases}
\arg\max_a Q(s,a) &amp; \text{with probability } 1-\epsilon \\
\text{random action} &amp; \text{with probability } \epsilon
\end{cases}\]</span> <strong>Decay Schedule:</strong></p>
<ul>
<li class="fragment"><p>Start with high <span class="math inline">\(\epsilon\)</span>
(explore more)</p></li>
<li class="fragment"><p>Decay <span class="math inline">\(\epsilon\)</span> over time
(exploit more)</p></li>
</ul>
<p><span style="color: gray">Balance: too much exploration wastes time;
too little misses optima</span></p>

</section>

<section>
<h2>Learning Curves</h2>


<div class="center">
<p><embed data-src="05_reward_curves/chart.pdf" style="width:55.0%"/></p>
</div>
<p><span style="color: gray">Reward improves as agent learns; DQN often
outperforms tabular Q-learning</span></p>

</section>

<section>
<h2>Part 3: RL for Trading</h2>

<p> <strong>Formulation:</strong></p>
<ul>
<li class="fragment"><p><strong>State</strong>: Price history, portfolio, technical
indicators</p></li>
<li class="fragment"><p><strong>Action</strong>: Buy, sell, hold (+ position
size)</p></li>
<li class="fragment"><p><strong>Reward</strong>: Profit/loss, risk-adjusted
return</p></li>
</ul>
<p><strong>Challenges:</strong></p>
<ul>
<li class="fragment"><p>Non-stationary environment</p></li>
<li class="fragment"><p>High noise, low signal-to-noise ratio</p></li>
<li class="fragment"><p>Transaction costs</p></li>
<li class="fragment"><p>Partial observability</p></li>
</ul>
<p><span style="color: gray">RL for trading is active research area; not
solved problem</span></p>

</section>

<section>
<h2>Trading Policy</h2>


<div class="center">
<p><embed data-src="06_policy_viz/chart.pdf" style="width:55.0%"/></p>
</div>
<p><span style="color: gray">Learned policy: buy when oversold/high
momentum, sell when overbought</span></p>

</section>

<section>
<h2>Deep Q-Networks (DQN)</h2>

<p> <strong>Idea</strong>: Use neural
network to approximate Q-function <span class="math display">\[Q(s, a;
\theta) \approx Q^*(s, a)\]</span> <strong>Key Innovations:</strong></p>
<ul>
<li class="fragment"><p><strong>Experience Replay</strong>: Store transitions, sample
randomly</p></li>
<li class="fragment"><p><strong>Target Network</strong>: Separate network for
stability</p></li>
<li class="fragment"><p><strong>Function Approximation</strong>: Handle large state
spaces</p></li>
</ul>
<p><span style="color: gray">DQN: breakthrough for RL on Atari games
(Mnih et al., 2015)</span></p>

</section>

<section>
<h2>Policy Gradient Methods</h2>

<p> <strong>Direct Policy
Optimization:</strong> <span class="math display">\[\nabla_\theta
J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log
\pi_\theta(a|s) \cdot Q^{\pi_\theta}(s, a) \right]\]</span>
<strong>Algorithms:</strong></p>
<ul>
<li class="fragment"><p>REINFORCE (vanilla policy gradient)</p></li>
<li class="fragment"><p>Actor-Critic (combine value and policy)</p></li>
<li class="fragment"><p>PPO (Proximal Policy Optimization)</p></li>
<li class="fragment"><p>A3C (Asynchronous Advantage Actor-Critic)</p></li>
</ul>
<p><span style="color: gray">Policy gradient: directly optimize policy,
can handle continuous actions</span></p>

</section>

<section>
<h2>Part 4: When to Use What</h2>


<div class="center">
<p><embed data-src="07_decision_flowchart/chart.pdf" style="width:50.0%"/></p>
</div>
<p><span style="color: gray">Embeddings for text/categorical; RL for
sequential decisions</span></p>

</section>

<section>
<h2>Comparison Table</h2>


<div class="center">
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: center;"><strong>Embeddings</strong></th>
<th style="text-align: center;"><strong>RL</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Input</td>
<td style="text-align: center;">Text, categorical</td>
<td style="text-align: center;">State sequence</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: center;">Dense vectors</td>
<td style="text-align: center;">Actions/policy</td>
</tr>
<tr>
<td style="text-align: left;">Learning</td>
<td style="text-align: center;">Unsupervised/supervised</td>
<td style="text-align: center;">Trial and error</td>
</tr>
<tr>
<td style="text-align: left;">Signal</td>
<td style="text-align: center;">Context (words)</td>
<td style="text-align: center;">Rewards</td>
</tr>
<tr>
<td style="text-align: left;">Key challenge</td>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Credit assignment</td>
</tr>
<tr>
<td style="text-align: left;">Finance use</td>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">Trading</td>
</tr>
</tbody>
</table>
</div>
<p><span style="color: gray">Both transform complex inputs into
learnable representations</span></p>

</section>

<section>
<h2>Part 5: Implementation</h2>

<p> <strong>Embeddings in
Python:</strong></p>
<ul>
<li class="fragment"><p><code>gensim.models.Word2Vec</code>: Train your own</p></li>
<li class="fragment"><p><code>gensim.downloader.load(’glove-wiki-gigaword-100’)</code>:
Pre-trained</p></li>
<li class="fragment"><p><code>transformers.BertModel</code>: BERT embeddings</p></li>
</ul>
<p><strong>RL Libraries:</strong></p>
<ul>
<li class="fragment"><p><code>gymnasium</code>: Environment interface (formerly OpenAI
Gym)</p></li>
<li class="fragment"><p><code>stable-baselines3</code>: Pre-implemented
algorithms</p></li>
<li class="fragment"><p><code>ray[rllib]</code>: Scalable RL</p></li>
</ul>
<p><span style="color: gray">Start with pre-trained embeddings; use
stable-baselines3 for RL</span></p>

</section>

<section>
<h2>Practical Tips</h2>

<p> <strong>Embeddings:</strong></p>
<ul>
<li class="fragment"><p>Start with pre-trained, fine-tune if needed</p></li>
<li class="fragment"><p>Check domain match (general vs financial)</p></li>
<li class="fragment"><p>Visualize with t-SNE/UMAP to verify quality</p></li>
</ul>
<p><strong>RL:</strong></p>
<ul>
<li class="fragment"><p>Start simple (tabular Q-learning before DQN)</p></li>
<li class="fragment"><p>Reward shaping is crucial (sparse rewards are hard)</p></li>
<li class="fragment"><p>Normalize observations</p></li>
<li class="fragment"><p>Use established environments first (Gym, FinRL)</p></li>
</ul>
<p><span style="color: gray">Both domains: start simple, iterate,
validate thoroughly</span></p>

</section>

<section>
<h2>Summary</h2>

<p> <strong>Embeddings:</strong></p>
<ul>
<li class="fragment"><p>Dense vector representations of text/categories</p></li>
<li class="fragment"><p>Capture semantic similarity</p></li>
<li class="fragment"><p>Use pre-trained (Word2Vec, GloVe, BERT)</p></li>
</ul>
<p><strong>Reinforcement Learning:</strong></p>
<ul>
<li class="fragment"><p>Agent learns from environment interaction</p></li>
<li class="fragment"><p>Q-learning: value-based, tabular or deep (DQN)</p></li>
<li class="fragment"><p>Applications: trading, portfolio optimization</p></li>
</ul>
<p><strong>Key Takeaway:</strong> Different tools for different problems
<span style="color: gray">Course complete! Apply these methods in your
capstone project</span></p>

</section>

<section>
<h2>References</h2>

<p> <strong>Embeddings:</strong></p>
<ul>
<li class="fragment"><p>Mikolov et al. (2013). Word2Vec</p></li>
<li class="fragment"><p>Pennington et al. (2014). GloVe</p></li>
<li class="fragment"><p>Devlin et al. (2019). BERT</p></li>
</ul>
<p><strong>Reinforcement Learning:</strong></p>
<ul>
<li class="fragment"><p>Sutton &amp; Barto (2018). RL: An Introduction (free
online)</p></li>
<li class="fragment"><p>Mnih et al. (2015). DQN (Atari)</p></li>
<li class="fragment"><p>Schulman et al. (2017). PPO</p></li>
</ul>
<p><strong>Finance Applications:</strong></p>
<ul>
<li class="fragment"><p>Liu et al. (2021). FinRL: Deep RL for Trading</p></li>
<li class="fragment"><p>Araci (2019). FinBERT</p></li>
</ul>
<p><span style="color: gray">Sutton &amp; Barto: the definitive RL
textbook (free at incompleteideas.net)</span></p>

</section>
        </div>

        <!-- Custom footer -->
        <div class="slide-footer">
            <div class="footer-left">Methods and Algorithms</div>
            <div class="footer-center">MSc Data Science</div>
            <div class="footer-right"></div>
        </div>
    </div>

    <!-- Reveal.js core -->
    <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

    <!-- Reveal.js plugins -->
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>

    <!-- Menu plugin -->
    <script src="https://unpkg.com/reveal.js-menu@2.1.0/menu.js"></script>

    <!-- Chalkboard plugin -->
    <script src="https://unpkg.com/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

    <script>
        Reveal.initialize({
            // Display settings
            controls: true,
            controlsTutorial: true,
            controlsLayout: 'bottom-right',
            progress: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            hash: true,
            history: true,

            // Navigation
            keyboard: true,
            overview: true,
            touch: true,
            loop: false,
            navigationMode: 'default',

            // Transitions
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',

            // Sizing (16:9 aspect ratio)
            width: 1600,
            height: 900,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            center: false,

            // Fragments
            fragments: true,
            fragmentInURL: false,

            // Auto-slide (disabled)
            autoSlide: 0,

            // Math configuration
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                tex: {
                    inlineMath: [['\\(', '\\)']],
                    displayMath: [['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            },

            // Menu plugin configuration
            menu: {
                side: 'left',
                width: 'normal',
                numbers: true,
                titleSelector: 'h1, h2',
                useTextContentForMissingTitles: true,
                hideMissingTitles: false,
                markers: true,
                custom: false,
                themes: [
                    { name: 'ML Theme (Light)', theme: 'css/ml-theme.css' },
                    { name: 'Black', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/black.css' },
                    { name: 'White', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css' }
                ],
                transitions: true,
                openButton: true,
                openSlideNumber: false,
                keyboard: true,
                sticky: false,
                autoOpen: true
            },

            // Chalkboard plugin configuration
            chalkboard: {
                boardmarkerWidth: 3,
                chalkWidth: 4,
                chalkEffect: 0.5,
                storage: null,
                src: null,
                readOnly: false,
                toggleChalkboardButton: { left: "30px", bottom: "30px", top: "auto", right: "auto" },
                toggleNotesButton: { left: "70px", bottom: "30px", top: "auto", right: "auto" },
                colorButtons: true,
                boardHandle: true,
                transition: 800,
                theme: "chalkboard"
            },

            // Plugins to load
            plugins: [
                RevealMath,
                RevealNotes,
                RevealSearch,
                RevealZoom,
                RevealHighlight,
                RevealMenu,
                RevealChalkboard
            ]
        });

        // Update footer with slide number
        Reveal.on('slidechanged', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const current = event.indexh + 1;
                const total = Reveal.getTotalSlides();
                footer.textContent = `${current} / ${total}`;
            }
        });

        // Initial footer update
        Reveal.on('ready', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const total = Reveal.getTotalSlides();
                footer.textContent = `1 / ${total}`;
            }
        });

        // Keyboard shortcuts info
        console.log('Keyboard shortcuts:');
        console.log('  S - Speaker notes');
        console.log('  B - Chalkboard');
        console.log('  C - Canvas (draw on slide)');
        console.log('  M - Menu');
        console.log('  O - Overview');
        console.log('  F - Fullscreen');
        console.log('  ? - Help');
    </script>
</body>
</html>
