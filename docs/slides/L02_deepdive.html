<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="pandoc + custom post-processor">
    <meta name="author" content="Methods and Algorithms – MSc Data Science">
    <title>L02: Logistic Regression</title>

    <!-- Reveal.js core -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">

    <!-- Custom ML theme -->
    <link rel="stylesheet" href="css/ml-theme.css">

    <!-- Menu plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-menu@2.1.0/menu.css">

    <!-- Chalkboard plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-plugins@latest/chalkboard/style.css">

    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        /* Additional inline styles */
        .reveal .sourceCode {
            overflow: visible;
        }
        code {
            white-space: pre-wrap;
        }
        span.smallcaps {
            font-variant: small-caps;
        }
        div.columns {
            display: flex;
            gap: min(4vw, 1.5em);
        }
        div.column {
            flex: auto;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section id="title-slide">
<h1 class="title">L02: Logistic Regression</h1>
<p class="subtitle">Mathematical Foundations and Implementation</p>
<p class="author">Methods and Algorithms – MSc Data Science</p>
</section>

<section>
<h2>From Linear to Logistic</h2>

<p> <strong>The Classification
Problem</strong></p>
<ul>
<li class="fragment"><p>Given features <span class="math inline">\(\mathbf{x} \in
\mathbb{R}^p\)</span>, predict <span class="math inline">\(y \in \{0,
1\}\)</span></p></li>
<li class="fragment"><p>Linear regression: <span class="math inline">\(\hat{y} =
\mathbf{w}^T\mathbf{x} + b\)</span> (unbounded)</p></li>
<li class="fragment"><p>Need: <span class="math inline">\(P(y=1|\mathbf{x}) \in [0,
1]\)</span></p></li>
</ul>
<p><strong>Solution: The Logistic Function</strong> <span class="math display">\[P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x}
+ b) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}\]</span> <span style="color: gray">The logistic function is also called the sigmoid
function</span></p>

</section>

<section>
<h2>The Sigmoid Function</h2>


<div class="center">
<p><embed data-src="01_sigmoid_function/chart.pdf" style="width:55.0%"/></p>
</div>
<p><strong>Key Properties:</strong></p>
<ul>
<li class="fragment"><p>Range: <span class="math inline">\((0, 1)\)</span> – perfect for
probabilities</p></li>
<li class="fragment"><p><span class="math inline">\(\sigma(0) = 0.5\)</span> – threshold
for classification</p></li>
<li class="fragment"><p><span class="math inline">\(\sigma'(z) = \sigma(z)(1 -
\sigma(z))\)</span> – simple gradient</p></li>
</ul>

</section>

<section>
<h2>Odds and Log-Odds</h2>

<p> <strong>Understanding the
Model</strong></p>
<ul>
<li class="fragment"><p>Odds: <span class="math inline">\(\frac{P(y=1)}{P(y=0)} =
\frac{p}{1-p}\)</span></p></li>
<li class="fragment"><p>Log-odds (logit): <span class="math inline">\(\log\left(\frac{p}{1-p}\right) =
\mathbf{w}^T\mathbf{x} + b\)</span></p></li>
</ul>
<p><strong>Coefficient Interpretation</strong></p>
<ul>
<li class="fragment"><p><span class="math inline">\(w_j\)</span>: change in log-odds per
unit increase in <span class="math inline">\(x_j\)</span></p></li>
<li class="fragment"><p><span class="math inline">\(e^{w_j}\)</span>: odds ratio –
multiplicative effect on odds</p></li>
<li class="fragment"><p>Example: <span class="math inline">\(w_{\text{income}} = 0.5
\Rightarrow\)</span> each unit increase in income multiplies odds by
<span class="math inline">\(e^{0.5} \approx 1.65\)</span></p></li>
</ul>
<p><span style="color: gray">Log-odds interpretation is key for
regulatory compliance in banking</span></p>

</section>

<section>
<h2>Maximum Likelihood Estimation</h2>

<p> <strong>The Likelihood
Function</strong></p>
<p>For observations <span class="math inline">\((x_i, y_i)\)</span>, the
likelihood is: <span class="math display">\[L(\mathbf{w}) =
\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}\]</span> where <span class="math inline">\(p_i = \sigma(\mathbf{w}^T\mathbf{x}_i +
b)\)</span></p>
<p><strong>Log-Likelihood (easier to optimize)</strong> <span class="math display">\[\ell(\mathbf{w}) = \sum_{i=1}^{n} \left[ y_i
\log(p_i) + (1-y_i)\log(1-p_i) \right]\]</span> <span style="color: gray">Maximize log-likelihood = minimize negative
log-likelihood (cross-entropy)</span></p>

</section>

<section>
<h2>Binary Cross-Entropy Loss</h2>


<div class="center">
<p><embed data-src="03_log_loss/chart.pdf" style="width:55.0%"/></p>
</div>
<p><strong>Loss Function</strong> <span class="math display">\[\mathcal{L} =
-\frac{1}{n}\sum_{i=1}^{n}\left[y_i\log(p_i) +
(1-y_i)\log(1-p_i)\right]\]</span> <span style="color: gray">Cross-entropy loss is convex in the weights –
guaranteed global optimum</span></p>

</section>

<section>
<h2>Gradient Derivation</h2>

<p> <strong>Computing the
Gradient</strong></p>
<p>For a single sample: <span class="math display">\[\frac{\partial
\mathcal{L}}{\partial w_j} = (p - y) x_j\]</span> <strong>In Matrix
Form</strong> <span class="math display">\[\nabla_{\mathbf{w}}
\mathcal{L} = \frac{1}{n}\mathbf{X}^T(\mathbf{p} - \mathbf{y})\]</span>
where <span class="math inline">\(\mathbf{p} =
\sigma(\mathbf{X}\mathbf{w})\)</span></p>
<p><strong>Key Insight</strong>: Same form as linear regression
gradient! <span style="color: gray">The elegance of logistic regression:
gradient has the same form as linear regression</span></p>

</section>

<section>
<h2>Gradient Descent Update</h2>

<p> <strong>Update Rule</strong>
<span class="math display">\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} -
\eta \nabla_{\mathbf{w}} \mathcal{L}\]</span> <span class="math display">\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} -
\frac{\eta}{n}\mathbf{X}^T(\sigma(\mathbf{X}\mathbf{w}^{(t)}) -
\mathbf{y})\]</span> <strong>Practical Considerations</strong></p>
<ul>
<li class="fragment"><p>Feature scaling: standardize inputs for faster
convergence</p></li>
<li class="fragment"><p>Learning rate: start with <span class="math inline">\(\eta =
0.01\)</span>, use line search or decay</p></li>
<li class="fragment"><p>Convergence: monitor loss, check gradient norm &lt;
tolerance</p></li>
</ul>
<p><span style="color: gray">No closed-form solution like normal
equation – must use iterative optimization</span></p>

</section>

<section>
<h2>Linear Decision Boundary</h2>


<div class="center">
<p><embed data-src="02_decision_boundary/chart.pdf" style="width:55.0%"/></p>
</div>
<p><strong>Decision Rule</strong>: Predict <span class="math inline">\(\hat{y} = 1\)</span> if <span class="math inline">\(\mathbf{w}^T\mathbf{x} + b \geq 0\)</span> <span style="color: gray">The decision boundary is always a hyperplane in the
feature space</span></p>

</section>

<section>
<h2>Threshold Selection</h2>

<p> <strong>Default Threshold:
0.5</strong></p>
<ul>
<li class="fragment"><p>Predict 1 if <span class="math inline">\(P(y=1|\mathbf{x}) \geq
0.5\)</span></p></li>
<li class="fragment"><p>Equivalent to: <span class="math inline">\(\mathbf{w}^T\mathbf{x}
+ b \geq 0\)</span></p></li>
</ul>
<p><strong>Custom Thresholds</strong></p>
<ul>
<li class="fragment"><p>Lower threshold: more sensitive (higher recall)</p></li>
<li class="fragment"><p>Higher threshold: more specific (higher precision)</p></li>
<li class="fragment"><p>Choose based on business costs of FP vs FN</p></li>
</ul>
<p><strong>Example: Fraud Detection</strong></p>
<ul>
<li class="fragment"><p>Cost of missing fraud (FN) &gt;&gt; Cost of false alarm
(FP)</p></li>
<li class="fragment"><p>Use lower threshold, e.g., 0.3</p></li>
</ul>
<p><span style="color: gray">Optimal threshold depends on the cost
matrix of your application</span></p>

</section>

<section>
<h2>Non-Linear Boundaries via Features</h2>

<p> <strong>Polynomial
Features</strong></p>
<ul>
<li class="fragment"><p>Original: <span class="math inline">\([x_1,
x_2]\)</span></p></li>
<li class="fragment"><p>Expanded: <span class="math inline">\([x_1, x_2, x_1^2, x_2^2,
x_1 x_2]\)</span></p></li>
<li class="fragment"><p>Creates curved decision boundaries</p></li>
</ul>
<p><strong>Trade-offs</strong></p>
<ul>
<li class="fragment"><p>More features: more flexible boundaries</p></li>
<li class="fragment"><p>Risk: overfitting to training data</p></li>
<li class="fragment"><p>Solution: regularization</p></li>
</ul>
<p><span style="color: gray">Logistic regression is linear in
parameters, but can model non-linear boundaries</span></p>

</section>

<section>
<h2>Multiclass Extension: Softmax</h2>

<p> <strong>One-vs-Rest
(OvR)</strong></p>
<ul>
<li class="fragment"><p>Train <span class="math inline">\(K\)</span> binary
classifiers</p></li>
<li class="fragment"><p>Predict class with highest probability</p></li>
</ul>
<p><strong>Multinomial (Softmax) Logistic Regression</strong> <span class="math display">\[P(y=k|\mathbf{x}) =
\frac{e^{\mathbf{w}_k^T\mathbf{x}}}{\sum_{j=1}^{K}e^{\mathbf{w}_j^T\mathbf{x}}}\]</span></p>
<ul>
<li class="fragment"><p>Single model, probabilities sum to 1</p></li>
<li class="fragment"><p>Loss: categorical cross-entropy</p></li>
</ul>
<p><span style="color: gray">scikit-learn: multi_class=’multinomial’ for
true softmax regression</span></p>

</section>

<section>
<h2>Confusion Matrix</h2>


<div class="center">
<p><embed data-src="06_confusion_matrix/chart.pdf" style="width:55.0%"/></p>
</div>
<p><span style="color: gray">Always start evaluation by examining the
confusion matrix</span></p>

</section>

<section>
<h2>Classification Metrics</h2>

<p> <strong>From the Confusion
Matrix</strong></p>
<ul>
<li class="fragment"><p><strong>Accuracy</strong>: <span class="math inline">\(\frac{TP +
TN}{TP + TN + FP + FN}\)</span> – overall correctness</p></li>
<li class="fragment"><p><strong>Precision</strong>: <span class="math inline">\(\frac{TP}{TP + FP}\)</span> – of predicted
positives, how many correct?</p></li>
<li class="fragment"><p><strong>Recall</strong>: <span class="math inline">\(\frac{TP}{TP
+ FN}\)</span> – of actual positives, how many found?</p></li>
<li class="fragment"><p><strong>F1 Score</strong>: <span class="math inline">\(\frac{2
\cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} +
\text{Recall}}\)</span></p></li>
</ul>
<p><strong>When Accuracy Fails</strong></p>
<ul>
<li class="fragment"><p>Imbalanced data: 99% negative class</p></li>
<li class="fragment"><p>Predicting all negatives gives 99% accuracy!</p></li>
</ul>
<p><span style="color: gray">Accuracy is misleading for imbalanced
datasets</span></p>

</section>

<section>
<h2>ROC Curve</h2>


<div class="center">
<p><embed data-src="04_roc_curve/chart.pdf" style="width:55.0%"/></p>
</div>
<p><strong>ROC = Receiver Operating Characteristic</strong></p>
<ul>
<li class="fragment"><p>X-axis: FPR (1 - Specificity)</p></li>
<li class="fragment"><p>Y-axis: TPR (Recall/Sensitivity)</p></li>
</ul>

</section>

<section>
<h2>Area Under ROC (AUC)</h2>

<p> <strong>Interpretation</strong></p>
<ul>
<li class="fragment"><p>AUC = 0.5: random guessing</p></li>
<li class="fragment"><p>AUC = 1.0: perfect classifier</p></li>
<li class="fragment"><p>AUC = probability(random positive &gt; random negative)</p></li>
</ul>
<p><strong>Guidelines</strong></p>
<ul>
<li class="fragment"><p>0.9–1.0: Excellent</p></li>
<li class="fragment"><p>0.8–0.9: Good</p></li>
<li class="fragment"><p>0.7–0.8: Fair</p></li>
<li class="fragment"><p>0.6–0.7: Poor</p></li>
<li class="fragment"><p>0.5–0.6: Fail</p></li>
</ul>
<p><span style="color: gray">AUC is threshold-independent – summarizes
performance across all thresholds</span></p>

</section>

<section>
<h2>Precision-Recall Curve</h2>


<div class="center">
<p><embed data-src="05_precision_recall/chart.pdf" style="width:55.0%"/></p>
</div>
<p><span style="color: gray">Use PR curves for imbalanced datasets where
positive class is rare</span></p>

</section>

<section>
<h2>ROC vs Precision-Recall</h2>

<p> <strong>When to Use
ROC</strong></p>
<ul>
<li class="fragment"><p>Balanced classes</p></li>
<li class="fragment"><p>Care equally about both classes</p></li>
<li class="fragment"><p>Comparing models at specific FPR</p></li>
</ul>
<p><strong>When to Use Precision-Recall</strong></p>
<ul>
<li class="fragment"><p>Imbalanced classes (fraud, disease)</p></li>
<li class="fragment"><p>Positive class is more important</p></li>
<li class="fragment"><p>High precision required</p></li>
</ul>
<p><span style="color: gray">ROC can be overly optimistic with
imbalanced data</span></p>

</section>

<section>
<h2>Calibration</h2>

<p> <strong>What is Calibration?</strong></p>
<ul>
<li class="fragment"><p>Predicted 70% probability should mean 70% actually
positive</p></li>
<li class="fragment"><p>Well-calibrated: predicted probabilities match observed
frequencies</p></li>
</ul>
<p><strong>Checking Calibration</strong></p>
<ul>
<li class="fragment"><p>Reliability diagram (calibration plot)</p></li>
<li class="fragment"><p>Brier score: <span class="math inline">\(\frac{1}{n}\sum(p_i -
y_i)^2\)</span></p></li>
</ul>
<p><strong>Logistic Regression Advantage</strong></p>
<ul>
<li class="fragment"><p>Naturally well-calibrated (MLE property)</p></li>
<li class="fragment"><p>Unlike trees/random forests that may need calibration</p></li>
</ul>
<p><span style="color: gray">Calibration is crucial when probabilities
are used for decision-making</span></p>

</section>

<section>
<h2>Regularization Motivation</h2>

<p> <strong>The Overfitting
Problem</strong></p>
<ul>
<li class="fragment"><p>Many features, limited data</p></li>
<li class="fragment"><p>Model fits noise, not signal</p></li>
<li class="fragment"><p>Perfect training accuracy, poor test performance</p></li>
</ul>
<p><strong>Solution: Penalize Large Coefficients</strong> <span class="math display">\[\mathcal{L}_{\text{regularized}} = \mathcal{L} +
\lambda \cdot \text{penalty}(\mathbf{w})\]</span></p>
<ul>
<li class="fragment"><p><span class="math inline">\(\lambda\)</span>: regularization
strength (hyperparameter)</p></li>
<li class="fragment"><p>Larger <span class="math inline">\(\lambda\)</span> = simpler
model</p></li>
</ul>
<p><span style="color: gray">Regularization trades bias for
variance</span></p>

</section>

<section>
<h2>L1 vs L2 Regularization</h2>

<p> <strong>L2 (Ridge)</strong>
<span class="math display">\[\mathcal{L}_{\text{Ridge}} = \mathcal{L} +
\lambda \sum_{j=1}^{p} w_j^2\]</span></p>
<ul>
<li class="fragment"><p>Shrinks coefficients toward zero</p></li>
<li class="fragment"><p>Keeps all features, reduces magnitude</p></li>
</ul>
<p><strong>L1 (Lasso)</strong> <span class="math display">\[\mathcal{L}_{\text{Lasso}} = \mathcal{L} +
\lambda \sum_{j=1}^{p} |w_j|\]</span></p>
<ul>
<li class="fragment"><p>Some coefficients exactly zero</p></li>
<li class="fragment"><p>Automatic feature selection</p></li>
</ul>
<p><span style="color: gray">L1 for sparse models, L2 when all features
likely relevant</span></p>

</section>

<section>
<h2>Elastic Net</h2>

<p> <strong>Best of Both Worlds</strong> <span class="math display">\[\mathcal{L}_{\text{ElasticNet}} = \mathcal{L} +
\lambda_1 \sum |w_j| + \lambda_2 \sum w_j^2\]</span>
<strong>Advantages</strong></p>
<ul>
<li class="fragment"><p>Handles correlated features better than Lasso alone</p></li>
<li class="fragment"><p>Can select groups of correlated features</p></li>
<li class="fragment"><p>More stable feature selection</p></li>
</ul>
<p><strong>In scikit-learn</strong></p>
<ul>
<li class="fragment"><p><code>LogisticRegression(penalty=’elasticnet’, solver=’saga’, l1_ratio=0.5)</code></p></li>
</ul>
<p><span style="color: gray">Elastic Net: l1_ratio = 1 is pure L1,
l1_ratio = 0 is pure L2</span></p>

</section>

<section>
<h2>Choosing\(\lambda\)</h2>

<p>
<strong>Cross-Validation</strong></p>
<ul>
<li class="fragment"><p>Try grid of <span class="math inline">\(\lambda\)</span> values:
[0.001, 0.01, 0.1, 1, 10, 100]</p></li>
<li class="fragment"><p>Use k-fold CV to estimate test performance</p></li>
<li class="fragment"><p>Select <span class="math inline">\(\lambda\)</span> with best CV
score</p></li>
</ul>
<p><strong>scikit-learn Convenience</strong></p>
<ul>
<li class="fragment"><p><code>LogisticRegressionCV</code>: automatic <span class="math inline">\(\lambda\)</span> search</p></li>
<li class="fragment"><p><code>Cs</code>: inverse of <span class="math inline">\(\lambda\)</span> (larger C = less
regularization)</p></li>
</ul>
<p><span style="color: gray">LogisticRegressionCV does cross-validation
internally</span></p>

</section>

<section>
<h2>Algorithm: Gradient Descent</h2>


<div class="algorithmic">
<p><strong>Input</strong>: <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{y}\)</span>, learning rate <span class="math inline">\(\eta\)</span>, max iterations <span class="math inline">\(T\)</span> Initialize <span class="math inline">\(\mathbf{w} = \mathbf{0}\)</span> <span class="math inline">\(\mathbf{p} = \sigma(\mathbf{X}\mathbf{w})\)</span>
<span class="math inline">\(\nabla = \frac{1}{n}\mathbf{X}^T(\mathbf{p}
- \mathbf{y})\)</span> <span class="math inline">\(\mathbf{w} =
\mathbf{w} - \eta \nabla\)</span> <strong>break</strong>
<strong>return</strong> <span class="math inline">\(\mathbf{w}\)</span></p>
</div>
<p><span style="color: gray">In practice, use quasi-Newton methods
(L-BFGS) for faster convergence</span></p>

</section>

<section>
<h2>scikit-learn Implementation</h2>

<p> <strong>Basic
Usage</strong></p>
<ul>
<li class="fragment"><p><code>from sklearn.linear_model import LogisticRegression</code></p></li>
<li class="fragment"><p><code>model = LogisticRegression()</code></p></li>
<li class="fragment"><p><code>model.fit(X_train, y_train)</code></p></li>
<li class="fragment"><p><code>y_pred = model.predict(X_test)</code></p></li>
<li class="fragment"><p><code>y_proba = model.predict_proba(X_test)</code></p></li>
</ul>
<p><strong>Key Parameters</strong></p>
<ul>
<li class="fragment"><p><code>C</code>: inverse regularization strength
(default=1.0)</p></li>
<li class="fragment"><p><code>penalty</code>: ’l1’, ’l2’, ’elasticnet’, ’none’</p></li>
<li class="fragment"><p><code>solver</code>: ’lbfgs’, ’liblinear’, ’saga’</p></li>
<li class="fragment"><p><code>class_weight</code>: ’balanced’ for imbalanced
data</p></li>
</ul>
<p><span style="color: gray">predict_proba returns [P(y=0), P(y=1)] –
use [:, 1] for positive class</span></p>

</section>

<section>
<h2>Handling Class Imbalance</h2>

<p> <strong>The
Problem</strong></p>
<ul>
<li class="fragment"><p>99% negatives, 1% positives</p></li>
<li class="fragment"><p>Model predicts all negatives: 99% accuracy!</p></li>
</ul>
<p><strong>Solutions</strong></p>
<ul>
<li class="fragment"><p><strong>Class weights</strong>:
<code>class_weight=’balanced’</code></p></li>
<li class="fragment"><p><strong>Oversampling</strong>: SMOTE, random
oversampling</p></li>
<li class="fragment"><p><strong>Undersampling</strong>: random undersampling</p></li>
<li class="fragment"><p><strong>Threshold tuning</strong>: optimize for F1 or business
metric</p></li>
</ul>
<p><strong>Weighted Loss</strong> <span class="math display">\[\mathcal{L}_{\text{weighted}} = -\sum_i
w_{y_i}[y_i\log p_i + (1-y_i)\log(1-p_i)]\]</span> <span style="color: gray">class_weight=’balanced’ sets <span class="math inline">\(w_k \propto 1/n_k\)</span></span></p>

</section>

<section>
<h2>Feature Engineering Tips</h2>

<p> <strong>For Logistic
Regression</strong></p>
<ul>
<li class="fragment"><p><strong>Standardization</strong>: mean=0, std=1 for all
features</p></li>
<li class="fragment"><p><strong>Missing values</strong>: impute or create indicator
variable</p></li>
<li class="fragment"><p><strong>Categorical</strong>: one-hot encoding (drop one
level)</p></li>
<li class="fragment"><p><strong>Interactions</strong>: <span class="math inline">\(x_1
\times x_2\)</span> if domain suggests</p></li>
<li class="fragment"><p><strong>Non-linearity</strong>: binning or polynomial
features</p></li>
</ul>
<p><strong>Credit Scoring Example</strong></p>
<ul>
<li class="fragment"><p>Age: may have non-linear effect (bin into groups)</p></li>
<li class="fragment"><p>Debt-to-income ratio: interaction of two features</p></li>
<li class="fragment"><p>Employment length: indicator for &lt; 2 years</p></li>
</ul>
<p><span style="color: gray">Feature engineering often matters more than
model selection</span></p>

</section>

<section>
<h2>Model Interpretation</h2>

<p> <strong>Coefficient
Analysis</strong></p>
<ul>
<li class="fragment"><p>Sign: direction of effect</p></li>
<li class="fragment"><p>Magnitude: strength (after standardization)</p></li>
<li class="fragment"><p>Odds ratio <span class="math inline">\(e^{w_j}\)</span>:
multiplicative effect</p></li>
</ul>
<p><strong>Example Interpretation</strong></p>
<ul>
<li class="fragment"><p><span class="math inline">\(w_{\text{income}} = 0.5\)</span>:
each $1000 income increase multiplies odds of approval by <span class="math inline">\(e^{0.5} = 1.65\)</span></p></li>
<li class="fragment"><p><span class="math inline">\(w_{\text{debt\_ratio}} =
-1.2\)</span>: each 0.1 increase in debt ratio multiplies odds by <span class="math inline">\(e^{-0.12} = 0.89\)</span> (11% decrease)</p></li>
</ul>
<p><span style="color: gray">This interpretability makes logistic
regression preferred in regulated industries</span></p>

</section>

<section>
<h2>Solver Selection</h2>

<p> <strong>Available Solvers in
scikit-learn</strong></p>
<ul>
<li class="fragment"><p><code>lbfgs</code>: default, works for L2 and no penalty</p></li>
<li class="fragment"><p><code>liblinear</code>: fast for small data, supports L1</p></li>
<li class="fragment"><p><code>saga</code>: supports all penalties, works for large
data</p></li>
<li class="fragment"><p><code>newton-cg</code>: similar to lbfgs</p></li>
<li class="fragment"><p><code>sag</code>: stochastic, for very large data</p></li>
</ul>
<p><strong>Guidelines</strong></p>
<ul>
<li class="fragment"><p>L1 penalty: use liblinear or saga</p></li>
<li class="fragment"><p>Large data: saga or sag</p></li>
<li class="fragment"><p>Default (L2): lbfgs</p></li>
</ul>
<p><span style="color: gray">solver=’saga’ is the most versatile but may
be slower for small datasets</span></p>

</section>

<section>
<h2>Convergence Issues</h2>

<p> <strong>Warning: “Convergence
Warning”</strong></p>
<ul>
<li class="fragment"><p>Model did not converge in max_iter iterations</p></li>
<li class="fragment"><p>May mean poor solution</p></li>
</ul>
<p><strong>Solutions</strong></p>
<ul>
<li class="fragment"><p>Increase <code>max_iter</code> (default=100)</p></li>
<li class="fragment"><p>Standardize features</p></li>
<li class="fragment"><p>Increase regularization (smaller C)</p></li>
<li class="fragment"><p>Use different solver</p></li>
</ul>
<p><span style="color: gray">Always check for convergence warnings in
production code</span></p>

</section>

<section>
<h2>Logistic vs Other Classifiers</h2>

<p> <strong>Strengths of
Logistic Regression</strong></p>
<ul>
<li class="fragment"><p>Interpretable coefficients</p></li>
<li class="fragment"><p>Well-calibrated probabilities</p></li>
<li class="fragment"><p>Fast training and prediction</p></li>
<li class="fragment"><p>Works well with few samples</p></li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li class="fragment"><p>Linear decision boundary</p></li>
<li class="fragment"><p>May underfit complex patterns</p></li>
<li class="fragment"><p>Sensitive to outliers (compared to trees)</p></li>
</ul>
<p><strong>When to Choose Alternatives</strong></p>
<ul>
<li class="fragment"><p>Complex patterns: Random Forests, Gradient Boosting</p></li>
<li class="fragment"><p>High-dimensional: SVM with RBF kernel</p></li>
<li class="fragment"><p>Interpretability not required: Neural Networks</p></li>
</ul>
<p><span style="color: gray">Start with logistic regression as baseline,
then try more complex models</span></p>

</section>

<section>
<h2>Decision Framework</h2>


<div class="center">
<p><embed data-src="07_decision_flowchart/chart.pdf" style="width:60.0%"/></p>
</div>
<p><span style="color: gray">Logistic regression: first choice for
binary classification with interpretability</span></p>

</section>

<section>
<h2>Key Takeaways</h2>

<p> <strong>Mathematical
Foundation</strong></p>
<ul>
<li class="fragment"><p>Sigmoid function maps linear combination to probability</p></li>
<li class="fragment"><p>Maximum likelihood estimation via gradient descent</p></li>
<li class="fragment"><p>Cross-entropy loss is convex, guaranteed global optimum</p></li>
</ul>
<p><strong>Evaluation</strong></p>
<ul>
<li class="fragment"><p>Use confusion matrix, precision, recall, F1</p></li>
<li class="fragment"><p>ROC/AUC for balanced data, PR curve for imbalanced</p></li>
<li class="fragment"><p>Calibration matters when using probabilities</p></li>
</ul>
<p><strong>Practice</strong></p>
<ul>
<li class="fragment"><p>Regularization prevents overfitting</p></li>
<li class="fragment"><p>Class weights handle imbalance</p></li>
<li class="fragment"><p>Coefficients are directly interpretable</p></li>
</ul>
<p><span style="color: gray">Logistic regression: simple, fast,
interpretable, and often competitive</span></p>

</section>

<section>
<h2>References</h2>

<p> <strong>Textbooks</strong></p>
<ul>
<li class="fragment"><p>James et al. (2021). <em>ISLR</em>, Chapter 4:
Classification</p></li>
<li class="fragment"><p>Hastie et al. (2009). <em>ESL</em>, Chapter 4: Linear
Methods</p></li>
</ul>
<p><strong>Documentation</strong></p>
<ul>
<li class="fragment"><p>scikit-learn: LogisticRegression user guide</p></li>
<li class="fragment"><p>statsmodels: Logit for statistical inference</p></li>
</ul>
<p><strong>Next Lecture</strong></p>
<ul>
<li class="fragment"><p>L03: KNN and K-Means</p></li>
<li class="fragment"><p>From parametric to non-parametric methods</p></li>
</ul>

</section>
        </div>

        <!-- Custom footer -->
        <div class="slide-footer">
            <div class="footer-left">Methods and Algorithms</div>
            <div class="footer-center">MSc Data Science</div>
            <div class="footer-right"></div>
        </div>
    </div>

    <!-- Reveal.js core -->
    <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

    <!-- Reveal.js plugins -->
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>

    <!-- Menu plugin -->
    <script src="https://unpkg.com/reveal.js-menu@2.1.0/menu.js"></script>

    <!-- Chalkboard plugin -->
    <script src="https://unpkg.com/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

    <script>
        Reveal.initialize({
            // Display settings
            controls: true,
            controlsTutorial: true,
            controlsLayout: 'bottom-right',
            progress: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            hash: true,
            history: true,

            // Navigation
            keyboard: true,
            overview: true,
            touch: true,
            loop: false,
            navigationMode: 'default',

            // Transitions
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',

            // Sizing (16:9 aspect ratio)
            width: 1600,
            height: 900,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            center: false,

            // Fragments
            fragments: true,
            fragmentInURL: false,

            // Auto-slide (disabled)
            autoSlide: 0,

            // Math configuration
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                tex: {
                    inlineMath: [['\\(', '\\)']],
                    displayMath: [['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            },

            // Menu plugin configuration
            menu: {
                side: 'left',
                width: 'normal',
                numbers: true,
                titleSelector: 'h1, h2',
                useTextContentForMissingTitles: true,
                hideMissingTitles: false,
                markers: true,
                custom: false,
                themes: [
                    { name: 'ML Theme (Light)', theme: 'css/ml-theme.css' },
                    { name: 'Black', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/black.css' },
                    { name: 'White', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css' }
                ],
                transitions: true,
                openButton: true,
                openSlideNumber: false,
                keyboard: true,
                sticky: false,
                autoOpen: true
            },

            // Chalkboard plugin configuration
            chalkboard: {
                boardmarkerWidth: 3,
                chalkWidth: 4,
                chalkEffect: 0.5,
                storage: null,
                src: null,
                readOnly: false,
                toggleChalkboardButton: { left: "30px", bottom: "30px", top: "auto", right: "auto" },
                toggleNotesButton: { left: "70px", bottom: "30px", top: "auto", right: "auto" },
                colorButtons: true,
                boardHandle: true,
                transition: 800,
                theme: "chalkboard"
            },

            // Plugins to load
            plugins: [
                RevealMath,
                RevealNotes,
                RevealSearch,
                RevealZoom,
                RevealHighlight,
                RevealMenu,
                RevealChalkboard
            ]
        });

        // Update footer with slide number
        Reveal.on('slidechanged', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const current = event.indexh + 1;
                const total = Reveal.getTotalSlides();
                footer.textContent = `${current} / ${total}`;
            }
        });

        // Initial footer update
        Reveal.on('ready', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const total = Reveal.getTotalSlides();
                footer.textContent = `1 / ${total}`;
            }
        });

        // Keyboard shortcuts info
        console.log('Keyboard shortcuts:');
        console.log('  S - Speaker notes');
        console.log('  B - Chalkboard');
        console.log('  C - Canvas (draw on slide)');
        console.log('  M - Menu');
        console.log('  O - Overview');
        console.log('  F - Fullscreen');
        console.log('  ? - Help');
    </script>
</body>
</html>
