<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="pandoc + custom post-processor">
    <meta name="author" content="Methods and Algorithms – MSc Data Science">
    <title>L03: KNN & K-Means</title>

    <!-- Reveal.js core -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css" id="theme">

    <!-- Custom ML theme -->
    <link rel="stylesheet" href="css/ml-theme.css">

    <!-- Menu plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-menu@2.1.0/menu.css">

    <!-- Chalkboard plugin CSS -->
    <link rel="stylesheet" href="https://unpkg.com/reveal.js-plugins@latest/chalkboard/style.css">

    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;600;700&display=swap" rel="stylesheet">

    <style>
        /* Additional inline styles */
        .reveal .sourceCode {
            overflow: visible;
        }
        code {
            white-space: pre-wrap;
        }
        span.smallcaps {
            font-variant: small-caps;
        }
        div.columns {
            display: flex;
            gap: min(4vw, 1.5em);
        }
        div.column {
            flex: auto;
            overflow-x: auto;
        }

        /* Spotlight cursor */
        .spotlight-cursor {
            position: fixed;
            pointer-events: none;
            z-index: 9999;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(255,0,0,0.3) 0%, transparent 70%);
            transform: translate(-50%, -50%);
        }

        /* PDF export button */
        .pdf-export-btn {
            position: fixed;
            bottom: 20px;
            right: 20px;
            z-index: 100;
            background: var(--ml-purple, #3333B2);
            color: white;
            border: none;
            padding: 10px 15px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            display: flex;
            align-items: center;
            gap: 8px;
            opacity: 0.8;
            transition: opacity 0.2s;
        }
        .pdf-export-btn:hover {
            opacity: 1;
        }

        /* Hide export button in print mode */
        @media print {
            .pdf-export-btn {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section id="title-slide" data-background="linear-gradient(135deg, #3333B2 0%, #0066CC 100%)">
    <div style="color: white; text-align: center;">
        <h1 style="color: white; font-size: 2.5em; margin-bottom: 0.3em; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);">L03: KNN & K-Means</h1>
        <p style="font-size: 1.4em; color: #ADADE0; margin-bottom: 1.5em;">Mathematical Foundations and Implementation</p>
        <p style="font-size: 1em; color: rgba(255,255,255,0.9);">Methods and Algorithms – MSc Data Science</p>
        <div style="margin-top: 2em;">
            <img src="images/qr_wiki.png" alt="QR Code" style="width: 120px; height: 120px; background: white; padding: 8px; border-radius: 8px;" onerror="this.style.display='none'">
            <p style="font-size: 0.7em; color: rgba(255,255,255,0.7); margin-top: 0.5em;">Scan for course materials</p>
        </div>
    </div>
</section>

<section>
<h2>KNN: The Lazy Learner</h2>

<p> <strong>Key Insight</strong></p>
<ul>
<li><p>No explicit model training (store all data)</p></li>
<li><p>Classification by majority vote of K nearest neighbors</p></li>
<li><p>“Lazy” because work is done at prediction time</p></li>
</ul>
<p><strong>The Algorithm</strong></p>
<ol>
<li><p>Store all training examples</p></li>
<li><p>For new query <span class="math inline">\(\mathbf{x}\)</span>:
find K nearest training points</p></li>
<li><p>Return majority class among neighbors</p></li>
</ol>
<p><span style="color: #666666">Instance-based learning: the training data
IS the model</span></p>

</section>

<section>
<h2>Distance Metrics</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/02_distance_metrics.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><strong>Common Metrics</strong></p>
<ul>
<li><p>Euclidean: <span class="math inline">\(d = \sqrt{\sum_i(x_i -
y_i)^2}\)</span> (straight line)</p></li>
<li><p>Manhattan: <span class="math inline">\(d = \sum_i|x_i -
y_i|\)</span> (city block)</p></li>
</ul>

</section>

<section>
<h2>Distance Metrics: Minkowski Family</h2>

<p> <strong>Minkowski
Distance</strong> <span class="math display">\[d_p(\mathbf{x},
\mathbf{y}) = \left(\sum_{i=1}^{n}|x_i -
y_i|^p\right)^{1/p}\]</span></p>
<ul>
<li><p><span class="math inline">\(p=1\)</span>: Manhattan (L1)</p></li>
<li><p><span class="math inline">\(p=2\)</span>: Euclidean (L2)</p></li>
<li><p><span class="math inline">\(p=\infty\)</span>: Chebyshev (max
absolute difference)</p></li>
</ul>
<p><strong>Choosing p</strong></p>
<ul>
<li><p><span class="math inline">\(p=2\)</span>: Default, works well in
most cases</p></li>
<li><p><span class="math inline">\(p=1\)</span>: More robust to
outliers</p></li>
<li><p>Higher <span class="math inline">\(p\)</span>: Sensitive to large
single differences</p></li>
</ul>
<p><span style="color: #666666">In high dimensions, all distances become
similar (curse of dimensionality)</span></p>

</section>

<section>
<h2>KNN Decision Boundaries</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/01_knn_boundaries.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><strong>Boundary Properties</strong></p>
<ul>
<li><p>Non-linear, locally adaptive</p></li>
<li><p>Small K: complex boundary, may overfit</p></li>
<li><p>Large K: smoother boundary, may underfit</p></li>
</ul>

</section>

<section>
<h2>Choosing K</h2>

<p> <strong>The Bias-Variance
Trade-off</strong></p>
<ul>
<li><p><span class="math inline">\(K=1\)</span>: High variance, low bias
(very flexible)</p></li>
<li><p><span class="math inline">\(K=n\)</span>: High bias, low variance
(always predicts majority class)</p></li>
</ul>
<p><strong>Practical Guidelines</strong></p>
<ul>
<li><p>Start with <span class="math inline">\(K = \sqrt{n}\)</span>
where <span class="math inline">\(n\)</span> is training size</p></li>
<li><p>Use odd K for binary classification (avoid ties)</p></li>
<li><p>Cross-validation to find optimal K</p></li>
</ul>
<p><strong>Common Choices</strong>: <span class="math inline">\(K \in
\{3, 5, 7, 11\}\)</span> <span style="color: #666666">Small K for complex
patterns, larger K for noisy data</span></p>

</section>

<section>
<h2>Weighted KNN</h2>

<p> <strong>Problem with Equal
Voting</strong></p>
<ul>
<li><p>All K neighbors have equal influence</p></li>
<li><p>A distant neighbor counts as much as closest neighbor</p></li>
</ul>
<p><strong>Solution: Distance Weighting</strong> <span class="math display">\[w_i = \frac{1}{d(\mathbf{x},
\mathbf{x}_i)^2}\]</span></p>
<ul>
<li><p>Closer neighbors get higher weight</p></li>
<li><p>Reduces sensitivity to K choice</p></li>
</ul>
<p><strong>In scikit-learn</strong></p>
<ul>
<li><p><code>weights=’uniform’</code>: equal weights (default)</p></li>
<li><p><code>weights=’distance’</code>: inverse distance
weighting</p></li>
</ul>
<p><span style="color: #666666">Distance weighting often improves
performance</span></p>

</section>

<section>
<h2>Feature Scaling for KNN</h2>

<p> <strong>Why Scaling
Matters</strong></p>
<p>Without scaling:</p>
<ul>
<li><p>Income: ranges 20,000–200,000</p></li>
<li><p>Age: ranges 20–80</p></li>
<li><p>Distance dominated by income (larger scale)!</p></li>
</ul>
<p><strong>Scaling Methods</strong></p>
<ul>
<li><p><strong>Standardization</strong>: <span class="math inline">\(z =
\frac{x - \mu}{\sigma}\)</span> (mean=0, std=1)</p></li>
<li><p><strong>Min-Max</strong>: <span class="math inline">\(x' =
\frac{x - x_{min}}{x_{max} - x_{min}}\)</span> (range [0,1])</p></li>
</ul>
<p><strong>Rule</strong>: Always scale features for distance-based
methods! <span style="color: #666666">StandardScaler for Gaussian-like
features, MinMaxScaler for bounded</span></p>

</section>

<section>
<h2>KNN: Curse of Dimensionality</h2>

<p> <strong>The
Problem</strong></p>
<p>In high dimensions:</p>
<ul>
<li><p>All points become approximately equidistant</p></li>
<li><p>“Nearest neighbor” becomes meaningless</p></li>
<li><p>Exponentially more data needed</p></li>
</ul>
<p><strong>Solutions</strong></p>
<ul>
<li><p><strong>Dimensionality reduction</strong>: PCA before
KNN</p></li>
<li><p><strong>Feature selection</strong>: keep only relevant
features</p></li>
<li><p><strong>Use domain knowledge</strong>: select meaningful
features</p></li>
</ul>
<p><span style="color: #666666">KNN works best with moderate number of
features (&lt;20)</span></p>

</section>

<section>
<h2>KNN: Computational Considerations</h2>

<p> <strong>Time
Complexity</strong></p>
<ul>
<li><p>Training: <span class="math inline">\(O(1)\)</span> – just store
data!</p></li>
<li><p>Prediction: <span class="math inline">\(O(nd)\)</span> for brute
force (<span class="math inline">\(n\)</span> samples, <span class="math inline">\(d\)</span> features)</p></li>
</ul>
<p><strong>Acceleration Techniques</strong></p>
<ul>
<li><p><strong>KD-Tree</strong>: <span class="math inline">\(O(d \log
n)\)</span> average for low <span class="math inline">\(d\)</span></p></li>
<li><p><strong>Ball Tree</strong>: Works better in higher
dimensions</p></li>
<li><p><strong>Approximate NN</strong>: Trade accuracy for
speed</p></li>
</ul>
<p><strong>In scikit-learn</strong></p>
<ul>
<li><p><code>algorithm=’auto’</code>: automatically chooses
best</p></li>
<li><p><code>algorithm=’brute’</code>: force brute force</p></li>
</ul>
<p><span style="color: #666666">For large datasets: consider approximate
methods or trees</span></p>

</section>

<section>
<h2>KNN: scikit-learn Implementation</h2>

<p>
<strong>Classification</strong></p>
<ul>
<li><p><code>from sklearn.neighbors import KNeighborsClassifier</code></p></li>
<li><p><code>knn = KNeighborsClassifier(n_neighbors=5)</code></p></li>
<li><p><code>knn.fit(X_train, y_train)</code></p></li>
<li><p><code>y_pred = knn.predict(X_test)</code></p></li>
</ul>
<p><strong>Key Parameters</strong></p>
<ul>
<li><p><code>n_neighbors</code>: K value</p></li>
<li><p><code>weights</code>: ’uniform’ or ’distance’</p></li>
<li><p><code>metric</code>: ’euclidean’, ’manhattan’, etc.</p></li>
<li><p><code>algorithm</code>: ’auto’, ’ball_tree’, ’kd_tree’,
’brute’</p></li>
</ul>
<p><span style="color: #666666">Also available: KNeighborsRegressor for
regression tasks</span></p>

</section>

<section>
<h2>K-Means: The Idea</h2>

<p> <strong>Goal</strong>: Partition <span class="math inline">\(n\)</span> points into <span class="math inline">\(K\)</span> clusters</p>
<p><strong>Objective</strong>: Minimize within-cluster sum of squares
(WCSS) <span class="math display">\[\sum_{k=1}^{K}\sum_{\mathbf{x} \in
C_k}\|\mathbf{x} - \mu_k\|^2\]</span> where <span class="math inline">\(\mu_k\)</span> is the centroid of cluster <span class="math inline">\(C_k\)</span></p>
<p><strong>Key Insight</strong></p>
<ul>
<li><p>Each point assigned to nearest centroid</p></li>
<li><p>Centroids are cluster means</p></li>
<li><p>Iterative refinement until convergence</p></li>
</ul>
<p><span style="color: #666666">K-Means finds locally optimal solution (not
guaranteed global)</span></p>

</section>

<section>
<h2>K-Means Algorithm</h2>


<div class="algorithmic">
<p><strong>Input</strong>: Data <span class="math inline">\(\mathbf{X}\)</span>, number of clusters <span class="math inline">\(K\)</span> Initialize <span class="math inline">\(K\)</span> centroids randomly
<strong>Assignment</strong>: assign each point to nearest centroid
<strong>Update</strong>: recompute centroids as cluster means
<strong>return</strong> cluster assignments, centroids</p>
</div>
<p><strong>Convergence</strong></p>
<ul>
<li><p>Guaranteed to converge (WCSS decreases each iteration)</p></li>
<li><p>May converge to local optimum</p></li>
</ul>
<p><span style="color: #666666">Each iteration: <span class="math inline">\(O(nKd)\)</span> where <span class="math inline">\(n\)</span> = samples, <span class="math inline">\(K\)</span> = clusters, <span class="math inline">\(d\)</span> = features</span></p>

</section>

<section>
<h2>K-Means: Visualization</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/03_kmeans_iteration.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><span style="color: #666666">Final state after convergence: points
colored by cluster, X marks centroids</span></p>

</section>

<section>
<h2>Initialization Strategies</h2>

<p> <strong>Random
Initialization</strong></p>
<ul>
<li><p>Pick K random points as initial centroids</p></li>
<li><p>Sensitive to choice, may get poor solution</p></li>
<li><p>Run multiple times, keep best result</p></li>
</ul>
<p><strong>K-Means++ (Default in scikit-learn)</strong></p>
<ul>
<li><p>Smart initialization: spread out initial centroids</p></li>
<li><p>First centroid: random</p></li>
<li><p>Next centroids: probability proportional to squared
distance</p></li>
</ul>
<p><strong>Result</strong>: Much better starting point, fewer iterations
<span style="color: #666666">K-Means++ gives provably better initialization
with theoretical guarantees</span></p>

</section>

<section>
<h2>Choosing K: Elbow Method</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/04_elbow_method.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><strong>Interpretation</strong>: Look for the “elbow” where WCSS
stops dropping sharply</p>

</section>

<section>
<h2>Choosing K: Silhouette Score</h2>

<p> <strong>For each point
<span class="math inline">\(i\)</span></strong></p>
<ul>
<li><p><span class="math inline">\(a(i)\)</span>: average distance to
points in same cluster</p></li>
<li><p><span class="math inline">\(b(i)\)</span>: average distance to
points in nearest other cluster</p></li>
</ul>
<p><strong>Silhouette score</strong> <span class="math display">\[s(i) =
\frac{b(i) - a(i)}{\max(a(i), b(i))}\]</span></p>
<ul>
<li><p>Range: <span class="math inline">\([-1, 1]\)</span></p></li>
<li><p><span class="math inline">\(s \approx 1\)</span>: point is
well-matched to cluster</p></li>
<li><p><span class="math inline">\(s \approx 0\)</span>: point is on
boundary</p></li>
<li><p><span class="math inline">\(s &lt; 0\)</span>: point may be in
wrong cluster</p></li>
</ul>
<p><span style="color: #666666">Average silhouette score summarizes overall
clustering quality</span></p>

</section>

<section>
<h2>Silhouette Plot</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/05_silhouette.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><span style="color: #666666">All clusters should have similar width and
scores above average line</span></p>

</section>

<section>
<h2>K-Means: Decision Regions</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/06_voronoi.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><span style="color: #666666">K-Means creates Voronoi tessellation around
centroids</span></p>

</section>

<section>
<h2>K-Means Assumptions</h2>

<p> <strong>What K-Means
Assumes</strong></p>
<ul>
<li><p>Clusters are spherical (isotropic)</p></li>
<li><p>Clusters have similar sizes</p></li>
<li><p>Clusters have similar densities</p></li>
</ul>
<p><strong>When K-Means Fails</strong></p>
<ul>
<li><p>Non-convex shapes (e.g., crescents)</p></li>
<li><p>Very different cluster sizes</p></li>
<li><p>Different densities</p></li>
<li><p>Outliers (pull centroids away)</p></li>
</ul>
<p><span style="color: #666666">Consider DBSCAN or Gaussian Mixture Models
for these cases</span></p>

</section>

<section>
<h2>K-Means Variants</h2>

<p> <strong>Mini-Batch K-Means</strong></p>
<ul>
<li><p>Uses random subsets for updates</p></li>
<li><p>Much faster for large datasets</p></li>
<li><p>Slightly worse results</p></li>
</ul>
<p><strong>K-Medoids</strong></p>
<ul>
<li><p>Centroids must be actual data points</p></li>
<li><p>More robust to outliers</p></li>
<li><p>Slower than K-Means</p></li>
</ul>
<p><strong>K-Means for Mixed Data</strong></p>
<ul>
<li><p>K-Modes: for categorical data</p></li>
<li><p>K-Prototypes: mixed continuous and categorical</p></li>
</ul>
<p><span style="color: #666666">Mini-Batch K-Means: good for &gt;10k
samples</span></p>

</section>

<section>
<h2>K-Means: scikit-learn</h2>

<p> <strong>Basic Usage</strong></p>
<ul>
<li><p><code>from sklearn.cluster import KMeans</code></p></li>
<li><p><code>kmeans = KMeans(n_clusters=3, random_state=42)</code></p></li>
<li><p><code>labels = kmeans.fit_predict(X)</code></p></li>
<li><p><code>centroids = kmeans.cluster_centers_</code></p></li>
</ul>
<p><strong>Key Parameters</strong></p>
<ul>
<li><p><code>n_clusters</code>: K (required)</p></li>
<li><p><code>init</code>: ’k-means++’ (default) or ’random’</p></li>
<li><p><code>n_init</code>: number of runs (default 10)</p></li>
<li><p><code>max_iter</code>: max iterations per run</p></li>
</ul>
<p><span style="color: #666666">inertia_ attribute gives WCSS after
fitting</span></p>

</section>

<section>
<h2>KNN vs K-Means: Key Differences</h2>


<div class="center">
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>KNN</strong></th>
<th style="text-align: left;"><strong>K-Means</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: left;">Classification/Regression</td>
<td style="text-align: left;">Clustering</td>
</tr>
<tr>
<td style="text-align: left;">Learning</td>
<td style="text-align: left;">Supervised (needs labels)</td>
<td style="text-align: left;">Unsupervised</td>
</tr>
<tr>
<td style="text-align: left;">K meaning</td>
<td style="text-align: left;">Number of neighbors</td>
<td style="text-align: left;">Number of clusters</td>
</tr>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: left;">None (lazy)</td>
<td style="text-align: left;">Iterative optimization</td>
</tr>
<tr>
<td style="text-align: left;">Prediction</td>
<td style="text-align: left;">Compute distances</td>
<td style="text-align: left;">Assign to centroid</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: left;">Class label</td>
<td style="text-align: left;">Cluster ID</td>
</tr>
</tbody>
</table>
</div>
<p><span style="color: #666666">The “K” in KNN and K-Means mean completely
different things!</span></p>

</section>

<section>
<h2>Finance Application: Customer Segmentation</h2>

<p>
<strong>Problem</strong>: Group customers for targeted marketing</p>
<p><strong>Features</strong></p>
<ul>
<li><p>Transaction frequency</p></li>
<li><p>Average transaction amount</p></li>
<li><p>Account balance</p></li>
<li><p>Product holdings</p></li>
</ul>
<p><strong>K-Means Solution</strong></p>
<ul>
<li><p>Cluster into segments (e.g., K=4)</p></li>
<li><p>Profile each segment</p></li>
<li><p>Tailor offerings to segment needs</p></li>
</ul>
<p><span style="color: #666666">Example segments: High-value frequent,
Dormant, New/Growing, Price-sensitive</span></p>

</section>

<section>
<h2>Finance Application: Fraud Detection</h2>

<p>
<strong>Problem</strong>: Flag suspicious transactions</p>
<p><strong>KNN Approach</strong></p>
<ul>
<li><p>Features: amount, time, location, merchant category</p></li>
<li><p>Find K similar past transactions</p></li>
<li><p>If most neighbors are fraud, flag as suspicious</p></li>
</ul>
<p><strong>K-Means Approach</strong></p>
<ul>
<li><p>Cluster “normal” transactions</p></li>
<li><p>New transaction far from all centroids = anomaly</p></li>
<li><p>Combined with distance threshold</p></li>
</ul>
<p><span style="color: #666666">KNN needs labeled fraud examples, K-Means
detects deviation from normal</span></p>

</section>

<section>
<h2>When to Use What</h2>

<p> <strong>Use KNN When</strong></p>
<ul>
<li><p>You have labeled training data</p></li>
<li><p>Local patterns matter (non-linear boundaries)</p></li>
<li><p>Interpretability: “similar to these examples”</p></li>
<li><p>Moderate dataset size</p></li>
</ul>
<p><strong>Use K-Means When</strong></p>
<ul>
<li><p>No labels available</p></li>
<li><p>Looking for natural groupings</p></li>
<li><p>Clusters are roughly spherical</p></li>
<li><p>Need fast clustering of large data</p></li>
</ul>
<p><span style="color: #666666">K-Means often used as preprocessing before
supervised learning</span></p>

</section>

<section>
<h2>Decision Framework</h2>


<div class="center">
<p><img src="images/L03_KNN_KMeans/07_decision_flowchart.png" style="max-width:100%; max-height:500px;"></p>
</div>
<p><span style="color: #666666">Start with KNN for classification, K-Means
for clustering</span></p>

</section>

<section>
<h2>Alternatives to Consider</h2>

<p> <strong>Instead of
KNN</strong></p>
<ul>
<li><p>Large data: use ball tree or approximate NN</p></li>
<li><p>Need probability: logistic regression</p></li>
<li><p>Many features: random forest</p></li>
</ul>
<p><strong>Instead of K-Means</strong></p>
<ul>
<li><p>Unknown K: DBSCAN (density-based)</p></li>
<li><p>Hierarchical structure: agglomerative clustering</p></li>
<li><p>Soft assignments: Gaussian Mixture Models</p></li>
<li><p>Non-spherical: spectral clustering</p></li>
</ul>
<p><span style="color: #666666">DBSCAN automatically determines number of
clusters</span></p>

</section>

<section>
<h2>Key Takeaways</h2>

<p> <strong>K-Nearest Neighbors</strong></p>
<ul>
<li><p>Instance-based, lazy learner</p></li>
<li><p>Scale features, choose K via cross-validation</p></li>
<li><p>Works best with moderate features, moderate data size</p></li>
</ul>
<p><strong>K-Means</strong></p>
<ul>
<li><p>Iterative: assign points, update centroids</p></li>
<li><p>K-Means++ for initialization, elbow/silhouette for K</p></li>
<li><p>Assumes spherical, similar-size clusters</p></li>
</ul>
<p><strong>Common Considerations</strong></p>
<ul>
<li><p>Feature scaling is critical for both</p></li>
<li><p>“K” means different things in each algorithm</p></li>
</ul>
<p><span style="color: #666666">Both are foundational algorithms: simple,
interpretable, widely used</span></p>

</section>

<section>
<h2>References</h2>

<p> <strong>Textbooks</strong></p>
<ul>
<li><p>James et al. (2021). <em>ISLR</em>, Chapters 2, 12</p></li>
<li><p>Hastie et al. (2009). <em>ESL</em>, Chapters 13, 14</p></li>
</ul>
<p><strong>Key Papers</strong></p>
<ul>
<li><p>Arthur &amp; Vassilvitskii (2007). K-Means++</p></li>
<li><p>Cover &amp; Hart (1967). Nearest Neighbor Pattern
Classification</p></li>
</ul>
<p><strong>Next Lecture</strong></p>
<ul>
<li><p>L04: Random Forests</p></li>
<li><p>Ensemble methods and feature importance</p></li>
</ul>

</section>
        </div>

        <!-- Custom footer -->
        <div class="slide-footer">
            <div class="footer-left">Methods and Algorithms</div>
            <div class="footer-center">MSc Data Science</div>
            <div class="footer-right"></div>
        </div>
    </div>

    <!-- PDF Export Button -->
    <button class="pdf-export-btn" onclick="exportPDF()" title="Export to PDF">
        <i class="fas fa-file-pdf"></i> PDF
    </button>

    <!-- Reveal.js core -->
    <script src="https://unpkg.com/reveal.js@4.5.0/dist/reveal.js"></script>

    <!-- Reveal.js plugins -->
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/search/search.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/zoom/zoom.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://unpkg.com/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>

    <!-- Menu plugin -->
    <script src="https://unpkg.com/reveal.js-menu@2.1.0/menu.js"></script>

    <!-- Chalkboard plugin -->
    <script src="https://unpkg.com/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

    <script>
        // Spotlight functionality
        let spotlightEnabled = false;
        let spotlightElement = null;

        function toggleSpotlight() {
            spotlightEnabled = !spotlightEnabled;
            if (spotlightEnabled) {
                spotlightElement = document.createElement('div');
                spotlightElement.className = 'spotlight-cursor';
                spotlightElement.style.width = '100px';
                spotlightElement.style.height = '100px';
                document.body.appendChild(spotlightElement);
                document.addEventListener('mousemove', updateSpotlight);
            } else if (spotlightElement) {
                document.removeEventListener('mousemove', updateSpotlight);
                spotlightElement.remove();
                spotlightElement = null;
            }
        }

        function updateSpotlight(e) {
            if (spotlightElement) {
                spotlightElement.style.left = e.clientX + 'px';
                spotlightElement.style.top = e.clientY + 'px';
            }
        }

        // PDF Export function
        function exportPDF() {
            // Open print dialog with PDF-optimized settings
            const printUrl = window.location.href + '?print-pdf';
            window.open(printUrl, '_blank');
        }

        // Check if we're in print mode
        if (window.location.search.includes('print-pdf')) {
            document.querySelector('.pdf-export-btn').style.display = 'none';
        }

        Reveal.initialize({
            // Display settings
            controls: true,
            controlsTutorial: true,
            controlsLayout: 'bottom-right',
            progress: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            hash: true,
            history: true,

            // Navigation
            keyboard: true,
            overview: true,
            touch: true,
            loop: false,
            navigationMode: 'default',

            // Transitions
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',

            // Sizing (16:9 aspect ratio)
            width: 1600,
            height: 900,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            center: false,

            // Fragments
            fragments: true,
            fragmentInURL: false,

            // Auto-slide (disabled)
            autoSlide: 0,

            // Math configuration
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                tex: {
                    inlineMath: [['\\(', '\\)']],
                    displayMath: [['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            },

            // Menu plugin configuration
            menu: {
                side: 'left',
                width: 'normal',
                numbers: true,
                titleSelector: 'h1, h2',
                useTextContentForMissingTitles: true,
                hideMissingTitles: false,
                markers: true,
                custom: [
                    {
                        title: 'Tools',
                        icon: '<i class="fas fa-tools"></i>',
                        content: `
                            <ul class="slide-menu-items">
                                <li class="slide-menu-item" onclick="toggleSpotlight(); RevealMenu.toggle();">
                                    <i class="fas fa-bullseye"></i> Toggle Spotlight (L)
                                </li>
                                <li class="slide-menu-item" onclick="exportPDF();">
                                    <i class="fas fa-file-pdf"></i> Export PDF
                                </li>
                            </ul>
                        `
                    }
                ],
                themes: [
                    { name: 'ML Theme (Light)', theme: 'css/ml-theme.css' },
                    { name: 'Black', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/black.css' },
                    { name: 'White', theme: 'https://unpkg.com/reveal.js@4.5.0/dist/theme/white.css' }
                ],
                transitions: true,
                openButton: true,
                openSlideNumber: false,
                keyboard: true,
                sticky: false,
                autoOpen: true
            },

            // Chalkboard plugin configuration
            chalkboard: {
                boardmarkerWidth: 3,
                chalkWidth: 4,
                chalkEffect: 0.5,
                storage: null,
                src: null,
                readOnly: false,
                toggleChalkboardButton: { left: "30px", bottom: "30px", top: "auto", right: "auto" },
                toggleNotesButton: { left: "70px", bottom: "30px", top: "auto", right: "auto" },
                colorButtons: true,
                boardHandle: true,
                transition: 800,
                theme: "chalkboard"
            },

            // Plugins to load
            plugins: [
                RevealMath,
                RevealNotes,
                RevealSearch,
                RevealZoom,
                RevealHighlight,
                RevealMenu,
                RevealChalkboard
            ]
        });

        // Custom keyboard shortcuts
        Reveal.addKeyBinding({ keyCode: 76, key: 'L' }, toggleSpotlight);  // L for spotlight

        // Update footer with slide number
        Reveal.on('slidechanged', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const current = event.indexh + 1;
                const total = Reveal.getTotalSlides();
                footer.textContent = `${current} / ${total}`;
            }
        });

        // Initial footer update
        Reveal.on('ready', event => {
            const footer = document.querySelector('.slide-footer .footer-right');
            if (footer) {
                const total = Reveal.getTotalSlides();
                footer.textContent = `1 / ${total}`;
            }
        });

        // Keyboard shortcuts info
        console.log('Keyboard shortcuts:');
        console.log('  S - Speaker notes');
        console.log('  B - Chalkboard');
        console.log('  C - Canvas (draw on slide)');
        console.log('  L - Spotlight/Laser pointer');
        console.log('  M - Menu');
        console.log('  O - Overview');
        console.log('  F - Fullscreen');
        console.log('  ? - Help');
    </script>
</body>
</html>
