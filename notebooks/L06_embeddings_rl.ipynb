{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L06: Embeddings & Reinforcement Learning\n",
    "## Text Representations and Sequential Decision Making\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/Methods_and_Algorithms/blob/main/notebooks/L06_embeddings_rl.ipynb)\n",
    "\n",
    "**Methods and Algorithms -- MSc Data Science**\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain word embeddings and their applications\n",
    "2. Apply pre-trained embeddings for text analysis\n",
    "3. Understand the reinforcement learning framework\n",
    "4. Implement basic Q-learning for decision problems\n",
    "\n",
    "### Finance Applications: Sentiment Analysis, Algorithmic Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "# !pip install gensim gymnasium\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 6)})\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theory Recap\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Dense vector representations of words where similar words have similar vectors.\n",
    "\n",
    "**Cosine Similarity:**\n",
    "$$\\text{sim}(u, v) = \\frac{u \\cdot v}{||u|| \\cdot ||v||}$$\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Learning from interaction with an environment through trial and error.\n",
    "\n",
    "**Q-Learning Update:**\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$: learning rate\n",
    "- $\\gamma$: discount factor\n",
    "- $r$: reward\n",
    "- $s, s'$: current and next state\n",
    "- $a, a'$: current and next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word Embeddings from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate pre-trained word embeddings\n",
    "# In practice, you would load Word2Vec, GloVe, or BERT embeddings\n",
    "\n",
    "def create_simulated_embeddings(dim=50):\n",
    "    \"\"\"\n",
    "    Create simulated word embeddings with semantic structure.\n",
    "    Similar words will have similar vectors.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Base vectors for semantic categories\n",
    "    base_finance = np.random.randn(dim) * 0.5\n",
    "    base_positive = np.random.randn(dim) * 0.5\n",
    "    base_negative = np.random.randn(dim) * 0.5\n",
    "    base_action = np.random.randn(dim) * 0.5\n",
    "    \n",
    "    embeddings = {}\n",
    "    \n",
    "    # Financial terms (similar to each other)\n",
    "    finance_words = ['stock', 'equity', 'share', 'bond', 'dividend', 'portfolio', 'market']\n",
    "    for i, word in enumerate(finance_words):\n",
    "        embeddings[word] = base_finance + np.random.randn(dim) * 0.2\n",
    "    \n",
    "    # Positive sentiment words\n",
    "    positive_words = ['bullish', 'profit', 'gain', 'surge', 'growth', 'rally']\n",
    "    for word in positive_words:\n",
    "        embeddings[word] = base_positive + np.random.randn(dim) * 0.2\n",
    "    \n",
    "    # Negative sentiment words\n",
    "    negative_words = ['bearish', 'loss', 'decline', 'crash', 'risk', 'selloff']\n",
    "    for word in negative_words:\n",
    "        embeddings[word] = base_negative + np.random.randn(dim) * 0.2\n",
    "    \n",
    "    # Action words\n",
    "    action_words = ['buy', 'sell', 'hold', 'trade', 'invest']\n",
    "    for word in action_words:\n",
    "        embeddings[word] = base_action + np.random.randn(dim) * 0.2\n",
    "    \n",
    "    # Make buy/sell more similar to each other (related concepts)\n",
    "    embeddings['sell'] = embeddings['buy'] + np.random.randn(dim) * 0.3\n",
    "    embeddings['bullish'] = -embeddings['bearish'] + np.random.randn(dim) * 0.2\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = create_simulated_embeddings(dim=50)\n",
    "print(f\"Created embeddings for {len(embeddings)} words\")\n",
    "print(f\"Embedding dimension: {len(list(embeddings.values())[0])}\")\n",
    "print(f\"\\nVocabulary: {list(embeddings.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "def find_similar_words(word, embeddings, top_n=5):\n",
    "    \"\"\"Find the most similar words to a given word.\"\"\"\n",
    "    if word not in embeddings:\n",
    "        return []\n",
    "    \n",
    "    target_vec = embeddings[word]\n",
    "    similarities = []\n",
    "    \n",
    "    for other_word, other_vec in embeddings.items():\n",
    "        if other_word != word:\n",
    "            sim = cosine_similarity(target_vec, other_vec)\n",
    "            similarities.append((other_word, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Test similarity\n",
    "print(\"Most similar words to 'stock':\")\n",
    "for word, sim in find_similar_words('stock', embeddings):\n",
    "    print(f\"  {word}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nMost similar words to 'bullish':\")\n",
    "for word, sim in find_similar_words('bullish', embeddings):\n",
    "    print(f\"  {word}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get all words and vectors\n",
    "words = list(embeddings.keys())\n",
    "vectors = np.array([embeddings[w] for w in words])\n",
    "\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by category\n",
    "colors = {'finance': 'blue', 'positive': 'green', 'negative': 'red', 'action': 'orange'}\n",
    "word_categories = {\n",
    "    'stock': 'finance', 'equity': 'finance', 'share': 'finance', 'bond': 'finance',\n",
    "    'dividend': 'finance', 'portfolio': 'finance', 'market': 'finance',\n",
    "    'bullish': 'positive', 'profit': 'positive', 'gain': 'positive',\n",
    "    'surge': 'positive', 'growth': 'positive', 'rally': 'positive',\n",
    "    'bearish': 'negative', 'loss': 'negative', 'decline': 'negative',\n",
    "    'crash': 'negative', 'risk': 'negative', 'selloff': 'negative',\n",
    "    'buy': 'action', 'sell': 'action', 'hold': 'action', 'trade': 'action', 'invest': 'action'\n",
    "}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    cat = word_categories.get(word, 'other')\n",
    "    color = colors.get(cat, 'gray')\n",
    "    ax.scatter(vectors_2d[i, 0], vectors_2d[i, 1], c=color, s=100, alpha=0.7)\n",
    "    ax.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=10,\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=cat.capitalize()) for cat, c in colors.items()]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Word Embeddings Visualization (2D Projection)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Words from same category cluster together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, embeddings):\n",
    "    \"\"\"\n",
    "    Get sentence embedding by averaging word vectors.\n",
    "    (Simple baseline - more advanced: Doc2Vec, BERT)\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation\n",
    "        word = word.strip('.,!?')\n",
    "        if word in embeddings:\n",
    "            word_vectors.append(embeddings[word])\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return None\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"stock market rally gain profit\",\n",
    "    \"equity portfolio growth bullish\",\n",
    "    \"market crash loss bearish selloff\",\n",
    "    \"decline risk loss\",\n",
    "    \"buy hold invest\"\n",
    "]\n",
    "\n",
    "# Get embeddings and compute similarities\n",
    "sentence_embeddings = {s: get_sentence_embedding(s, embeddings) for s in sentences}\n",
    "\n",
    "print(\"Sentence Similarities:\")\n",
    "print(\"-\" * 50)\n",
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences):\n",
    "        if i < j:\n",
    "            sim = cosine_similarity(sentence_embeddings[s1], sentence_embeddings[s2])\n",
    "            print(f\"Sim({i+1}, {j+1}): {sim:.3f}\")\n",
    "            print(f\"  S{i+1}: {s1}\")\n",
    "            print(f\"  S{j+1}: {s2}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Simple Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic financial news\n",
    "import json\n",
    "\n",
    "# Synthetic data (would normally load from file)\n",
    "news_data = [\n",
    "    {\"text\": \"stock surge profit gain bullish\", \"sentiment\": \"positive\"},\n",
    "    {\"text\": \"equity growth rally market\", \"sentiment\": \"positive\"},\n",
    "    {\"text\": \"market crash loss bearish decline\", \"sentiment\": \"negative\"},\n",
    "    {\"text\": \"selloff risk loss decline\", \"sentiment\": \"negative\"},\n",
    "    {\"text\": \"stock market trade hold\", \"sentiment\": \"neutral\"},\n",
    "    {\"text\": \"portfolio dividend bond invest\", \"sentiment\": \"neutral\"}\n",
    "]\n",
    "\n",
    "# Create feature vectors using embeddings\n",
    "X = []\n",
    "y = []\n",
    "texts = []\n",
    "\n",
    "for item in news_data:\n",
    "    emb = get_sentence_embedding(item['text'], embeddings)\n",
    "    if emb is not None:\n",
    "        X.append(emb)\n",
    "        y.append(item['sentiment'])\n",
    "        texts.append(item['text'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels: {np.unique(y, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifier using embedding similarity to sentiment centroids\n",
    "def simple_sentiment_classifier(sentence, embeddings):\n",
    "    \"\"\"\n",
    "    Classify sentiment by comparing to positive/negative word centroids.\n",
    "    \"\"\"\n",
    "    positive_words = ['bullish', 'profit', 'gain', 'surge', 'growth', 'rally']\n",
    "    negative_words = ['bearish', 'loss', 'decline', 'crash', 'risk', 'selloff']\n",
    "    \n",
    "    # Compute centroids\n",
    "    pos_centroid = np.mean([embeddings[w] for w in positive_words if w in embeddings], axis=0)\n",
    "    neg_centroid = np.mean([embeddings[w] for w in negative_words if w in embeddings], axis=0)\n",
    "    \n",
    "    # Get sentence embedding\n",
    "    sent_emb = get_sentence_embedding(sentence, embeddings)\n",
    "    if sent_emb is None:\n",
    "        return 'unknown', 0.0\n",
    "    \n",
    "    # Compute similarities\n",
    "    pos_sim = cosine_similarity(sent_emb, pos_centroid)\n",
    "    neg_sim = cosine_similarity(sent_emb, neg_centroid)\n",
    "    \n",
    "    if pos_sim > neg_sim + 0.1:\n",
    "        return 'positive', pos_sim - neg_sim\n",
    "    elif neg_sim > pos_sim + 0.1:\n",
    "        return 'negative', neg_sim - pos_sim\n",
    "    else:\n",
    "        return 'neutral', abs(pos_sim - neg_sim)\n",
    "\n",
    "# Test classifier\n",
    "test_sentences = [\n",
    "    \"stock market rally profit\",\n",
    "    \"market crash decline loss\",\n",
    "    \"buy stock hold portfolio\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Classification Results:\")\n",
    "print(\"-\" * 50)\n",
    "for sentence in test_sentences:\n",
    "    sentiment, confidence = simple_sentiment_classifier(sentence, embeddings)\n",
    "    print(f\"Text: '{sentence}'\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {confidence:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Reinforcement Learning - Q-Learning from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTradingEnv:\n",
    "    \"\"\"\n",
    "    Simple trading environment for Q-learning demonstration.\n",
    "    \n",
    "    State: (position, price_trend)\n",
    "        - position: -1 (short), 0 (neutral), 1 (long)\n",
    "        - price_trend: 0 (down), 1 (neutral), 2 (up)\n",
    "    \n",
    "    Actions: 0 (sell), 1 (hold), 2 (buy)\n",
    "    \n",
    "    Rewards: Profit/loss from position * price change\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.position = 0  # -1, 0, 1\n",
    "        self.price_trend = 1  # 0, 1, 2\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 20\n",
    "        \n",
    "    def reset(self):\n",
    "        self.position = 0\n",
    "        self.price_trend = np.random.choice([0, 1, 2])\n",
    "        self.step_count = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        return (self.position + 1, self.price_trend)  # Shift position to 0, 1, 2\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute action and return (next_state, reward, done).\n",
    "        action: 0=sell, 1=hold, 2=buy\n",
    "        \"\"\"\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Update position based on action\n",
    "        if action == 0:  # Sell\n",
    "            self.position = max(-1, self.position - 1)\n",
    "        elif action == 2:  # Buy\n",
    "            self.position = min(1, self.position + 1)\n",
    "        # Hold: position stays same\n",
    "        \n",
    "        # Simulate price change\n",
    "        price_change = self.price_trend - 1  # -1, 0, 1\n",
    "        \n",
    "        # Reward = position * price change (+ small penalty for trading)\n",
    "        reward = self.position * price_change\n",
    "        if action != 1:  # Trading cost\n",
    "            reward -= 0.1\n",
    "        \n",
    "        # Update price trend (random walk with persistence)\n",
    "        if np.random.random() < 0.3:  # 30% chance to change\n",
    "            self.price_trend = np.random.choice([0, 1, 2])\n",
    "        \n",
    "        done = self.step_count >= self.max_steps\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "# Test environment\n",
    "env = SimpleTradingEnv()\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "print(f\"State space: position (0-2) x trend (0-2) = 9 states\")\n",
    "print(f\"Action space: sell (0), hold (1), buy (2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent with epsilon-greedy exploration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        # Q-table: state -> action -> value\n",
    "        # State is (position, trend), so we flatten to single index\n",
    "        self.q_table = np.zeros((3, 3, n_actions))  # 3 positions x 3 trends x 3 actions\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Q-learning update.\"\"\"\n",
    "        current_q = self.q_table[state[0], state[1], action]\n",
    "        max_next_q = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "        \n",
    "        # Q-learning update rule\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state[0], state[1], action] = new_q\n",
    "\n",
    "# Create agent\n",
    "agent = QLearningAgent(n_states=9, n_actions=3, alpha=0.1, gamma=0.95, epsilon=0.2)\n",
    "print(\"Q-Learning agent created\")\n",
    "print(f\"Q-table shape: {agent.q_table.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_agent(env, agent, n_episodes=500):\n",
    "    \"\"\"Train Q-learning agent.\"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if episode > 0 and episode % 100 == 0:\n",
    "            agent.epsilon = max(0.01, agent.epsilon * 0.9)\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Train the agent\n",
    "env = SimpleTradingEnv()\n",
    "agent = QLearningAgent(n_states=9, n_actions=3, alpha=0.1, gamma=0.95, epsilon=0.3)\n",
    "\n",
    "print(\"Training Q-learning agent...\")\n",
    "rewards = train_agent(env, agent, n_episodes=500)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Smooth rewards with moving average\n",
    "window = 20\n",
    "smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(rewards, alpha=0.3, color='blue', label='Episode reward')\n",
    "ax.plot(range(window-1, len(rewards)), smoothed_rewards, color='blue', \n",
    "        linewidth=2, label=f'Moving average ({window} episodes)')\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('Q-Learning Training: Cumulative Reward per Episode')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal average reward (last 50 episodes): {np.mean(rewards[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned Q-values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "action_names = ['Sell', 'Hold', 'Buy']\n",
    "position_names = ['Short', 'Neutral', 'Long']\n",
    "trend_names = ['Down', 'Neutral', 'Up']\n",
    "\n",
    "for action_idx, (ax, action_name) in enumerate(zip(axes, action_names)):\n",
    "    q_values = agent.q_table[:, :, action_idx]\n",
    "    \n",
    "    im = ax.imshow(q_values, cmap='RdYlGn', aspect='auto')\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_yticks(range(3))\n",
    "    ax.set_xticklabels(trend_names)\n",
    "    ax.set_yticklabels(position_names)\n",
    "    ax.set_xlabel('Price Trend')\n",
    "    ax.set_ylabel('Current Position')\n",
    "    ax.set_title(f'Q-values for Action: {action_name}')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            text = ax.text(j, i, f'{q_values[i, j]:.2f}',\n",
    "                           ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('Learned Q-Values', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display learned policy\n",
    "print(\"Learned Policy:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Position':<12} {'Trend':<12} {'Best Action':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for pos_idx, pos_name in enumerate(position_names):\n",
    "    for trend_idx, trend_name in enumerate(trend_names):\n",
    "        best_action = np.argmax(agent.q_table[pos_idx, trend_idx])\n",
    "        action_name = action_names[best_action]\n",
    "        print(f\"{pos_name:<12} {trend_name:<12} {action_name:<15}\")\n",
    "\n",
    "print(\"\\nPolicy Interpretation:\")\n",
    "print(\"- When trend is UP and not already LONG: BUY\")\n",
    "print(\"- When trend is DOWN and not already SHORT: SELL\")\n",
    "print(\"- Otherwise: HOLD current position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Word Analogies\n",
    "Implement word analogy: find the word such that `stock - equity + debt â‰ˆ ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: result = embeddings['stock'] - embeddings['equity'] + embeddings['debt']\n",
    "# Then find the word with highest cosine similarity to result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Epsilon Decay\n",
    "Modify the Q-learning agent to use exponential epsilon decay. Compare learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: epsilon = initial_epsilon * (decay_rate ** episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: More Complex State\n",
    "Add \"momentum\" to the state (whether trend is accelerating or decelerating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Track previous trend and compute momentum = current_trend - previous_trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Word Embeddings**:\n",
    "   - Map words to dense vectors\n",
    "   - Similar words have similar vectors\n",
    "   - Use pre-trained (Word2Vec, GloVe, BERT)\n",
    "   - Applications: sentiment analysis, document similarity\n",
    "\n",
    "2. **Reinforcement Learning**:\n",
    "   - Agent learns from environment interaction\n",
    "   - Q-learning: value-based, tabular method\n",
    "   - Balance exploration vs exploitation\n",
    "   - Applications: trading, portfolio optimization\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Embeddings: start with pre-trained, fine-tune if needed\n",
    "   - RL: start simple (tabular), scale up (DQN)\n",
    "   - Both: validate thoroughly before deployment\n",
    "\n",
    "### Course Complete!\n",
    "Apply these methods in your capstone project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
