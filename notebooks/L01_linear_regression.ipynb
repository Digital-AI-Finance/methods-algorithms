{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# L01: Introduction & Linear Regression\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/methods-algorithms/blob/master/notebooks/L01_linear_regression.ipynb)\n\n**Course**: Methods and Algorithms - MSc Data Science\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the OLS framework and derive the normal equation\n2. Implement linear regression from scratch using NumPy\n3. Apply gradient descent for optimization\n4. Interpret coefficients in a business context"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Housing Data\n",
    "\n",
    "We create a synthetic housing dataset for hands-on practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load housing dataset (with fallback to synthetic generation)\nimport os\n\n# Try to load the actual dataset first\ntry:\n    # Try local path first\n    if os.path.exists('../datasets/housing_synthetic.csv'):\n        df = pd.read_csv('../datasets/housing_synthetic.csv')\n        print('Loaded dataset from local file')\n    else:\n        # Try GitHub URL for Colab\n        url = 'https://raw.githubusercontent.com/Digital-AI-Finance/methods-algorithms/master/datasets/housing_synthetic.csv'\n        df = pd.read_csv(url)\n        print('Loaded dataset from GitHub')\nexcept Exception as e:\n    print(f'Could not load dataset: {e}')\n    print('Generating synthetic data instead...')\n    \n    # Generate synthetic housing data (fallback)\n    n_samples = 200\n\n    # Features\n    size = np.random.uniform(50, 250, n_samples)  # sqm\n    bedrooms = np.random.randint(1, 6, n_samples)  # number of bedrooms\n    age = np.random.uniform(0, 50, n_samples)  # years\n    distance_center = np.random.uniform(1, 20, n_samples)  # km to city center\n\n    # True coefficients\n    true_intercept = 50000\n    true_coefs = [2000, 15000, -800, -3000]  # size, bedrooms, age, distance\n\n    # Generate target with noise\n    noise = np.random.normal(0, 30000, n_samples)\n    price = (true_intercept + \n             true_coefs[0] * size + \n             true_coefs[1] * bedrooms + \n             true_coefs[2] * age + \n             true_coefs[3] * distance_center + \n             noise)\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'size_sqm': size,\n        'bedrooms': bedrooms,\n        'age_years': age,\n        'distance_km': distance_center,\n        'price': price\n    })\n\nprint(f'Dataset shape: {df.shape}')\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].scatter(df['size_sqm'], df['price']/1000, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Size (sqm)')\n",
    "axes[0, 0].set_ylabel('Price (thousands)')\n",
    "axes[0, 0].set_title('Price vs Size')\n",
    "\n",
    "axes[0, 1].scatter(df['bedrooms'], df['price']/1000, alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Bedrooms')\n",
    "axes[0, 1].set_ylabel('Price (thousands)')\n",
    "axes[0, 1].set_title('Price vs Bedrooms')\n",
    "\n",
    "axes[1, 0].scatter(df['age_years'], df['price']/1000, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Age (years)')\n",
    "axes[1, 0].set_ylabel('Price (thousands)')\n",
    "axes[1, 0].set_title('Price vs Age')\n",
    "\n",
    "axes[1, 1].scatter(df['distance_km'], df['price']/1000, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Distance to Center (km)')\n",
    "axes[1, 1].set_ylabel('Price (thousands)')\n",
    "axes[1, 1].set_title('Price vs Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory: The Normal Equation\n",
    "\n",
    "For linear regression, the optimal coefficients minimize the sum of squared errors:\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[['size_sqm', 'bedrooms', 'age_years', 'distance_km']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Add intercept column (column of 1s)\n",
    "X_with_intercept = np.column_stack([np.ones(len(X)), X])\n",
    "\n",
    "print(f'X shape: {X_with_intercept.shape}')\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal equation implementation\n",
    "def fit_normal_equation(X, y):\n",
    "    \"\"\"Fit linear regression using the normal equation.\"\"\"\n",
    "    # beta = (X'X)^{-1} X'y\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    beta = np.linalg.solve(XtX, Xty)  # More stable than inv()\n",
    "    return beta\n",
    "\n",
    "# Fit model\n",
    "beta_normal = fit_normal_equation(X_with_intercept, y)\n",
    "\n",
    "print('Coefficients from Normal Equation:')\n",
    "print(f'  Intercept: {beta_normal[0]:.2f}')\n",
    "print(f'  Size:      {beta_normal[1]:.2f} per sqm')\n",
    "print(f'  Bedrooms:  {beta_normal[2]:.2f} per bedroom')\n",
    "print(f'  Age:       {beta_normal[3]:.2f} per year')\n",
    "print(f'  Distance:  {beta_normal[4]:.2f} per km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with true coefficients\n",
    "print('\\nComparison with true coefficients:')\n",
    "print(f'  Intercept: True={true_intercept}, Estimated={beta_normal[0]:.0f}')\n",
    "for i, name in enumerate(['Size', 'Bedrooms', 'Age', 'Distance']):\n",
    "    print(f'  {name}: True={true_coefs[i]}, Estimated={beta_normal[i+1]:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fit_gradient_descent(X, y, learning_rate=0.0001, n_iterations=1000, tol=1e-6):\n    \"\"\"Fit linear regression using gradient descent.\n    \n    Args:\n        X: Design matrix with intercept column\n        y: Target values\n        learning_rate: Step size for gradient updates\n        n_iterations: Maximum iterations\n        tol: Convergence tolerance - stops when loss change < tol\n        \n    Returns:\n        beta: Fitted coefficients\n        losses: Loss history for each iteration\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)  # Initialize coefficients\n    losses = []\n    \n    for i in range(n_iterations):\n        # Predictions\n        y_pred = X @ beta\n        \n        # Compute loss\n        loss = np.mean((y - y_pred)**2)\n        losses.append(loss)\n        \n        # Check convergence criterion\n        if i > 0 and abs(losses[-2] - losses[-1]) < tol:\n            print(f'Converged at iteration {i} (loss change < {tol})')\n            break\n        \n        # Compute gradient\n        gradient = -2/n * X.T @ (y - y_pred)\n        \n        # Update coefficients\n        beta = beta - learning_rate * gradient\n    \n    return beta, losses\n\n# Normalize features for gradient descent (important!)\nX_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\nX_norm_with_intercept = np.column_stack([np.ones(len(X)), X_normalized])\n\n# Fit model with convergence criterion\nbeta_gd, losses = fit_gradient_descent(X_norm_with_intercept, y, \n                                        learning_rate=0.1, \n                                        n_iterations=1000,\n                                        tol=1e-6)\n\nprint(f'Total iterations: {len(losses)}')\nprint(f'Final loss: {losses[-1]:.2f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print('scikit-learn LinearRegression:')\n",
    "print(f'  Train R2: {r2_score(y_train, y_pred_train):.4f}')\n",
    "print(f'  Test R2:  {r2_score(y_test, y_pred_test):.4f}')\n",
    "print(f'  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (coefficients)\n",
    "feature_names = ['Size (sqm)', 'Bedrooms', 'Age (years)', 'Distance (km)']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_names, lr.coef_)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Feature Coefficients')\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('\\nCoefficient interpretation:')\n",
    "for name, coef in zip(feature_names, lr.coef_):\n",
    "    direction = 'increases' if coef > 0 else 'decreases'\n",
    "    print(f'  - 1 unit increase in {name} {direction} price by ${abs(coef):.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals vs Fitted\n",
    "axes[0].scatter(y_pred_test/1000, residuals/1000, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Fitted Values (thousands)')\n",
    "axes[0].set_ylabel('Residuals (thousands)')\n",
    "axes[0].set_title('Residuals vs Fitted Values')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularization: Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS, Ridge, and Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features for fair comparison\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit models\n",
    "ols = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=100)\n",
    "\n",
    "ols.fit(X_train_scaled, y_train)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compare coefficients\n",
    "print('Standardized Coefficients Comparison:')\n",
    "print(f'{\"Feature\":<15} {\"OLS\":>12} {\"Ridge\":>12} {\"Lasso\":>12}')\n",
    "print('-' * 55)\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f'{name:<15} {ols.coef_[i]:>12.0f} {ridge.coef_[i]:>12.0f} {lasso.coef_[i]:>12.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement RMSE\n",
    "Write a function to compute RMSE from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION: Implement RMSE from scratch\ndef rmse(y_true, y_pred):\n    \"\"\"Compute Root Mean Squared Error.\n\n    RMSE measures the average magnitude of prediction errors,\n    penalizing larger errors more heavily due to squaring.\n\n    Args:\n        y_true: Actual target values\n        y_pred: Predicted values\n\n    Returns:\n        float: RMSE value (same units as target)\n    \"\"\"\n    # Step 1: Compute squared errors\n    squared_errors = (y_true - y_pred) ** 2\n    # Step 2: Take mean of squared errors (MSE)\n    mse = np.mean(squared_errors)\n    # Step 3: Take square root to get RMSE\n    return np.sqrt(mse)\n\n# Test the implementation\ny_pred_test = lr.predict(X_test)\nour_rmse = rmse(y_test, y_pred_test)\nsklearn_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\nprint(f'Our RMSE: {our_rmse:.2f}')\nprint(f'sklearn RMSE: {sklearn_rmse:.2f}')\nprint(f'Match: {np.isclose(our_rmse, sklearn_rmse)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cross-Validation\n",
    "Use 5-fold cross-validation to find the optimal Ridge alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION: Cross-Validation for Ridge alpha selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\n\n# Define alpha values to test (log scale is common)\nalphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n\n# Store results for comparison\nresults = []\n\nprint('Ridge Regularization - 5-Fold Cross-Validation')\nprint('=' * 50)\nprint(f'{\"Alpha\":<12} {\"Mean R2\":<12} {\"Std R2\":<12}')\nprint('-' * 50)\n\nbest_alpha = None\nbest_score = -np.inf\n\nfor alpha in alphas:\n    # Create Ridge model with current alpha\n    ridge = Ridge(alpha=alpha)\n\n    # Perform 5-fold CV, scoring with R2\n    cv_scores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='r2')\n\n    mean_score = cv_scores.mean()\n    std_score = cv_scores.std()\n    results.append({'alpha': alpha, 'mean': mean_score, 'std': std_score})\n\n    print(f'{alpha:<12.3f} {mean_score:<12.4f} {std_score:<12.4f}')\n\n    # Track best alpha\n    if mean_score > best_score:\n        best_score = mean_score\n        best_alpha = alpha\n\nprint('-' * 50)\nprint(f'Best alpha: {best_alpha} with CV R2: {best_score:.4f}')\n\n# Final model with best alpha\nfinal_ridge = Ridge(alpha=best_alpha)\nfinal_ridge.fit(X_train, y_train)\ntest_r2 = r2_score(y_test, final_ridge.predict(X_test))\nprint(f'Test R2 with best alpha: {test_r2:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "1. Linear regression finds coefficients that minimize squared errors\n",
    "2. Normal equation gives closed-form solution; gradient descent is iterative\n",
    "3. Coefficients are directly interpretable as marginal effects\n",
    "4. Regularization (Ridge/Lasso) prevents overfitting"
   ]
  }
 ]
}