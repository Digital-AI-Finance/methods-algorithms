{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# L01: Introduction & Linear Regression\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
        "\n",
        "**Course**: Methods and Algorithms - MSc Data Science\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. Understand the OLS framework and derive the normal equation\n",
        "2. Implement linear regression from scratch using NumPy\n",
        "3. Apply gradient descent for optimization\n",
        "4. Interpret coefficients in a business context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Plotting settings\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic Housing Data\n",
        "\n",
        "We create a synthetic housing dataset for hands-on practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic housing data\n",
        "n_samples = 200\n",
        "\n",
        "# Features\n",
        "size = np.random.uniform(50, 250, n_samples)  # sqm\n",
        "bedrooms = np.random.randint(1, 6, n_samples)  # number of bedrooms\n",
        "age = np.random.uniform(0, 50, n_samples)  # years\n",
        "distance_center = np.random.uniform(1, 20, n_samples)  # km to city center\n",
        "\n",
        "# True coefficients\n",
        "true_intercept = 50000\n",
        "true_coefs = [2000, 15000, -800, -3000]  # size, bedrooms, age, distance\n",
        "\n",
        "# Generate target with noise\n",
        "noise = np.random.normal(0, 30000, n_samples)\n",
        "price = (true_intercept + \n",
        "         true_coefs[0] * size + \n",
        "         true_coefs[1] * bedrooms + \n",
        "         true_coefs[2] * age + \n",
        "         true_coefs[3] * distance_center + \n",
        "         noise)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'size_sqm': size,\n",
        "    'bedrooms': bedrooms,\n",
        "    'age_years': age,\n",
        "    'distance_km': distance_center,\n",
        "    'price': price\n",
        "})\n",
        "\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "axes[0, 0].scatter(df['size_sqm'], df['price']/1000, alpha=0.5)\n",
        "axes[0, 0].set_xlabel('Size (sqm)')\n",
        "axes[0, 0].set_ylabel('Price (thousands)')\n",
        "axes[0, 0].set_title('Price vs Size')\n",
        "\n",
        "axes[0, 1].scatter(df['bedrooms'], df['price']/1000, alpha=0.5)\n",
        "axes[0, 1].set_xlabel('Bedrooms')\n",
        "axes[0, 1].set_ylabel('Price (thousands)')\n",
        "axes[0, 1].set_title('Price vs Bedrooms')\n",
        "\n",
        "axes[1, 0].scatter(df['age_years'], df['price']/1000, alpha=0.5)\n",
        "axes[1, 0].set_xlabel('Age (years)')\n",
        "axes[1, 0].set_ylabel('Price (thousands)')\n",
        "axes[1, 0].set_title('Price vs Age')\n",
        "\n",
        "axes[1, 1].scatter(df['distance_km'], df['price']/1000, alpha=0.5)\n",
        "axes[1, 1].set_xlabel('Distance to Center (km)')\n",
        "axes[1, 1].set_ylabel('Price (thousands)')\n",
        "axes[1, 1].set_title('Price vs Distance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Theory: The Normal Equation\n",
        "\n",
        "For linear regression, the optimal coefficients minimize the sum of squared errors:\n",
        "\n",
        "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implementation from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "X = df[['size_sqm', 'bedrooms', 'age_years', 'distance_km']].values\n",
        "y = df['price'].values\n",
        "\n",
        "# Add intercept column (column of 1s)\n",
        "X_with_intercept = np.column_stack([np.ones(len(X)), X])\n",
        "\n",
        "print(f'X shape: {X_with_intercept.shape}')\n",
        "print(f'y shape: {y.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normal equation implementation\n",
        "def fit_normal_equation(X, y):\n",
        "    \"\"\"Fit linear regression using the normal equation.\"\"\"\n",
        "    # beta = (X'X)^{-1} X'y\n",
        "    XtX = X.T @ X\n",
        "    Xty = X.T @ y\n",
        "    beta = np.linalg.solve(XtX, Xty)  # More stable than inv()\n",
        "    return beta\n",
        "\n",
        "# Fit model\n",
        "beta_normal = fit_normal_equation(X_with_intercept, y)\n",
        "\n",
        "print('Coefficients from Normal Equation:')\n",
        "print(f'  Intercept: {beta_normal[0]:.2f}')\n",
        "print(f'  Size:      {beta_normal[1]:.2f} per sqm')\n",
        "print(f'  Bedrooms:  {beta_normal[2]:.2f} per bedroom')\n",
        "print(f'  Age:       {beta_normal[3]:.2f} per year')\n",
        "print(f'  Distance:  {beta_normal[4]:.2f} per km')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with true coefficients\n",
        "print('\\nComparison with true coefficients:')\n",
        "print(f'  Intercept: True={true_intercept}, Estimated={beta_normal[0]:.0f}')\n",
        "for i, name in enumerate(['Size', 'Bedrooms', 'Age', 'Distance']):\n",
        "    print(f'  {name}: True={true_coefs[i]}, Estimated={beta_normal[i+1]:.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_gradient_descent(X, y, learning_rate=0.0001, n_iterations=1000):\n",
        "    \"\"\"Fit linear regression using gradient descent.\"\"\"\n",
        "    n, p = X.shape\n",
        "    beta = np.zeros(p)  # Initialize coefficients\n",
        "    losses = []\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        # Predictions\n",
        "        y_pred = X @ beta\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = np.mean((y - y_pred)**2)\n",
        "        losses.append(loss)\n",
        "        \n",
        "        # Compute gradient\n",
        "        gradient = -2/n * X.T @ (y - y_pred)\n",
        "        \n",
        "        # Update coefficients\n",
        "        beta = beta - learning_rate * gradient\n",
        "    \n",
        "    return beta, losses\n",
        "\n",
        "# Normalize features for gradient descent (important!)\n",
        "X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "X_norm_with_intercept = np.column_stack([np.ones(len(X)), X_normalized])\n",
        "\n",
        "# Fit model\n",
        "beta_gd, losses = fit_gradient_descent(X_norm_with_intercept, y, \n",
        "                                        learning_rate=0.1, \n",
        "                                        n_iterations=1000)\n",
        "\n",
        "print('Gradient Descent completed!')\n",
        "print(f'Final loss: {losses[-1]:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Gradient Descent Convergence')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Using scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = lr.predict(X_train)\n",
        "y_pred_test = lr.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print('scikit-learn LinearRegression:')\n",
        "print(f'  Train R2: {r2_score(y_train, y_pred_train):.4f}')\n",
        "print(f'  Test R2:  {r2_score(y_test, y_pred_test):.4f}')\n",
        "print(f'  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (coefficients)\n",
        "feature_names = ['Size (sqm)', 'Bedrooms', 'Age (years)', 'Distance (km)']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(feature_names, lr.coef_)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Coefficients')\n",
        "plt.axvline(x=0, color='black', linewidth=0.5)\n",
        "plt.show()\n",
        "\n",
        "print('\\nCoefficient interpretation:')\n",
        "for name, coef in zip(feature_names, lr.coef_):\n",
        "    direction = 'increases' if coef > 0 else 'decreases'\n",
        "    print(f'  - 1 unit increase in {name} {direction} price by ${abs(coef):.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Residual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Residuals\n",
        "residuals = y_test - y_pred_test\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Residuals vs Fitted\n",
        "axes[0].scatter(y_pred_test/1000, residuals/1000, alpha=0.5)\n",
        "axes[0].axhline(y=0, color='red', linestyle='--')\n",
        "axes[0].set_xlabel('Fitted Values (thousands)')\n",
        "axes[0].set_ylabel('Residuals (thousands)')\n",
        "axes[0].set_title('Residuals vs Fitted Values')\n",
        "\n",
        "# Q-Q plot\n",
        "from scipy import stats\n",
        "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
        "axes[1].set_title('Q-Q Plot of Residuals')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Regularization: Ridge and Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare OLS, Ridge, and Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize features for fair comparison\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit models\n",
        "ols = LinearRegression()\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=100)\n",
        "\n",
        "ols.fit(X_train_scaled, y_train)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Compare coefficients\n",
        "print('Standardized Coefficients Comparison:')\n",
        "print(f'{\"Feature\":<15} {\"OLS\":>12} {\"Ridge\":>12} {\"Lasso\":>12}')\n",
        "print('-' * 55)\n",
        "for i, name in enumerate(feature_names):\n",
        "    print(f'{name:<15} {ols.coef_[i]:>12.0f} {ridge.coef_[i]:>12.0f} {lasso.coef_[i]:>12.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1: Implement RMSE\n",
        "Write a function to compute RMSE from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your solution here\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Compute Root Mean Squared Error.\"\"\"\n",
        "    # TODO: Implement\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Cross-Validation\n",
        "Use 5-fold cross-validation to find the optimal Ridge alpha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your solution here\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# TODO: Try different alpha values and find the best one\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Key takeaways from this notebook:\n",
        "\n",
        "1. Linear regression finds coefficients that minimize squared errors\n",
        "2. Normal equation gives closed-form solution; gradient descent is iterative\n",
        "3. Coefficients are directly interpretable as marginal effects\n",
        "4. Regularization (Ridge/Lasso) prevents overfitting"
      ]
    }
  ]
}
