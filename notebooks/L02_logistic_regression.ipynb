{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# L02: Logistic Regression\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/methods-algorithms/blob/master/notebooks/L02_logistic_regression.ipynb)\n\n**Course**: Methods and Algorithms - MSc Data Science\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the logistic function and maximum likelihood estimation\n2. Implement logistic regression from scratch using NumPy\n3. Evaluate classifiers using confusion matrix, ROC, and PR curves\n4. Handle class imbalance in credit scoring applications"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_curve, auc,\n",
    "                             precision_recall_curve, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Credit Data\n",
    "\n",
    "We create a synthetic credit scoring dataset for hands-on practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic credit data\n",
    "n_samples = 1000\n",
    "\n",
    "# Features\n",
    "income = np.random.normal(50000, 15000, n_samples)  # Annual income\n",
    "debt_ratio = np.random.uniform(0.1, 0.8, n_samples)  # Debt-to-income ratio\n",
    "credit_history = np.random.randint(1, 10, n_samples)  # Credit history (years)\n",
    "num_accounts = np.random.randint(1, 10, n_samples)  # Number of credit accounts\n",
    "\n",
    "# True probability of default (logistic relationship)\n",
    "z = -3 + (-0.00003 * income) + (4 * debt_ratio) + (-0.1 * credit_history) + (0.05 * num_accounts)\n",
    "prob_default = 1 / (1 + np.exp(-z))\n",
    "default = (np.random.random(n_samples) < prob_default).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'income': income,\n",
    "    'debt_ratio': debt_ratio,\n",
    "    'credit_history_years': credit_history,\n",
    "    'num_accounts': num_accounts,\n",
    "    'default': default\n",
    "})\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'\\nDefault rate: {df[\"default\"].mean():.1%}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "colors = ['green' if d == 0 else 'red' for d in df['default']]\n",
    "\n",
    "axes[0, 0].scatter(df['income']/1000, df['debt_ratio'], c=colors, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Income (thousands)')\n",
    "axes[0, 0].set_ylabel('Debt Ratio')\n",
    "axes[0, 0].set_title('Income vs Debt Ratio')\n",
    "\n",
    "axes[0, 1].hist([df[df['default']==0]['income']/1000, df[df['default']==1]['income']/1000],\n",
    "               bins=20, label=['No Default', 'Default'], color=['green', 'red'], alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Income (thousands)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Income Distribution by Default Status')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].hist([df[df['default']==0]['debt_ratio'], df[df['default']==1]['debt_ratio']],\n",
    "               bins=20, label=['No Default', 'Default'], color=['green', 'red'], alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Debt Ratio')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Debt Ratio Distribution by Default Status')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "default_rate_by_history = df.groupby('credit_history_years')['default'].mean()\n",
    "axes[1, 1].bar(default_rate_by_history.index, default_rate_by_history.values, color='steelblue')\n",
    "axes[1, 1].set_xlabel('Credit History (years)')\n",
    "axes[1, 1].set_ylabel('Default Rate')\n",
    "axes[1, 1].set_title('Default Rate by Credit History')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Theory: The Sigmoid Function\n",
    "\n",
    "The logistic (sigmoid) function maps any real number to the (0, 1) interval:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Visualize sigmoid\n",
    "z = np.linspace(-8, 8, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid(z), linewidth=3, color='purple')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[['income', 'debt_ratio', 'credit_history_years', 'num_accounts']].values\n",
    "y = df['default'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "def cross_entropy_loss(y_true, y_pred):\n    \"\"\"Compute binary cross-entropy loss.\"\"\"\n    epsilon = 1e-15  # Prevent log(0)\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef fit_logistic_gd(X, y, learning_rate=0.1, n_iterations=1000, tol=1e-6):\n    \"\"\"Fit logistic regression using gradient descent with early stopping.\"\"\"\n    n, p = X.shape\n    \n    # Add intercept column\n    X_with_intercept = np.column_stack([np.ones(n), X])\n    \n    # Initialize weights\n    w = np.zeros(p + 1)\n    losses = []\n    \n    for i in range(n_iterations):\n        # Forward pass\n        z = X_with_intercept @ w\n        y_pred = sigmoid(z)\n        \n        # Compute loss\n        loss = cross_entropy_loss(y, y_pred)\n        losses.append(loss)\n        \n        # Early stopping: check convergence\n        if i > 0 and abs(losses[-2] - losses[-1]) < tol:\n            print(f'Converged at iteration {i}')\n            break\n        \n        # Compute gradient\n        gradient = (1/n) * X_with_intercept.T @ (y_pred - y)\n        \n        # Update weights\n        w = w - learning_rate * gradient\n    \n    return w, losses\n\n# Fit model\nw_gd, losses = fit_logistic_gd(X_train, y_train, learning_rate=0.5, n_iterations=500, tol=1e-6)\nprint(f'Final loss: {losses[-1]:.4f}')\nprint(f'Iterations: {len(losses)}')\nprint(f'\\nLearned weights:')\nprint(f'  Intercept: {w_gd[0]:.4f}')\nfor i, name in enumerate(['income', 'debt_ratio', 'credit_history', 'num_accounts']):\n    print(f'  {name}: {w_gd[i+1]:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model using scikit-learn\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "y_proba = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('scikit-learn coefficients:')\n",
    "print(f'  Intercept: {lr.intercept_[0]:.4f}')\n",
    "for name, coef in zip(['income', 'debt_ratio', 'credit_history', 'num_accounts'], lr.coef_[0]):\n",
    "    print(f'  {name}: {coef:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['No Default', 'Default']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, str(cm[i, j]), ha='center', va='center', fontsize=20)\n",
    "\n",
    "plt.xticks([0, 1], ['Predicted: No Default', 'Predicted: Default'])\n",
    "plt.yticks([0, 1], ['Actual: No Default', 'Actual: Default'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, color='green', linewidth=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Coefficient Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds ratios\n",
    "feature_names = ['income', 'debt_ratio', 'credit_history', 'num_accounts']\n",
    "odds_ratios = np.exp(lr.coef_[0])\n",
    "\n",
    "print('Odds Ratios (per 1 std increase in feature):')\n",
    "for name, or_val, coef in zip(feature_names, odds_ratios, lr.coef_[0]):\n",
    "    direction = 'increases' if coef > 0 else 'decreases'\n",
    "    change = abs(or_val - 1) * 100\n",
    "    print(f'  {name}: OR = {or_val:.3f} ({direction} odds by {change:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['green' if c < 0 else 'red' for c in lr.coef_[0]]\n",
    "plt.barh(feature_names, lr.coef_[0], color=colors)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients (Standardized)')\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "print('  - Positive coefficient = increases probability of default')\n",
    "print('  - Negative coefficient = decreases probability of default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds\n",
    "thresholds_to_try = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "print('Threshold | Accuracy | Precision | Recall | F1')\n",
    "print('-' * 55)\n",
    "\n",
    "for thresh in thresholds_to_try:\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    f1 = f1_score(y_test, y_pred_thresh)\n",
    "    print(f'   {thresh:.1f}    |  {acc:.3f}   |   {prec:.3f}   | {rec:.3f}  | {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Accuracy from Scratch\n",
    "Write a function to compute accuracy from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Implement accuracy from scratch\n",
    "def accuracy_from_scratch(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy without sklearn.\"\"\"\n",
    "    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "acc_manual = accuracy_from_scratch(y_test, y_pred)\n",
    "print(f\"Manual accuracy: {acc_manual:.4f}\")\n",
    "print(f\"sklearn accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Exercise 2: Handle Class Imbalance\n",
    "Use class_weight='balanced' and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Handle class imbalance with class_weight='balanced'\n",
    "clf_balanced = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "clf_balanced.fit(X_train, y_train)\n",
    "y_pred_balanced = clf_balanced.predict(X_test)\n",
    "print(\"With class_weight='balanced':\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n",
    "\n",
    "print(f\"\\nDefault accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Balanced accuracy: {accuracy_score(y_test, y_pred_balanced):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "1. Logistic regression models binary outcomes using the sigmoid function\n",
    "2. Maximum likelihood estimation finds optimal parameters via gradient descent\n",
    "3. Evaluation requires multiple metrics: accuracy, precision, recall, F1, AUC\n",
    "4. Coefficients are interpretable as log-odds (use odds ratios for business)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}