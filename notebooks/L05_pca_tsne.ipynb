{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L05: PCA & t-SNE\n",
    "## Dimensionality Reduction for Visualization and Preprocessing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/Methods_and_Algorithms/blob/main/notebooks/L05_pca_tsne.ipynb)\n",
    "\n",
    "**Methods and Algorithms -- MSc Data Science**\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Apply PCA for dimensionality reduction and feature extraction\n",
    "2. Interpret variance explained and choose number of components\n",
    "3. Use t-SNE for visualization of high-dimensional data\n",
    "4. Compare linear (PCA) vs non-linear (t-SNE) methods\n",
    "\n",
    "### Finance Application: Portfolio Risk Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_blobs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 6)})\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theory Recap\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA finds orthogonal directions of maximum variance:\n",
    "\n",
    "**Covariance Matrix:**\n",
    "$$\\Sigma = \\frac{1}{n-1} X^T X$$\n",
    "\n",
    "**Eigendecomposition:**\n",
    "$$\\Sigma v = \\lambda v$$\n",
    "\n",
    "where $v$ = eigenvector (principal direction), $\\lambda$ = eigenvalue (variance)\n",
    "\n",
    "**Variance Explained:**\n",
    "$$\\text{Explained Variance}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}$$\n",
    "\n",
    "### t-SNE\n",
    "\n",
    "t-SNE preserves local neighborhood structure through probability matching:\n",
    "\n",
    "**High-dimensional similarity (Gaussian):**\n",
    "$$p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}$$\n",
    "\n",
    "**Low-dimensional similarity (t-distribution):**\n",
    "$$q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}$$\n",
    "\n",
    "**Objective: Minimize KL divergence**\n",
    "$$KL(P||Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generate Synthetic Portfolio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic portfolio returns data\n",
    "def generate_portfolio_data(n_assets=50, n_periods=252, n_factors=5):\n",
    "    \"\"\"\n",
    "    Generate synthetic asset returns with latent factor structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_assets: Number of assets\n",
    "    - n_periods: Number of time periods (trading days)\n",
    "    - n_factors: Number of latent factors\n",
    "    \n",
    "    Returns:\n",
    "    - returns: Asset returns matrix (n_periods x n_assets)\n",
    "    - factor_loadings: True factor loadings matrix\n",
    "    - sectors: Sector labels for each asset\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create latent factors (market, size, value, momentum, sector)\n",
    "    factors = np.random.randn(n_periods, n_factors) * 0.02  # Daily factor returns\n",
    "    \n",
    "    # Create sector structure (5 sectors)\n",
    "    sectors = np.repeat(['Tech', 'Finance', 'Healthcare', 'Energy', 'Consumer'], n_assets // 5)\n",
    "    \n",
    "    # Factor loadings with sector structure\n",
    "    factor_loadings = np.random.randn(n_assets, n_factors) * 0.5\n",
    "    \n",
    "    # Add sector-specific loading on one factor\n",
    "    for i, sector in enumerate(['Tech', 'Finance', 'Healthcare', 'Energy', 'Consumer']):\n",
    "        mask = sectors == sector\n",
    "        factor_loadings[mask, i % n_factors] += 1.0  # Higher loading on sector factor\n",
    "    \n",
    "    # Generate returns: R = F * B' + epsilon\n",
    "    returns = factors @ factor_loadings.T + np.random.randn(n_periods, n_assets) * 0.01\n",
    "    \n",
    "    return returns, factor_loadings, sectors\n",
    "\n",
    "# Generate data\n",
    "returns, true_loadings, sectors = generate_portfolio_data()\n",
    "\n",
    "# Create asset names\n",
    "asset_names = [f\"{s[:3]}{i+1}\" for i, s in enumerate(sectors)]\n",
    "\n",
    "# Create DataFrame\n",
    "df_returns = pd.DataFrame(returns, columns=asset_names)\n",
    "\n",
    "print(f\"Portfolio data shape: {df_returns.shape}\")\n",
    "print(f\"Number of assets: {df_returns.shape[1]}\")\n",
    "print(f\"Number of periods: {df_returns.shape[0]}\")\n",
    "print(f\"\\nSectors: {np.unique(sectors)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing PCA from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_from_scratch(X, n_components=None):\n",
    "    \"\"\"\n",
    "    Implement PCA from scratch using NumPy.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Data matrix (n_samples x n_features)\n",
    "    - n_components: Number of components to keep (default: all)\n",
    "    \n",
    "    Returns:\n",
    "    - components: Principal components (eigenvectors)\n",
    "    - explained_variance: Variance explained by each component\n",
    "    - explained_variance_ratio: Proportion of variance explained\n",
    "    - transformed: Data projected onto principal components\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data (subtract mean)\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Step 2: Compute covariance matrix\n",
    "    n_samples = X.shape[0]\n",
    "    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n",
    "    \n",
    "    # Step 3: Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Step 4: Sort by eigenvalue (descending)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Step 5: Select top n_components\n",
    "    if n_components is None:\n",
    "        n_components = X.shape[1]\n",
    "    \n",
    "    components = eigenvectors[:, :n_components]\n",
    "    explained_variance = eigenvalues[:n_components]\n",
    "    explained_variance_ratio = explained_variance / np.sum(eigenvalues)\n",
    "    \n",
    "    # Step 6: Transform data\n",
    "    transformed = X_centered @ components\n",
    "    \n",
    "    return components, explained_variance, explained_variance_ratio, transformed\n",
    "\n",
    "# Apply PCA from scratch\n",
    "X = df_returns.values\n",
    "components, var_explained, var_ratio, X_pca = pca_from_scratch(X, n_components=10)\n",
    "\n",
    "print(\"PCA from Scratch Results:\")\n",
    "print(f\"Components shape: {components.shape}\")\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "for i, (var, ratio) in enumerate(zip(var_explained[:5], var_ratio[:5])):\n",
    "    print(f\"  PC{i+1}: {var:.6f} ({ratio*100:.2f}%)\")\n",
    "print(f\"\\nCumulative variance (first 5 components): {np.sum(var_ratio[:5])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: PCA with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA()\n",
    "X_pca_sklearn = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Compare with our implementation\n",
    "print(\"Comparison of variance explained ratios:\")\n",
    "print(f\"{'Component':<12} {'From Scratch':>15} {'scikit-learn':>15}\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(5):\n",
    "    print(f\"PC{i+1:<10} {var_ratio[i]*100:>14.2f}% {pca.explained_variance_ratio_[i]*100:>14.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual variance\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, 16), pca.explained_variance_ratio_[:15] * 100, color='steelblue', alpha=0.7)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Variance Explained (%)')\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.set_xticks(range(1, 16))\n",
    "\n",
    "# Cumulative variance\n",
    "ax2 = axes[1]\n",
    "cumulative_var = np.cumsum(pca.explained_variance_ratio_[:15]) * 100\n",
    "ax2.plot(range(1, 16), cumulative_var, 'o-', color='steelblue', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=80, color='red', linestyle='--', label='80% threshold')\n",
    "ax2.axhline(y=95, color='orange', linestyle='--', label='95% threshold')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance Explained (%)')\n",
    "ax2.set_title('Cumulative Variance Explained')\n",
    "ax2.legend()\n",
    "ax2.set_xticks(range(1, 16))\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal number of components\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_80 = np.argmax(cumvar >= 0.80) + 1\n",
    "n_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 80% variance: {n_80}\")\n",
    "print(f\"Components needed for 95% variance: {n_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize assets in PC space (transpose data: assets as samples)\n# For asset-level analysis, we treat each asset as a sample with 252 features (daily returns)\nX_assets = df_returns.values.T  # Shape: (50 assets, 252 periods)\nX_assets_scaled = StandardScaler().fit_transform(X_assets)\n\n# Apply PCA to assets\npca_assets = PCA()\nX_pca_assets = pca_assets.fit_transform(X_assets_scaled)\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Color by sector\nsector_colors = {'Tech': '#0066CC', 'Finance': '#2CA02C', 'Healthcare': '#D62728', \n                 'Energy': '#FF7F0E', 'Consumer': '#9467BD'}\ncolors = [sector_colors[s] for s in sectors]\n\nscatter = ax.scatter(X_pca_assets[:, 0], X_pca_assets[:, 1], \n                     c=colors, alpha=0.7, s=100)\n\n# Add asset labels\nfor i, name in enumerate(asset_names):\n    ax.annotate(name, (X_pca_assets[i, 0], X_pca_assets[i, 1]), \n                fontsize=7, alpha=0.7)\n\nax.set_xlabel(f'PC1 ({pca_assets.explained_variance_ratio_[0]*100:.1f}%)')\nax.set_ylabel(f'PC2 ({pca_assets.explained_variance_ratio_[1]*100:.1f}%)')\nax.set_title('Asset Relationships: First Two Principal Components')\nax.grid(True, alpha=0.3)\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=c, label=s) for s, c in sector_colors.items()]\nax.legend(handles=legend_elements, loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Each point is an asset. Assets that cluster together have similar return patterns.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: PCA Loadings Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze factor loadings for PC1 and PC2 using time-series PCA\n# Here we use the original orientation: time periods as samples, assets as features\n# This gives us asset loadings on each principal component\n\n# Get loadings from the time-series PCA (pca fitted on X_scaled)\nloadings = pca.components_[:3].T  # First 3 components, shape: (50 assets, 3)\nloadings_df = pd.DataFrame(loadings, \n                           columns=['PC1', 'PC2', 'PC3'],\n                           index=asset_names)\nloadings_df['Sector'] = sectors\n\n# Average loadings by sector\nsector_loadings = loadings_df.groupby('Sector').mean()\nprint(\"Average Factor Loadings by Sector (Time-Series PCA):\")\nprint(sector_loadings.round(3))\n\nprint(\"\\nInterpretation:\")\nprint(\"- These loadings show how much each asset contributes to each principal component\")\nprint(\"- PC1 loadings represent exposure to the first latent factor (often market risk)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(sector_loadings.index))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, sector_loadings['PC1'], width, label='PC1', color='steelblue')\n",
    "bars2 = ax.bar(x, sector_loadings['PC2'], width, label='PC2', color='coral')\n",
    "bars3 = ax.bar(x + width, sector_loadings['PC3'], width, label='PC3', color='forestgreen')\n",
    "\n",
    "ax.set_xlabel('Sector')\n",
    "ax.set_ylabel('Average Loading')\n",
    "ax.set_title('Average PCA Loadings by Sector')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sector_loadings.index)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- PC1 likely represents the 'market factor' (all sectors move together)\")\n",
    "print(\"- PC2/PC3 capture sector-specific risk factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction error for different numbers of components\n",
    "n_components_range = range(1, 21)\n",
    "reconstruction_errors = []\n",
    "\n",
    "for k in n_components_range:\n",
    "    pca_k = PCA(n_components=k)\n",
    "    X_reduced = pca_k.fit_transform(X_scaled)\n",
    "    X_reconstructed = pca_k.inverse_transform(X_reduced)\n",
    "    error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
    "    reconstruction_errors.append(error)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(n_components_range, reconstruction_errors, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Mean Squared Reconstruction Error')\n",
    "ax.set_title('Reconstruction Error vs. Number of Components')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark elbow\n",
    "elbow_idx = 4  # Approximately where the elbow is\n",
    "ax.axvline(x=elbow_idx, color='red', linestyle='--', label=f'Elbow at k={elbow_idx}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nReconstruction error with 5 components: {reconstruction_errors[4]:.6f}\")\n",
    "print(f\"Reconstruction error with 10 components: {reconstruction_errors[9]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: t-SNE for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clustered data for t-SNE demonstration\n",
    "X_clusters, y_clusters = make_blobs(n_samples=300, n_features=50, \n",
    "                                     centers=5, cluster_std=2.0,\n",
    "                                     random_state=42)\n",
    "\n",
    "# Standardize\n",
    "X_clusters_scaled = StandardScaler().fit_transform(X_clusters)\n",
    "\n",
    "print(f\"Cluster data shape: {X_clusters.shape}\")\n",
    "print(f\"Number of clusters: {len(np.unique(y_clusters))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare perplexity values\n",
    "perplexities = [5, 30, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, 5))\n",
    "\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    # Fit t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000)\n",
    "    X_tsne = tsne.fit_transform(X_clusters_scaled)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    for i in range(5):\n",
    "        mask = y_clusters == i\n",
    "        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
    "                   c=[colors[i]], label=f'Cluster {i+1}', s=30, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'Perplexity = {perp}')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.suptitle('t-SNE: Effect of Perplexity', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerplexity Guidelines:\")\n",
    "print(\"- Low (5-10): Focus on very local structure\")\n",
    "print(\"- Medium (30-50): Balanced view (default)\")\n",
    "print(\"- High (100+): More global structure (if dataset is large)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: PCA vs t-SNE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to cluster data\n",
    "pca_cluster = PCA(n_components=2)\n",
    "X_pca_cluster = pca_cluster.fit_transform(X_clusters_scaled)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne_cluster = tsne.fit_transform(X_clusters_scaled)\n",
    "\n",
    "# Compare visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, 5))\n",
    "\n",
    "# PCA\n",
    "ax1 = axes[0]\n",
    "for i in range(5):\n",
    "    mask = y_clusters == i\n",
    "    ax1.scatter(X_pca_cluster[mask, 0], X_pca_cluster[mask, 1],\n",
    "                c=[colors[i]], label=f'Cluster {i+1}', s=50, alpha=0.7)\n",
    "ax1.set_xlabel(f'PC1 ({pca_cluster.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax1.set_ylabel(f'PC2 ({pca_cluster.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax1.set_title('PCA Projection')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE\n",
    "ax2 = axes[1]\n",
    "for i in range(5):\n",
    "    mask = y_clusters == i\n",
    "    ax2.scatter(X_tsne_cluster[mask, 0], X_tsne_cluster[mask, 1],\n",
    "                c=[colors[i]], label=f'Cluster {i+1}', s=50, alpha=0.7)\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "ax2.set_title('t-SNE Projection')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PCA vs t-SNE: Cluster Visualization', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "comparison = {\n",
    "    'Aspect': ['Type', 'Speed', 'Deterministic', 'Preserves', 'Reversible', 'Use for ML', 'Visualization'],\n",
    "    'PCA': ['Linear', 'Fast O(np^2)', 'Yes', 'Global variance', 'Yes', 'Yes (preprocessing)', 'Okay'],\n",
    "    't-SNE': ['Non-linear', 'Slow O(n^2)', 'No', 'Local neighbors', 'No', 'No', 'Excellent']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PCA vs t-SNE Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Best Practice - PCA then t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate larger dataset\n",
    "X_large, y_large = make_blobs(n_samples=1000, n_features=200, \n",
    "                               centers=5, random_state=42)\n",
    "X_large_scaled = StandardScaler().fit_transform(X_large)\n",
    "\n",
    "# Direct t-SNE\n",
    "start = time.time()\n",
    "tsne_direct = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne_direct = tsne_direct.fit_transform(X_large_scaled)\n",
    "time_direct = time.time() - start\n",
    "\n",
    "# PCA + t-SNE\n",
    "start = time.time()\n",
    "pca_pre = PCA(n_components=50)  # Reduce to 50 dimensions first\n",
    "X_pca_pre = pca_pre.fit_transform(X_large_scaled)\n",
    "tsne_after_pca = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne_pca = tsne_after_pca.fit_transform(X_pca_pre)\n",
    "time_pca_tsne = time.time() - start\n",
    "\n",
    "print(f\"Direct t-SNE time: {time_direct:.2f}s\")\n",
    "print(f\"PCA (50 dims) + t-SNE time: {time_pca_tsne:.2f}s\")\n",
    "print(f\"Speedup: {time_direct/time_pca_tsne:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, 5))\n",
    "\n",
    "# Direct t-SNE\n",
    "ax1 = axes[0]\n",
    "for i in range(5):\n",
    "    mask = y_large == i\n",
    "    ax1.scatter(X_tsne_direct[mask, 0], X_tsne_direct[mask, 1],\n",
    "                c=[colors[i]], label=f'Cluster {i+1}', s=20, alpha=0.6)\n",
    "ax1.set_title(f'Direct t-SNE\\n({time_direct:.2f}s)')\n",
    "ax1.set_xlabel('t-SNE 1')\n",
    "ax1.set_ylabel('t-SNE 2')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PCA + t-SNE\n",
    "ax2 = axes[1]\n",
    "for i in range(5):\n",
    "    mask = y_large == i\n",
    "    ax2.scatter(X_tsne_pca[mask, 0], X_tsne_pca[mask, 1],\n",
    "                c=[colors[i]], label=f'Cluster {i+1}', s=20, alpha=0.6)\n",
    "ax2.set_title(f'PCA (50 dims) + t-SNE\\n({time_pca_tsne:.2f}s)')\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparison: Direct t-SNE vs PCA + t-SNE', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest Practice:\")\n",
    "print(\"1. Standardize data\")\n",
    "print(\"2. Apply PCA to reduce to 30-50 dimensions\")\n",
    "print(\"3. Apply t-SNE for 2D visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Choose Number of Components\n",
    "Using the portfolio returns data, determine the optimal number of components using:\n",
    "1. 90% variance threshold\n",
    "2. Kaiser criterion (eigenvalues > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1 Solution: Choose Number of Components\n\n# 1. Kaiser criterion: eigenvalues > 1 (for standardized data)\neigenvalues = pca.explained_variance_\nkaiser_k = np.sum(eigenvalues > 1)\nprint(f\"Kaiser criterion (eigenvalues > 1): k = {kaiser_k}\")\n\n# 2. 90% variance threshold\ncumvar = np.cumsum(pca.explained_variance_ratio_)\nvar90_k = np.argmax(cumvar >= 0.90) + 1\nprint(f\"90% variance threshold: k = {var90_k}\")\n\n# 3. Visual comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Eigenvalues with Kaiser threshold\nax1 = axes[0]\nax1.bar(range(1, min(21, len(eigenvalues)+1)), eigenvalues[:20], color='steelblue', alpha=0.7)\nax1.axhline(y=1, color='red', linestyle='--', linewidth=2, label='Kaiser threshold (=1)')\nax1.set_xlabel('Principal Component')\nax1.set_ylabel('Eigenvalue')\nax1.set_title('Kaiser Criterion')\nax1.legend()\nax1.set_xlim(0, 21)\n\n# Cumulative variance with 90% threshold\nax2 = axes[1]\nax2.plot(range(1, min(21, len(cumvar)+1)), cumvar[:20] * 100, 'o-', color='steelblue', linewidth=2, markersize=6)\nax2.axhline(y=90, color='red', linestyle='--', linewidth=2, label='90% threshold')\nax2.axvline(x=var90_k, color='green', linestyle=':', linewidth=2, label=f'k={var90_k}')\nax2.set_xlabel('Number of Components')\nax2.set_ylabel('Cumulative Variance (%)')\nax2.set_title('Variance Explained')\nax2.legend()\nax2.set_xlim(0, 21)\nax2.set_ylim(0, 105)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nConclusion:\")\nprint(f\"  - Kaiser criterion suggests k = {kaiser_k} components\")\nprint(f\"  - 90% variance requires k = {var90_k} components\")\nprint(f\"  - For preprocessing, choose based on downstream task requirements\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: t-SNE Stability\n",
    "Run t-SNE 3 times with different random seeds. How different are the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2 Solution: t-SNE Stability\n\n# Run t-SNE with different random seeds\nseeds = [0, 42, 123]\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ncolors = plt.cm.Set1(np.linspace(0, 1, 5))\n\nfor ax, seed in zip(axes, seeds):\n    tsne = TSNE(n_components=2, perplexity=30, random_state=seed, n_iter=1000)\n    X_tsne = tsne.fit_transform(X_clusters_scaled)\n    \n    for i in range(5):\n        mask = y_clusters == i\n        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n                   c=[colors[i]], label=f'Cluster {i+1}', s=40, alpha=0.7)\n    \n    ax.set_title(f't-SNE (seed={seed})')\n    ax.set_xlabel('t-SNE 1')\n    ax.set_ylabel('t-SNE 2')\n    ax.grid(True, alpha=0.3)\n\naxes[0].legend(loc='upper right')\nplt.suptitle('t-SNE Stability: Different Random Seeds', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Observations:\")\nprint(\"1. Cluster STRUCTURE is preserved across runs (5 distinct groups)\")\nprint(\"2. Exact POSITIONS and ORIENTATIONS differ significantly\")\nprint(\"3. Cluster SIZES may appear different (not meaningful!)\")\nprint(\"4. DISTANCES between clusters vary (also not meaningful!)\")\nprint(\"\\nBest Practice: Run t-SNE multiple times and look for consistent patterns.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Interpret PCA Loadings\n",
    "For the portfolio data, which sector has the highest loading on PC1? What does this mean economically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 3 Solution: Interpret PCA Loadings\n\n# Get PC1 loadings (first row of components matrix)\npc1_loadings = pca.components_[0]\n\n# Create DataFrame with loadings and sector info\nloadings_analysis = pd.DataFrame({\n    'Asset': asset_names,\n    'PC1_Loading': pc1_loadings,\n    'Abs_Loading': np.abs(pc1_loadings),\n    'Sector': sectors\n})\n\n# Calculate sector average loadings\nsector_avg = loadings_analysis.groupby('Sector')['PC1_Loading'].agg(['mean', 'std']).round(4)\nsector_avg = sector_avg.sort_values('mean', ascending=False)\n\nprint(\"Average PC1 Loading by Sector:\")\nprint(sector_avg)\nprint()\n\n# Find sector with highest loading\ntop_sector = sector_avg['mean'].idxmax()\nprint(f\"Sector with highest PC1 loading: {top_sector}\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\ncolors = ['#0066CC' if s == top_sector else '#808080' for s in sector_avg.index]\nbars = ax.barh(range(len(sector_avg)), sector_avg['mean'].values, color=colors, alpha=0.7)\nax.set_yticks(range(len(sector_avg)))\nax.set_yticklabels(sector_avg.index)\nax.set_xlabel('Average PC1 Loading')\nax.set_title('Sector Contribution to Principal Component 1')\nax.axvline(x=0, color='black', linewidth=0.5)\nax.grid(True, alpha=0.3, axis='x')\n\n# Error bars\nax.errorbar(sector_avg['mean'].values, range(len(sector_avg)), \n            xerr=sector_avg['std'].values, fmt='none', color='black', capsize=3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nEconomic Interpretation:\")\nprint(f\"PC1 represents the dominant factor in portfolio returns.\")\nprint(f\"The '{top_sector}' sector has the highest loading on PC1.\")\nprint(f\"This suggests that {top_sector} stocks are most representative of the\")\nprint(f\"market-wide (systematic) risk factor captured by PC1.\")\nprint(f\"\\nIn factor model terms: high PC1 loading = high beta to market factor.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **PCA** is a linear dimensionality reduction technique:\n",
    "   - Finds orthogonal directions of maximum variance\n",
    "   - Fast, deterministic, and reversible\n",
    "   - Use for preprocessing and feature extraction\n",
    "\n",
    "2. **t-SNE** is a non-linear technique for visualization:\n",
    "   - Preserves local neighborhood structure\n",
    "   - Excellent for finding clusters\n",
    "   - Non-deterministic; cluster sizes/distances are NOT meaningful\n",
    "\n",
    "3. **Best Practice Pipeline**:\n",
    "   - Standardize -> PCA (30-50 dims) -> t-SNE (2D)\n",
    "\n",
    "4. **Choosing Components**:\n",
    "   - 80-95% cumulative variance threshold\n",
    "   - Scree plot elbow method\n",
    "   - Kaiser criterion (eigenvalue > 1)\n",
    "\n",
    "### Next Steps\n",
    "- Explore UMAP as a modern alternative to t-SNE\n",
    "- Apply PCA for noise reduction in time series\n",
    "- Use dimensionality reduction for ML preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}