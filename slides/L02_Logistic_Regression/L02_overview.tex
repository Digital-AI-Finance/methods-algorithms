\documentclass[8pt,aspectratio=169]{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{hyperref}

% Custom colors (ML palette)
\definecolor{MLPurple}{RGB}{51,51,178}
\definecolor{MLBlue}{RGB}{0,102,204}
\definecolor{MLOrange}{RGB}{255,127,14}
\definecolor{MLGreen}{RGB}{44,160,44}
\definecolor{MLRed}{RGB}{214,39,40}

% Apply colors
\setbeamercolor{structure}{fg=MLPurple}
\setbeamercolor{title}{fg=MLPurple}
\setbeamercolor{frametitle}{fg=MLPurple}

% Footer customization
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Custom commands
\newcommand{\bottomnote}[1]{\vfill\footnotesize\textit{#1}}
\newcommand{\highlight}[1]{\textcolor{MLOrange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

% Title information
\title[L02: Logistic Regression]{L02: Logistic Regression}
\subtitle{Classification with Probability Estimates}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

% Outline
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% Slide 2: Learning Objectives
\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item Explain how logistic regression models binary outcomes
\item Derive the maximum likelihood estimation for logistic regression
\item Interpret classification metrics (precision, recall, AUC)
\item Apply logistic regression for credit scoring decisions
\end{enumerate}
\vspace{1em}
\textbf{Finance Application:} Credit default prediction
\bottomnote{These objectives span Bloom's levels: Understand, Apply, Analyze}
\end{frame}

% Slide 3: Motivation
\section{Problem}

\begin{frame}[t]{Why Logistic Regression?}
\textbf{The Business Problem}
\begin{itemize}
\item Banks must decide: approve or reject loan applications
\item Need probability of default, not just yes/no prediction
\item Regulatory requirement: interpretable, auditable models
\end{itemize}
\vspace{0.5em}
\textbf{Why Not Linear Regression?}
\begin{itemize}
\item Linear regression can predict values outside [0,1]
\item Binary outcomes need probability-based approach
\item Logistic regression outputs calibrated probabilities
\end{itemize}
\bottomnote{Logistic regression: the industry standard for credit scoring since 1980s}
\end{frame}

% Slide 4: The Sigmoid Function
\section{Method}

\begin{frame}[t]{The Sigmoid Function}
\textbf{From Linear to Probability}
\begin{itemize}
\item Maps any real number to (0, 1) range
\item Smooth, differentiable, interpretable
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{01_sigmoid_function/chart.pdf}
\end{center}
\bottomnote{$\sigma(z) = 1/(1+e^{-z})$ transforms linear combination to probability}
\end{frame}

% Slide 5: Decision Boundary
\begin{frame}[t]{Decision Boundary}
\begin{center}
\includegraphics[width=0.65\textwidth]{02_decision_boundary/chart.pdf}
\end{center}
\bottomnote{The decision boundary is where $P(y=1|x) = 0.5$, i.e., $w'x + b = 0$}
\end{frame}

% Slide 6: Loss Function
\begin{frame}[t]{Binary Cross-Entropy Loss}
\textbf{Why Not MSE?}
\begin{itemize}
\item MSE with sigmoid creates non-convex loss landscape
\item Cross-entropy is convex, guarantees global optimum
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_log_loss/chart.pdf}
\end{center}
\bottomnote{Heavily penalizes confident wrong predictions}
\end{frame}

% Slide 7: Model Evaluation
\section{Solution}

\begin{frame}[t]{ROC Curve and AUC}
\vspace{-1.2em}
\begin{center}
\includegraphics[width=0.48\textwidth]{04_roc_curve/chart.pdf}
\end{center}
\vspace{-0.8em}
\bottomnote{AUC = probability random positive ranks higher than random negative}
\end{frame}

% Slide 8: Precision-Recall
\begin{frame}[t]{Precision-Recall Trade-off}
\begin{center}
\includegraphics[width=0.65\textwidth]{05_precision_recall/chart.pdf}
\end{center}
\bottomnote{Use PR curve when classes are imbalanced (common in fraud detection)}
\end{frame}

% Slide 9: Confusion Matrix
\begin{frame}[t]{Confusion Matrix: Reading the Results}
\begin{center}
\includegraphics[width=0.65\textwidth]{06_confusion_matrix/chart.pdf}
\end{center}
\bottomnote{FP = approve bad loans (costly), FN = reject good customers (lost revenue)}
\end{frame}

% Slide 10: Decision Framework
\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}

  \begin{itemize}
    \item Exercise 1: Implement logistic regression from scratch
    \item Exercise 2: Train model on credit scoring data
    \item Exercise 3: Evaluate with ROC curve and confusion matrix
  \end{itemize}

  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/} [TBD]
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{When to Use Logistic Regression}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\bottomnote{Key strengths: interpretable coefficients, probability outputs, fast training}
\end{frame}

\section{Summary}

\begin{frame}[t]{References}
  \footnotesize
  \begin{itemize}
    \item James et al. (2021). \textit{Introduction to Statistical Learning}. \url{https://www.statlearning.com/}
    \item Hastie et al. (2009). \textit{Elements of Statistical Learning}. \url{https://hastie.su.domains/ElemStatLearn/}
  \end{itemize}
\end{frame}

\end{document}
