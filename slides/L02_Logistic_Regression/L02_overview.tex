\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L02: Logistic Regression]{L02: Logistic Regression}
\subtitle{Classification with Probability Estimates}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

% Outline
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% Slide 2: Learning Objectives
\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item Derive the MLE for logistic regression via gradient of the log-likelihood
\item Analyze model fit using deviance, LRT, AIC/BIC, and Hosmer-Lemeshow
\item Evaluate classification performance using ROC, calibration, and cost-sensitive metrics
\item Apply logistic regression to credit scoring with regulatory interpretation (Basel PD)
\end{enumerate}
\vspace{1em}
\textbf{Finance Application:} Credit scoring and probability of default (PD)
\bottomnote{Bloom's Levels 4--5: Analyze, Evaluate, Create}
\end{frame}

% Slide 3: Motivation
\section{Problem}

\begin{frame}[t]{Why Logistic Regression?}
\textbf{The Business Problem}
\begin{itemize}
\item Banks must decide: approve or reject loan applications
\item Need probability of default, not just yes/no prediction
\item Regulatory requirement: interpretable, auditable models
\end{itemize}
\vspace{0.5em}
\textbf{Why Not Linear Regression?}
\begin{itemize}
\item Linear regression can predict values outside [0,1]
\item Binary outcomes need probability-based approach
\item Logistic regression outputs calibrated probabilities
\end{itemize}
\bottomnote{Logistic regression: the industry standard for credit scoring since 1980s}
\end{frame}

% Slide 4: The Sigmoid Function
\section{Method}

\begin{frame}[t]{The Sigmoid Function}
\textbf{From Linear to Probability}

\textbf{Logistic function (probability of class 1):}
\begin{equation}
p(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\beta_0 + \boldsymbol{\beta}^\top \mathbf{x})}}
\end{equation}

\begin{center}
\includegraphics[width=0.50\textwidth]{01_sigmoid_function/chart.pdf}
\end{center}
\bottomnote{Maps any real number to $(0,1)$; smooth, differentiable, invertible}
\end{frame}

% Slide 5: Decision Boundary
\begin{frame}[t]{Decision Boundary}
\begin{center}
\includegraphics[width=0.55\textwidth]{02_decision_boundary/chart.pdf}
\end{center}
\textbf{Log-odds (logit):}
\begin{equation}
\log\frac{p}{1-p} = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p
\end{equation}

\textbf{Decision boundary:} $P(y=1|\mathbf{x}) = 0.5 \iff \mathbf{w}'\mathbf{x} + b = 0$
\bottomnote{The boundary is a hyperplane; points on one side predict class 1, the other class 0}
\end{frame}

% Slide 6: Loss Function
\begin{frame}[t]{Binary Cross-Entropy Loss}
\textbf{Maximum Likelihood Estimation:}
\begin{equation}
\ell(\boldsymbol{\beta}) = \sum_{i=1}^{N} \bigl[y_i \log p_i + (1-y_i)\log(1-p_i)\bigr]
\end{equation}
where $p_i = p(y_i=1|\mathbf{x}_i)$.

\begin{center}
\includegraphics[width=0.48\textwidth]{03_log_loss/chart.pdf}
\end{center}
\bottomnote{Maximize log-likelihood = minimize cross-entropy loss (convex, guaranteed global optimum)}
\end{frame}

% Slide 7: Model Evaluation
\section{Solution}

\begin{frame}[t]{ROC Curve and AUC}
\vspace{-1.2em}
\begin{center}
\includegraphics[width=0.48\textwidth]{04_roc_curve/chart.pdf}
\end{center}
\vspace{-0.8em}
\bottomnote{AUC = probability random positive ranks higher than random negative}
\end{frame}

% Slide 8: Precision-Recall
\begin{frame}[t]{Precision-Recall Trade-off}
\begin{center}
\includegraphics[width=0.65\textwidth]{05_precision_recall/chart.pdf}
\end{center}
\bottomnote{Use PR curve when classes are imbalanced (common in fraud detection)}
\end{frame}

% Slide 9: Confusion Matrix
\begin{frame}[t]{Confusion Matrix: Reading the Results}
\begin{center}
\includegraphics[width=0.65\textwidth]{06_confusion_matrix/chart.pdf}
\end{center}
\bottomnote{FP = approve bad loans (costly), FN = reject good customers (lost revenue)}
\end{frame}

% Slide 10: Decision Framework
\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}

  \begin{itemize}
    \item Exercise 1: Implement logistic regression from scratch
    \item Exercise 2: Train model on credit scoring data
    \item Exercise 3: Evaluate with ROC curve and confusion matrix
  \end{itemize}

  \vspace{1em}
  \textbf{Link:} See course materials on GitHub
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{When to Use Logistic Regression}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\bottomnote{Key strengths: interpretable coefficients, probability outputs, fast training}
\end{frame}

\section{Summary}

\begin{frame}[t]{References}
  \footnotesize
  \begin{itemize}
    \item James et al. (2021). \textit{Introduction to Statistical Learning}. \url{https://www.statlearning.com/}
    \item Hastie et al. (2009). \textit{Elements of Statistical Learning}. \url{https://hastie.su.domains/ElemStatLearn/}
  \end{itemize}
\end{frame}

\end{document}
