\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L06: Embeddings \& RL Deep Dive]{L06: Embeddings \& RL}
\subtitle{Deep Dive: Theory, Implementation, and Applications}
\author{Methods and Algorithms}
\institute{MSc Data Science}
\date{Spring 2026}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MAIN BODY (42 slides)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 1: Title
\begin{frame}
\titlepage
\end{frame}

%% Slide 2: Outline
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%% Slide 3: Opening Comic
\begin{frame}[t]{Pouring Data into Linear Algebra}
\begin{center}
\includegraphics[height=0.65\textheight]{images/1838_machine_learning.png}
\end{center}
\bottomnote{XKCD \#1838 ``Machine Learning'' by Randall Munroe (CC BY-NC 2.5)}
\end{frame}

%% Slide 4: Learning Objectives
\begin{frame}[t]{Learning Objectives}
After this lecture, you will be able to:
\begin{enumerate}
\item \textbf{Derive} the Skip-Gram objective and negative sampling approximation
\item \textbf{Evaluate} static vs contextual embeddings (Word2Vec, GloVe, FinBERT)
\item \textbf{Analyze} Q-learning convergence and the exploration-exploitation trade-off
\item \textbf{Critique} RL trading strategies (transaction costs, non-stationarity, overfitting)
\end{enumerate}
\vspace{0.5em}
\textbf{Finance Applications:} Sentiment analysis with embeddings, algorithmic trading with RL
\bottomnote{Bloom's Levels 4--5: Analyze, Evaluate, Derive, Critique}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Word Embeddings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 5: Part 1: Word Embeddings Introduction
\begin{frame}[t]{Part 1: Word Embeddings Introduction}
\textbf{The Problem with One-Hot Encoding}
\begin{itemize}
\item Vocabulary of 10,000 words $\rightarrow$ 10,000-dim sparse vectors
\item No semantic similarity: ``king'' and ``queen'' equally distant from ``car''
\item Curse of dimensionality
\end{itemize}
\vspace{0.5em}
\textbf{Solution: Dense Embeddings}
\begin{itemize}
\item Map words to dense vectors (50-300 dimensions)
\item Similar words $\rightarrow$ similar vectors
\item Learn from context (distributional hypothesis)
\end{itemize}
\bottomnote{``You shall know a word by the company it keeps'' -- Firth, 1957}
\end{frame}

%% Slide 6: Word2Vec: Skip-gram
\begin{frame}[t]{Word2Vec: Skip-gram}
\textbf{Objective:} Predict context words given target word
\[
P(w_{context} | w_{target}) = \frac{\exp(v_{context}^T v_{target})}{\sum_{w \in V} \exp(v_w^T v_{target})}
\]

\textbf{Training:}
\begin{itemize}
\item Slide window over text corpus
\item For each word, predict surrounding words
\item Update embeddings via gradient descent
\end{itemize}
\bottomnote{Skip-gram works well for rare words; CBOW better for frequent words}
\end{frame}

%% Slide 7: Skip-gram: Computational Challenge (HOSTILE REVIEW FIX C1)
\begin{frame}[t]{Skip-gram: Computational Challenge}
\textbf{Problem:} The softmax denominator sums over \highlight{entire vocabulary}:
\[
\sum_{w \in V} \exp(v_w^T v_{target}) \quad \text{--- } O(|V|) \text{ per update!}
\]
For $|V| = 100{,}000$ words, this is computationally intractable.

\vspace{0.5em}
\textbf{Solution: Negative Sampling} (Mikolov et al., 2013b)

Replace full softmax with binary classification:
\[
\log \sigma(v'_{w_O}{}^T v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n}\left[\log \sigma(-v'_{w_i}{}^T v_{w_I})\right]
\]
\begin{itemize}
\item Positive pair: (target, true context) $\rightarrow$ predict 1
\item $k$ negative pairs: (target, random word) $\rightarrow$ predict 0
\item Reduces $O(|V|)$ to $O(k)$ where $k = 5$--$20$
\end{itemize}
\bottomnote{Negative sampling: the key innovation that made Word2Vec practical}
\end{frame}

%% Slide 8: Negative Sampling Illustrated (CHART 10)
\begin{frame}[t]{Negative Sampling Illustrated}
\begin{center}
\includegraphics[width=0.55\textwidth]{10_negative_sampling/chart.pdf}
\end{center}
\bottomnote{Binary classification: distinguish true context words from random ``noise'' words}
\end{frame}

%% Slide 9: Skip-gram Architecture (CHART 08)
\begin{frame}[t]{Skip-gram Architecture}
\begin{center}
\includegraphics[width=0.55\textwidth]{08_skipgram_architecture/chart.pdf}
\end{center}
\bottomnote{Two embedding matrices: input $W$ (word vectors) and output $W'$ (context vectors)}
\end{frame}

%% Slide 10: Skip-Gram with Negative Sampling: Algorithm (HOSTILE REVIEW FIX M4, INSTRUCTOR GUIDE)
\begin{frame}[fragile,t]{Skip-Gram with Negative Sampling: Algorithm}
\begin{algorithmic}[1]
\REQUIRE corpus, embedding dim $d$, negatives $k$, window size, epochs
\STATE Initialize $W, W' \in \mathbb{R}^{|V| \times d}$ randomly
\FOR{each epoch}
  \FOR{each word $w_t$ in corpus}
    \FOR{each context word $w_c$ within window}
      \STATE \textbf{Positive}: update $(w_t, w_c)$ to increase $\sigma(v_{w_t}^\top v'_{w_c})$
      \FOR{$i = 1, \ldots, k$}
        \STATE Sample $w_n \sim P_n(w) \propto f(w)^{3/4}$
        \STATE \textbf{Negative}: update $(w_t, w_n)$ to decrease $\sigma(v_{w_t}^\top v'_{w_n})$
      \ENDFOR
    \ENDFOR
  \ENDFOR
\ENDFOR
\RETURN $W$ (word embeddings)
\end{algorithmic}
\vspace{0.5em}
\textbf{Key}: Negative sampling (3-5 negatives per positive) replaces expensive softmax over entire vocabulary.
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
{\footnotesize\textbf{Mikolov et al.\ (2013). Distributed representations of words and phrases. \textit{NeurIPS}, 3111--3119.}}
\end{frame}

%% Slide 11: Word Analogies
\begin{frame}[t]{Word Analogies}
\textbf{Famous Example:}
\[
\vec{king} - \vec{man} + \vec{woman} \approx \vec{queen}
\]

\textbf{Finance Examples:}
\begin{itemize}
\item $\vec{stock} - \vec{equity} + \vec{debt} \approx \vec{bond}$
\item $\vec{CEO} - \vec{company} + \vec{country} \approx \vec{president}$
\end{itemize}
\vspace{0.5em}
\textbf{How it works:}
\begin{itemize}
\item Vector arithmetic in embedding space
\item Relationships encoded as directions
\end{itemize}
\bottomnote{Embeddings capture relational structure, not just similarity}
\end{frame}

%% Slide 12: Word Analogy Limitations (HOSTILE REVIEW FIX M2)
\begin{frame}[t]{Word Analogy Limitations}
\textbf{Known Issues:}
\begin{itemize}
\item Success rates typically 40--70\%, not near 100\% (Levy \& Goldberg, 2014)
\item Evaluation methodology inflates accuracy (nearest-neighbor dominance)
\item Finance-domain analogies ($\vec{stock} - \vec{equity} + \vec{debt} \approx \vec{bond}$) not empirically validated
\end{itemize}
\vspace{0.5em}
\textbf{Bias in Embeddings:}
\begin{itemize}
\item Embeddings encode societal biases from training data (Bolukbasi et al., 2016)
\item Example: man:programmer :: woman:homemaker
\item \highlight{Finance concern}: Biased embeddings in credit scoring or hiring tools
\end{itemize}
\bottomnote{Critical thinking: embeddings capture statistical patterns, including harmful ones}
\end{frame}

%% Slide 13: Similarity Measures (CHART 02, HOSTILE REVIEW Mi3 already applied)
\begin{frame}[t]{Similarity Measures}
\textbf{Cosine Similarity:}
\[
\text{sim}(u, v) = \frac{u \cdot v}{||u|| \cdot ||v||} = \cos(\theta)
\]
\vspace{-0.3em}
\begin{itemize}
\item Range: $[-1, 1]$; 1=same direction, 0=orthogonal, $-1$=opposite
\end{itemize}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.45\textwidth]{02_similarity_heatmap/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Cosine similarity ignores magnitude, focuses on direction}
\end{frame}

%% Slide 14: Pre-trained Embeddings (HOSTILE REVIEW Mo1 already applied)
\begin{frame}[t]{Pre-trained Embeddings}
\textbf{Popular Options:}
\begin{itemize}
\item \textbf{Word2Vec}: Google, 300-dim, 3M words
\item \textbf{GloVe}: Stanford, trained on Wikipedia + Common Crawl
\item \textbf{FastText}: Facebook, handles subwords (OOV robust)
\end{itemize}
\vspace{0.5em}
\textbf{Domain-Specific:}
\begin{itemize}
\item \textbf{FinBERT}: BERT further pre-trained on financial corpora (Araci, 2019)
\item BioBERT: Biomedical domain
\end{itemize}
\bottomnote{Fine-tuning pre-trained embeddings usually outperforms training from scratch}
\end{frame}

%% Slide 15: Static vs Contextual Embeddings (HOSTILE REVIEW FIX M5)
\begin{frame}[t]{Static vs Contextual Embeddings}
\textbf{Static Embeddings} (Word2Vec, GloVe, FastText):
\begin{itemize}
\item ONE fixed vector per word, regardless of context
\item ``bank'' in ``river bank'' = ``bank'' in ``bank account''
\item Fast, simple, good baseline
\end{itemize}
\vspace{0.5em}
\textbf{Contextual Embeddings} (BERT, GPT, FinBERT):
\begin{itemize}
\item DIFFERENT vector per occurrence based on surrounding context
\item ``bank'' gets different representations in different sentences
\item State-of-the-art for most NLP tasks
\end{itemize}
\vspace{0.5em}
\textbf{Key Insight:} Contextual models solve polysemy (multiple word senses)
\bottomnote{Static: one meaning per word. Contextual: meaning depends on context.}
\end{frame}

%% Slide 16: Embeddings in Finance
\begin{frame}[t]{Embeddings in Finance}
\textbf{Applications:}
\begin{itemize}
\item \textbf{Sentiment Analysis}: News $\rightarrow$ embedding $\rightarrow$ positive/negative
\item \textbf{Document Similarity}: Find similar SEC filings
\item \textbf{Named Entity Recognition}: Extract company names
\item \textbf{Event Detection}: Identify earnings announcements
\end{itemize}
\vspace{0.5em}
\textbf{Sentence Embeddings:}
\begin{itemize}
\item Average word vectors (simple but loses word order: ``bank robber'' = ``robber bank'')
\item Doc2Vec (paragraph vectors)
\item Sentence-BERT (state-of-the-art)
\end{itemize}
\bottomnote{Aggregate word embeddings to represent documents}
\end{frame}

%% Slide 17: Finance Example: Embedding-Based Sentiment (HOSTILE REVIEW FIX C2a)
\begin{frame}[t]{Finance Example: Embedding-Based Sentiment}
\textbf{Task:} Classify ``Fed signals rate hike'' as positive or negative
\vspace{0.3em}

\textbf{Step 1:} Average word embeddings (simplified 3-dim vectors):
\[
\vec{v}_{\text{sentence}} = \frac{1}{4}(\vec{v}_{\text{Fed}} + \vec{v}_{\text{signals}} + \vec{v}_{\text{rate}} + \vec{v}_{\text{hike}}) = [0.12, -0.31, 0.45]
\]

\textbf{Step 2:} Compare to sentiment anchors via cosine similarity:
\begin{itemize}
\item $\text{sim}(\vec{v}_{\text{sentence}}, \vec{v}_{\text{positive}}) = 0.23$
\item $\text{sim}(\vec{v}_{\text{sentence}}, \vec{v}_{\text{negative}}) = 0.61$
\end{itemize}

\textbf{Step 3:} Classify: \highlight{Negative sentiment} (rate hikes $\rightarrow$ tighter policy)
\vspace{0.3em}

\textbf{Real-world:} Use FinBERT for production sentiment (up to 87\% accuracy on financial text; Araci, 2019)
\bottomnote{Simplified example --- real embeddings are 300-768 dimensions with learned sentiment structure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reinforcement Learning Framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 18: Part 2: Reinforcement Learning Framework
\begin{frame}[t]{Part 2: Reinforcement Learning Framework}
\textbf{Key Components:}
\vspace{-0.2em}
\begin{itemize}
\item \textbf{Agent}: Learner/decision-maker
\item \textbf{Environment}: What agent interacts with
\item \textbf{State} $s$: Current situation
\item \textbf{Action} $a$: What agent can do
\item \textbf{Reward} $r$: Feedback signal
\end{itemize}
\vspace{0.5em}
\textbf{The RL Loop:}
\begin{center}
Agent observes State $\rightarrow$ Agent selects Action $\rightarrow$ Environment transitions $\rightarrow$ Environment emits Reward $\rightarrow$ Agent updates $\rightarrow$ (repeat)
\end{center}
\bottomnote{RL: Learning from interaction, not from labeled examples}
\end{frame}

%% Slide 19: Markov Decision Process (HOSTILE REVIEW Mo4 already applied)
\begin{frame}[t]{Markov Decision Process}
\textbf{MDP Tuple:} $(S, A, P, R, \gamma)$
\begin{itemize}
\item $S$: Set of states
\item $A$: Set of actions
\item $P(s'|s, a)$: Transition probability
\item $R(s, a, s')$: Reward function
\item $\gamma \in [0,1)$: Discount factor (or $\gamma \in [0,1]$ for episodic tasks)
\end{itemize}
\vspace{0.5em}
\textbf{Markov Property:}
\[
P(s_{t+1} | s_t, a_t, s_{t-1}, ...) = P(s_{t+1} | s_t, a_t)
\]
\bottomnote{Future depends only on current state, not history}
\end{frame}

%% Slide 20: Policy and Value Functions
\begin{frame}[t]{Policy and Value Functions}
\textbf{Policy:} $\pi(a|s) = P(A_t = a | S_t = s)$
\begin{itemize}
\item Maps states to action probabilities
\item Goal: Find optimal policy $\pi^*$
\end{itemize}
\vspace{0.5em}
\textbf{Value Function:}
\[
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s \right]
\]

\textbf{Q-Function (Action-Value):}
\[
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s, A_0 = a \right]
\]
\bottomnote{Q-function: expected return starting from state s, taking action a}
\end{frame}

%% Slide 21: Bellman Equation
\begin{frame}[t]{Bellman Equation}
\textbf{Optimal Q-Function:}
\[
Q^*(s, a) = \mathbb{E} \left[ R + \gamma \max_{a'} Q^*(s', a') \right]
\]
\vspace{0.5em}
\textbf{Interpretation:}
\begin{itemize}
\item Value = immediate reward + discounted future value
\item Recursive definition enables dynamic programming
\end{itemize}
\vspace{0.5em}
\textbf{Optimal Policy:}
\[
\pi^*(s) = \arg\max_a Q^*(s, a)
\]
\bottomnote{Bellman equation: foundation of all value-based RL methods}
\end{frame}

%% Slide 22: Temporal Difference Learning (HOSTILE REVIEW FIX M3)
\begin{frame}[t]{Temporal Difference Learning}
\textbf{TD(0) Update Rule} --- learn from each transition:
\[
V(s) \leftarrow V(s) + \alpha \left[ r + \gamma V(s') - V(s) \right]
\]
\vspace{-0.3em}
\textbf{TD Error:} $\delta_t = r + \gamma V(s') - V(s)$ (surprise signal)
\vspace{0.3em}
\begin{itemize}
\item \textbf{vs Monte Carlo}: MC waits for episode end; TD updates every step
\item \textbf{vs Dynamic Programming}: DP requires model $P(s'|s,a)$; TD is model-free
\item \textbf{Q-learning}: TD applied to Q-function with $\max$ over actions
\end{itemize}
\bottomnote{TD learning: the theoretical foundation connecting DP, MC, and Q-learning (Sutton, 1988)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Q-Learning and Trading}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 23: Q-Learning Algorithm (HOSTILE REVIEW FIX M9: Robbins-Monro in bottomnote)
\begin{frame}[t]{Q-Learning Algorithm}
\textbf{Update Rule:}
\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]
\vspace{0.5em}
\textbf{Algorithm:}
\begin{enumerate}
\item Initialize $Q(s,a)$ arbitrarily
\item For each episode:
\begin{itemize}
\item Observe state $s$
\item Choose action $a$ ($\epsilon$-greedy)
\item Execute $a$, observe $r$, $s'$
\item Update $Q(s, a)$
\end{itemize}
\end{enumerate}
\bottomnote{Q-learning is off-policy: converges to $Q^*$ given Robbins-Monro conditions ($\sum \alpha_t = \infty$, $\sum \alpha_t^2 < \infty$) and sufficient exploration (Watkins \& Dayan, 1992)}
\end{frame}

%% Slide 24: Q-Learning: Worked Example (HOSTILE REVIEW FIX M8)
\begin{frame}[t]{Q-Learning: Worked Example}
\textbf{Trading scenario:} State $s_1$ = [RSI=25, position=none]

\vspace{0.3em}
\textbf{Current Q-values:} $Q(s_1, \text{buy}) = 3.2$, $Q(s_1, \text{hold}) = 1.0$
\vspace{0.3em}

Agent takes action \textbf{buy}, observes:
\begin{itemize}
\item Reward $r = -0.5$ (transaction cost)
\item New state $s_2$ = [RSI=35, position=long]
\item Best future: $\max_{a'} Q(s_2, a') = 4.0$
\end{itemize}
\vspace{0.3em}
\textbf{Update} ($\alpha = 0.1$, $\gamma = 0.9$):
\[
\underbrace{r + \gamma \max_{a'} Q(s_2, a')}_{\text{TD target}} - \underbrace{Q(s_1, \text{buy})}_{\text{current}} = -0.5 + 0.9 \times 4.0 - 3.2 = -0.1
\]
\[
Q(s_1, \text{buy}) \leftarrow 3.2 + 0.1 \times (-0.1) = \mathbf{3.19}
\]
\bottomnote{Each update moves Q toward the ``better'' estimate: immediate reward + discounted future}
\end{frame}

%% Slide 25: Q-Learning Algorithm: Pseudocode (INSTRUCTOR GUIDE REQUIREMENT)
\begin{frame}[t]{Q-Learning Algorithm: Pseudocode}
\begin{algorithmic}[1]
\REQUIRE environment, $\alpha$, $\gamma$, $\epsilon$, episodes
\STATE Initialize $Q(s, a) \leftarrow 0$ for all $s \in \mathcal{S}$, $a \in \mathcal{A}$
\FOR{episode $= 1, \ldots,$ episodes}
  \STATE $s \leftarrow$ initial state
  \WHILE{$s$ is not terminal}
    \STATE $a \leftarrow \begin{cases} \text{random } a \in \mathcal{A} & \text{with prob.\ } \epsilon \\ \arg\max_{a'} Q(s, a') & \text{otherwise}\end{cases}$ \COMMENT{$\epsilon$-greedy}
    \STATE Take action $a$, observe reward $r$ and next state $s'$
    \STATE $Q(s,a) \leftarrow Q(s,a) + \alpha\bigl[r + \gamma \max_{a'} Q(s', a') - Q(s,a)\bigr]$
    \STATE $s \leftarrow s'$
  \ENDWHILE
\ENDFOR
\RETURN $Q$
\end{algorithmic}

\medskip
\textbf{Key}: The $\max_{a'}$ makes Q-learning \textbf{off-policy} --- it learns the optimal policy regardless of the exploration strategy used.

\bottomnote{Watkins \& Dayan (1992). Q-learning. \textit{Machine Learning}, 8(3-4), 279--292.}
\end{frame}

%% Slide 26: Q-Values Visualization (CHART 04)
\begin{frame}[t]{Q-Values Visualization}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.50\textwidth]{04_q_learning_grid/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{Arrows show policy; colors show Q-values (green=high, red=negative)}
\end{frame}

%% Slide 27: Exploration vs Exploitation
\begin{frame}[t]{Exploration vs Exploitation}
\textbf{The Dilemma:}
\begin{itemize}
\item \textbf{Exploit}: Choose best known action (greedy)
\item \textbf{Explore}: Try new actions (discover better options)
\end{itemize}
\vspace{0.5em}
\textbf{$\epsilon$-Greedy Strategy:}
\[
a = \begin{cases}
\arg\max_a Q(s,a) & \text{with probability } 1-\epsilon \\
\text{random action} & \text{with probability } \epsilon
\end{cases}
\]
\vspace{0.5em}
\textbf{Decay Schedule:}
\begin{itemize}
\item Start with high $\epsilon$ (explore more)
\item Decay $\epsilon$ over time (exploit more)
\end{itemize}
\bottomnote{Balance: too much exploration wastes time; too little misses optima}
\end{frame}

%% Slide 28: RL for Trading
\begin{frame}[t]{RL for Trading}
\textbf{Formulation:}
\begin{itemize}
\item \textbf{State}: Price history, portfolio, technical indicators
\item \textbf{Action}: Buy, sell, hold (+ position size)
\item \textbf{Reward}: Profit/loss, risk-adjusted return
\end{itemize}
\vspace{0.5em}
\textbf{Challenges:}
\begin{itemize}
\item Non-stationary environment
\item High noise, low signal-to-noise ratio
\item Transaction costs
\item Partial observability
\end{itemize}
\bottomnote{RL for trading is active research area; not solved problem}
\end{frame}

%% Slide 29: Trading Reward Function Design (HOSTILE REVIEW FIX C2b)
\begin{frame}[t]{Trading Reward Function Design}
\textbf{Reward with transaction costs:}
\[
r_t = R_t^{\text{portfolio}} - c \cdot |\Delta w_t|
\]
where $R_t^{\text{portfolio}}$ = portfolio return, $c$ = transaction cost rate, $\Delta w_t$ = position change

\vspace{0.3em}
\textbf{Common State Features:}
\begin{itemize}
\item Price returns (1-day, 5-day, 20-day)
\item Technical indicators: RSI, MACD, Bollinger width
\item Current position and unrealized P\&L
\end{itemize}
\vspace{0.3em}
\textbf{Alternative Rewards:}
\begin{itemize}
\item Sharpe ratio: $r_t = \frac{\bar{R}_t}{\sigma_{R_t}}$ (risk-adjusted, but non-stationary)
\item Log return: $r_t = \log(1 + R_t)$ (additive over time)
\end{itemize}
\bottomnote{Reward design is THE most critical decision in RL for trading}
\end{frame}

%% Slide 30: Backtesting RL Trading Strategies
\begin{frame}[t]{Backtesting RL Trading Strategies}
\textbf{Critical Challenge:} RL agents overfit to historical data

\vspace{0.3em}
\textbf{Walk-Forward Validation:}
\begin{enumerate}
\item Train on period $[t_0, t_1]$, test on $[t_1, t_2]$
\item Roll forward: train on $[t_1, t_2]$, test on $[t_2, t_3]$
\item Report average out-of-sample performance
\end{enumerate}
\vspace{0.3em}
\textbf{Honest Evaluation:}
\begin{itemize}
\item Compare to buy-and-hold benchmark (most RL strategies fail to beat after costs)
\item Include realistic transaction costs (0.1--0.5\% per trade)
\item Test across multiple market regimes (bull, bear, sideways)
\end{itemize}
\bottomnote{If your RL agent beats buy-and-hold after costs, you likely have a bug --- verify carefully}
\end{frame}

%% Slide 31: Trading Policy (CHART 06)
\begin{frame}[t]{Trading Policy}
\begin{center}
\includegraphics[width=0.65\textwidth]{06_policy_viz/chart.pdf}
\end{center}
\bottomnote{Q-learning trained policy: agent discovers buy/sell/hold regions from reward signal}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deep RL and Advanced Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 32: Deep Q-Networks (DQN) (CHART 09, HOSTILE REVIEW FIX M7)
\begin{frame}[t]{Deep Q-Networks (DQN)}
\textbf{Idea}: Neural network approximates Q-function: $Q(s, a; \theta) \approx Q^*(s, a)$

\textbf{Loss Function:}
\[
L(\theta) = \mathbb{E}\left[\left(r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta)\right)^2\right]
\]
\vspace{-0.3em}
\textbf{Key Innovations:}
\begin{itemize}
\item \textbf{Experience Replay}: Store $(s,a,r,s')$, sample random mini-batches (breaks temporal correlation)
\item \textbf{Target Network} $\theta^-$: Separate, slowly-updated copy for stability
\end{itemize}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.50\textwidth]{09_dqn_architecture/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{DQN: Atari-level play from raw pixels (Mnih et al., 2015); loss is mean squared TD error}
\end{frame}

%% Slide 33: Policy Gradient Methods (HOSTILE REVIEW FIX M6)
\begin{frame}[t]{Policy Gradient Methods}
\textbf{Policy Gradient Theorem} (Sutton et al., 2000):
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot A^{\pi_\theta}(s, a) \right]
\]
where $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ is the \textbf{advantage function}
\vspace{0.3em}
\begin{itemize}
\item \textbf{REINFORCE}: Uses episode returns $G_t$ as $A$; high variance
\item \textbf{Actor-Critic}: Actor (policy $\pi_\theta$) + Critic (learns $V^\phi$); lower variance
\item \textbf{PPO}: Clips policy ratio to prevent large updates; widely used
\end{itemize}
\bottomnote{Policy gradient handles continuous actions; advantage reduces variance vs raw returns}
\end{frame}

%% Slide 34: Statistical Inference for Embeddings & RL (HOSTILE REVIEW FIX C4)
\begin{frame}[t]{Statistical Inference for Embeddings \& RL}
\textbf{Embedding Uncertainty:}
\begin{itemize}
\item Bootstrap cosine similarity: resample corpus, retrain, compute CI
\item Permutation test: shuffle word-context pairs, check if similarity is significant
\end{itemize}
\vspace{0.5em}
\textbf{RL Uncertainty:}
\begin{itemize}
\item Q-value confidence: run $N$ independent training runs, report mean $\pm$ std
\item Off-policy evaluation: importance sampling to estimate policy value from logged data
\[
\hat{V}(\pi) = \frac{1}{n}\sum_{i=1}^{n} \prod_{t=0}^{T} \frac{\pi(a_t|s_t)}{\beta(a_t|s_t)} \cdot G_i
\]
\end{itemize}
\bottomnote{Always report uncertainty --- a single training run is not evidence of a good policy}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Practice}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 35: Hands-on Exercise
\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Explore word embeddings with Word2Vec
    \item Exercise 2: Implement basic Q-learning
    \item Exercise 3: Apply RL to a simple trading environment
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/github/Digital-AI-Finance/methods-algorithms/blob/master/notebooks/L06_embeddings_rl.ipynb}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Decision Framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 36: When to Use What (Full Comparison Table â€” NO chart 07)
\begin{frame}[t]{When to Use What}
\vspace{-0.3em}
\begin{center}
\small
\begin{tabular}{l|cc}
\toprule
\textbf{Aspect} & \textbf{Embeddings} & \textbf{RL} \\
\midrule
Input & Text, categorical & State sequence \\
Output & Dense vectors & Actions/policy \\
Learning & Unsupervised/supervised & Trial and error \\
Signal & Context (words) & Rewards \\
Key challenge & Semantics & Credit assignment \\
Finance use & Sentiment & Trading \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{Both transform complex inputs into learnable representations}
\end{frame}

%% Slide 37: Implementation (fragile)
\begin{frame}[t,fragile]{Implementation}
\textbf{Embeddings in Python:}
\small
\begin{itemize}
\item \texttt{gensim.models.Word2Vec}: Train your own
\item \texttt{gensim.downloader.load('glove-wiki-gigaword-100')}: Pre-trained
\item \texttt{transformers.BertModel}: BERT embeddings
\end{itemize}
\normalsize
\vspace{0.5em}
\textbf{RL Libraries:}
\small
\begin{itemize}
\item \texttt{gymnasium}: Environment interface (formerly OpenAI Gym)
\item \texttt{stable-baselines3}: Pre-implemented algorithms
\item \texttt{ray[rllib]}: Scalable RL
\end{itemize}
\bottomnote{Start with pre-trained embeddings; use stable-baselines3 for RL}
\end{frame}

%% Slide 38: Practical Tips
\begin{frame}[t]{Practical Tips}
\textbf{Embeddings:}
\begin{itemize}
\item Start with pre-trained, fine-tune if needed
\item Check domain match (general vs financial)
\item Visualize with t-SNE/UMAP to verify quality
\end{itemize}
\vspace{0.5em}
\textbf{RL:}
\begin{itemize}
\item Start simple (tabular Q-learning before DQN)
\item Reward shaping is crucial (sparse rewards are hard)
\item Normalize observations
\item Use established environments first (Gym, FinRL)
\end{itemize}
\bottomnote{Both domains: start simple, iterate, validate thoroughly}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Slide 39: Key Takeaways
\begin{frame}[t]{Key Takeaways}
\textbf{Embeddings:}
\begin{itemize}
\item Dense vector representations of text/categories
\item Capture semantic similarity
\item Use pre-trained (Word2Vec, GloVe, BERT)
\end{itemize}

\textbf{Reinforcement Learning:}
\begin{itemize}
\item Agent learns from environment interaction
\item Q-learning: value-based, tabular or deep (DQN)
\item Applications: trading, portfolio optimization
\end{itemize}

\textbf{Key Takeaway:} Different tools for different problems
\bottomnote{Course complete! Apply these methods in your capstone project}
\end{frame}

%% Slide 40: Closing Comic (XKCD #1838 text callback)
\begin{frame}[t]{Closing Thought}
\vspace{2em}
\begin{center}
\Large\textit{``After six lectures of methods and algorithms,}

\textit{we've learned the most important lesson:}

\vspace{0.5em}

\textit{pour the data into the right pile of linear algebra,}

\textit{and the answers will come out the other side.}

\vspace{0.5em}

\textit{The hard part is knowing which pile.'\,''}
\end{center}

\vspace{1em}
\begin{center}
\normalsize --- Adapted from XKCD \#1838 ``Machine Learning'' by Randall Munroe
\end{center}
\bottomnote{Callback to XKCD \#1838 by Randall Munroe (CC BY-NC 2.5). Course complete!}
\end{frame}

%% Slide 41: References (Embeddings)
\begin{frame}[t]{References: Embeddings}
\small
\begin{itemize}
\item Mikolov, T., Sutskever, I., Chen, K., Corrado, G., \& Dean, J. (2013). Distributed representations of words and phrases and their compositionality. \textit{NeurIPS}, 3111--3119.
\item Pennington, J., Socher, R., \& Manning, C. (2014). GloVe: Global vectors for word representation. \textit{EMNLP}, 1532--1543.
\item Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers. \textit{NAACL}, 4171--4186.
\item Levy, O. \& Goldberg, Y. (2014). Linguistic regularities in sparse and explicit word representations. \textit{CoNLL}, 171--180.
\item Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., \& Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. \textit{NeurIPS}, 4349--4357.
\end{itemize}
\end{frame}

%% Slide 42: References (RL and Finance)
\begin{frame}[t]{References: RL and Finance}
\small
\begin{itemize}
\item Sutton, R. \& Barto, A. (2018). \textit{Reinforcement Learning: An Introduction} (2nd ed.). MIT Press.
\item Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. \textit{Nature}, 518, 529--533.
\item Schulman, J., Wolski, F., Dhariwal, P., Radford, A., \& Klimov, O. (2017). Proximal policy optimization algorithms. \textit{arXiv:1707.06347}.
\item Watkins, C. \& Dayan, P. (1992). Q-learning. \textit{Machine Learning}, 8(3-4), 279--292.
\item Liu, X.-Y., Yang, H., Gao, J., \& Wang, C. (2021). FinRL: Deep reinforcement learning framework for automated trading. \textit{SSRN}.
\item Araci, D. (2019). FinBERT: Financial sentiment analysis with pre-trained language models. \textit{arXiv:1908.10063}.
\end{itemize}
\bottomnote{Sutton \& Barto: the definitive RL textbook (free at incompleteideas.net)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX (8 slides)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\section*{Advanced Topics}

%% Slide A1: Appendix Divider
\begin{frame}
\begin{center}
\vspace{2cm}
{\Huge \textcolor{mlpurple}{Appendix}}

\vspace{0.5cm}
{\Large Advanced Topics and Proofs}

\vspace{0.5cm}
{\normalsize Supplementary material for self-study and reference}
\end{center}
\bottomnote{Appendix slides are not covered in lecture --- provided for advanced students and exam preparation.}
\end{frame}

%% Slide A2: Skip-Gram Objective Derivation
\begin{frame}[t]{Skip-Gram Objective Derivation}
\textbf{Maximum Likelihood Objective:}

Given corpus of word-context pairs $(w_t, w_c)$, maximize:
\[
\mathcal{L} = \sum_{(w_t, w_c) \in D} \log P(w_c | w_t)
\]

\textbf{With softmax parameterization:}
\[
\log P(w_c | w_t) = v_{w_c}^{\prime\top} v_{w_t} - \log \sum_{w \in V} \exp(v_w^{\prime\top} v_{w_t})
\]

\textbf{Simplification:} The log-sum-exp term is the log-partition function. Maximizing $\mathcal{L}$ is equivalent to minimizing cross-entropy between the model distribution and the empirical context distribution.

\vspace{0.3em}
\textbf{Connection to cross-entropy:}
\[
H(p_{\text{empirical}}, p_{\text{model}}) = -\sum_{w_c} \hat{p}(w_c | w_t) \log p_\theta(w_c | w_t)
\]
\bottomnote{Skip-Gram is a discriminative model: it models $P(\text{context}|\text{target})$ directly, not a generative process}
\end{frame}

%% Slide A3: Negative Sampling Theory
\begin{frame}[t]{Negative Sampling Theory}
\textbf{Origin: Noise Contrastive Estimation (NCE)}
\begin{itemize}
\item NCE (Gutmann \& Hyv\"arinen, 2012): estimate unnormalized models by contrasting data with noise
\item Negative sampling is a simplified variant of NCE
\end{itemize}
\vspace{0.3em}
\textbf{Why the $3/4$ Power?}
\begin{itemize}
\item Noise distribution: $P_n(w) \propto f(w)^{3/4}$ where $f(w)$ is unigram frequency
\item Exponent $< 1$ upweights rare words relative to frequency
\item Empirically chosen by Mikolov et al.\ (2013) --- not theoretically derived
\end{itemize}
\vspace{0.3em}
\textbf{Implicit Matrix Factorization} (Levy \& Goldberg, 2014):

SGNS implicitly factorizes a shifted PMI matrix:
\[
v_w \cdot v_c' \approx \text{PMI}(w, c) - \log k
\]
where PMI$(w,c) = \log \frac{P(w,c)}{P(w)P(c)}$ and $k$ = number of negatives
\bottomnote{Negative sampling implicitly factorizes a shifted PMI matrix}
\end{frame}

%% Slide A4: Bellman Equation Convergence
\begin{frame}[t]{Bellman Equation Convergence}
\textbf{Robbins-Monro Conditions} for step sizes $\alpha_t$:
\[
\sum_{t=0}^{\infty} \alpha_t = \infty, \qquad \sum_{t=0}^{\infty} \alpha_t^2 < \infty
\]
\vspace{0.3em}
\textbf{Contraction Mapping Argument:}
\begin{itemize}
\item The Bellman optimality operator $T$ is a $\gamma$-contraction in $\ell_\infty$-norm
\item $\|TQ_1 - TQ_2\|_\infty \leq \gamma \|Q_1 - Q_2\|_\infty$
\item By Banach fixed-point theorem, $T$ has unique fixed point $Q^*$
\item Q-learning converges to $Q^*$ when Robbins-Monro conditions hold and all $(s,a)$ pairs visited infinitely often
\end{itemize}
\vspace{0.3em}
\textbf{Fixed $\alpha$ Issue:}
\begin{itemize}
\item Constant $\alpha$ violates $\sum \alpha_t^2 < \infty$ --- Q-values oscillate around $Q^*$
\item In practice: fixed $\alpha$ works well in non-stationary environments (tracks changes)
\end{itemize}
\bottomnote{Watkins \& Dayan (1992): formal convergence proof requires decaying step sizes and full exploration}
\end{frame}

%% Slide A5: DQN Architecture Details
\begin{frame}[t]{DQN Architecture Details}
\textbf{Experience Replay Buffer:}
\begin{itemize}
\item Store transitions $(s, a, r, s', \text{done})$ in buffer of size $N$ (e.g., $10^6$)
\item Sample uniform random mini-batches for training
\item Breaks temporal correlation $\rightarrow$ approximately i.i.d.\ data
\end{itemize}
\vspace{0.3em}
\textbf{Target Network Updates:}
\begin{itemize}
\item \textbf{Hard update}: Copy $\theta^- \leftarrow \theta$ every $C$ steps (Mnih et al., 2015)
\item \textbf{Soft update}: $\theta^- \leftarrow \tau\theta + (1-\tau)\theta^-$ with $\tau \ll 1$ (Polyak averaging)
\end{itemize}
\vspace{0.3em}
\textbf{Extensions:}
\begin{itemize}
\item \textbf{Double DQN} (van Hasselt et al., 2016): Decouple action selection from evaluation to reduce overestimation bias
\item \textbf{Dueling DQN} (Wang et al., 2016): Separate value $V(s)$ and advantage $A(s,a)$ streams: $Q(s,a) = V(s) + A(s,a) - \bar{A}(s)$
\end{itemize}
\bottomnote{Mnih et al. (2015): DQN achieved human-level play on 29/49 Atari games}
\end{frame}

%% Slide A6: Word Embedding Bias and Fairness
\begin{frame}[t]{Word Embedding Bias and Fairness}
\textbf{Bias in Word Embeddings} (Bolukbasi et al., 2016):
\begin{itemize}
\item Gender: he:doctor :: she:nurse (reflects training corpus stereotypes)
\item Race, religion, and other protected attributes similarly affected
\end{itemize}
\vspace{0.3em}
\textbf{Debiasing Techniques:}
\begin{itemize}
\item \textbf{Post-hoc projection}: Remove gender direction from embedding space
\item \textbf{Counterfactual data augmentation}: Balance training examples
\item \textbf{Adversarial debiasing}: Train to be invariant to protected attributes
\end{itemize}
\vspace{0.3em}
\textbf{Finance Implications:}
\begin{itemize}
\item Biased embeddings in \highlight{credit scoring} can violate fair lending laws
\item \highlight{Hiring tools} using biased embeddings risk discrimination claims
\item \textbf{EU AI Act}: High-risk AI systems (credit, hiring) require bias auditing
\end{itemize}
\bottomnote{Embedding bias is a compliance risk in regulated financial services}
\end{frame}

%% Slide A7: SARSA vs Q-Learning
\begin{frame}[t]{SARSA vs Q-Learning}
\textbf{SARSA} (on-policy):
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\bigl[r + \gamma Q(s', a') - Q(s,a)\bigr]
\]
Uses the \textbf{actual next action} $a'$ chosen by the current policy.

\vspace{0.3em}
\textbf{Q-Learning} (off-policy):
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\bigl[r + \gamma \max_{a'} Q(s', a') - Q(s,a)\bigr]
\]
Uses the \textbf{greedy maximum} regardless of what action was actually taken.

\vspace{0.3em}
\textbf{Key Differences:}
\begin{itemize}
\item \textbf{Safety}: SARSA accounts for exploration risk; Q-learning ignores it
\item \textbf{Cliff-walking example}: SARSA learns the safe path (away from cliff edge); Q-learning learns the optimal but risky path (along the edge)
\item \textbf{Convergence}: Both converge given Robbins-Monro conditions; Q-learning to $Q^*$, SARSA to $Q^\pi$
\end{itemize}
\bottomnote{SARSA: safer path; Q-learning: optimal path}
\end{frame}

%% Slide A8: References and Further Reading
\begin{frame}[t]{Appendix References and Further Reading}
\small
\begin{itemize}
\item Levy, O. \& Goldberg, Y. (2014). Neural word embedding as implicit matrix factorization. \textit{NeurIPS}, 2177--2185.
\item Bolukbasi, T. et al. (2016). Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. \textit{NeurIPS}, 4349--4357.
\item Watkins, C. \& Dayan, P. (1992). Q-learning. \textit{Machine Learning}, 8(3-4), 279--292.
\item van Hasselt, H., Guez, A., \& Silver, D. (2016). Deep reinforcement learning with double Q-learning. \textit{AAAI}, 2094--2100.
\item Wang, Z., Schaul, T., Hessel, M., et al. (2016). Dueling network architectures for deep reinforcement learning. \textit{ICML}, 1995--2003.
\item Sutton, R. \& Barto, A. (2018). \textit{Reinforcement Learning: An Introduction}, Chapters 6 and 16. MIT Press.
\end{itemize}
\bottomnote{All appendix references are freely available online}
\end{frame}

\end{document}
