\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L06: Embeddings \& RL]{L06: Embeddings \& RL}
\subtitle{Text Representations and Sequential Decision Making}
\author{Methods and Algorithms}
\institute{MSc Data Science}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Problem}

\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item \textbf{Derive} the Skip-Gram objective and analyze the negative sampling approximation
\item \textbf{Evaluate} static vs.\ contextual embeddings for domain-specific NLP tasks (e.g., FinBERT)
\item \textbf{Analyze} the convergence properties of Q-learning and the role of the exploration--exploitation tradeoff
\item \textbf{Critique} RL-based trading strategies and their limitations (transaction costs, non-stationarity, overfitting)
\end{enumerate}
\bottomnote{Bloom's taxonomy levels 4--5: Analyze, Evaluate, Derive, Critique}
\end{frame}

\begin{frame}[t]{The Business Problem}
\textbf{Text Data Challenge}
\begin{itemize}
\item Financial news, reports, social media contain valuable signals
\item Text is unstructured---how to feed it to ML models?
\item Need to capture semantic meaning (``bullish'' similar to ``positive'')
\end{itemize}
\vspace{0.5em}
\textbf{Sequential Decision Challenge}
\begin{itemize}
\item Trading requires sequences of buy/sell/hold decisions
\item Actions have delayed consequences (profit realized later)
\end{itemize}
\bottomnote{Embeddings solve text, RL solves sequential decisions}
\end{frame}

\begin{frame}[t]{Why Is This Hard?}
\begin{center}
\textit{``There are only two hard problems in NLP: understanding language, and getting your regex to work.''} --- adapted from Phil Karlton
\end{center}
\vspace{1em}
\textbf{This Lecture:}
\begin{itemize}
\item Part 1: Turn text into numbers that capture meaning (Embeddings)
\item Part 2: Learn to make good decisions over time (RL)
\end{itemize}
\bottomnote{XKCD \#1838 by Randall Munroe (CC BY-NC 2.5): ``Machine Learning'' --- relevant to both topics}
\end{frame}

\section{Method}

\begin{frame}[t]{Key Equations}
\textbf{Embeddings --- Skip-Gram Objective:}
\[
\max \sum_{t=1}^{T} \sum_{\substack{-c \leq j \leq c \\ j \neq 0}} \log p(w_{t+j} \mid w_t)
\]

\textbf{Cosine Similarity:}
\[
\text{sim}(u, v) = \frac{u \cdot v}{\|u\| \, \|v\|}
\]

\textbf{Reinforcement Learning --- Bellman Equation:}
\[
Q^*(s,a) = \mathbb{E}\bigl[r + \gamma \max_{a'} Q^*(s', a') \mid s, a\bigr]
\]

\textbf{TD Update (Q-Learning):}
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\bigl[r + \gamma \max_{a'} Q(s', a') - Q(s,a)\bigr]
\]
\bottomnote{These four equations are the mathematical backbone of this lecture}
\end{frame}

\begin{frame}[t]{Word Embedding Space}
\begin{center}
\includegraphics[width=0.65\textwidth]{01_word_embedding_space/chart.pdf}
\end{center}
\bottomnote{Similar words cluster together in embedding space}
\end{frame}

\begin{frame}[t]{Embedding Similarity}
\begin{center}
\includegraphics[width=0.50\textwidth]{02_similarity_heatmap/chart.pdf}
\end{center}
\bottomnote{Cosine similarity captures semantic relationships}
\end{frame}

\begin{frame}[t]{RL: Agent-Environment Loop}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_rl_loop/chart.pdf}
\end{center}
\bottomnote{Agent takes actions, receives rewards, learns optimal policy}
\end{frame}

\begin{frame}[t]{Q-Learning: Value Function}
\begin{center}
\includegraphics[width=0.45\textwidth]{04_q_learning_grid/chart.pdf}
\end{center}
\bottomnote{Q-values show expected reward from each state-action}
\end{frame}

\section{Solution}

\begin{frame}[t]{Learning Progress}
\begin{center}
\includegraphics[width=0.65\textwidth]{05_reward_curves/chart.pdf}
\end{center}
\bottomnote{RL agents improve through exploration and exploitation}
\end{frame}

\begin{frame}[t]{Learned Trading Policy}
\begin{center}
\includegraphics[width=0.65\textwidth]{06_policy_viz/chart.pdf}
\end{center}
\bottomnote{Policy maps states to actions (when to buy/sell/hold)}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Explore word embeddings with Word2Vec
    \item Exercise 2: Implement basic Q-learning
    \item Exercise 3: Apply RL to a simple trading environment
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/github/Digital-AI-Finance/methods-algorithms/blob/master/notebooks/L06_embeddings_rl.ipynb}
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{Decision Framework}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\bottomnote{Embeddings for text, RL for sequential decisions with delayed rewards}
\end{frame}

\section{Summary}

\begin{frame}[t]{References}
  \footnotesize
  \begin{itemize}
    \item Mikolov et al. (2013). \textit{Efficient Estimation of Word Representations in Vector Space}. arXiv.
    \item Sutton, R. \& Barto, A. (2018). \textit{Reinforcement Learning: An Introduction}. MIT Press.
    \item Jurafsky \& Martin (2024). \textit{Speech and Language Processing}. \url{https://web.stanford.edu/~jurafsky/slp3/}
  \end{itemize}
\end{frame}

\end{document}
