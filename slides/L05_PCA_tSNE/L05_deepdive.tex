\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L05: PCA \& t-SNE Deep Dive]{L05: PCA \& t-SNE}
\subtitle{Deep Dive: Theory, Implementation, and Applications}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%% PART 1: PCA FOUNDATIONS
\section{Problem}

\begin{frame}[t]{Part 1: PCA Foundations}
\textbf{Principal Component Analysis (PCA)}
\begin{itemize}
\item Find orthogonal directions of maximum variance
\item Project data onto these directions
\item Reduce dimensions while preserving information
\end{itemize}
\vspace{0.5em}
\textbf{Key Properties:}
\begin{itemize}
\item Linear transformation
\item Components are uncorrelated
\item Partially reversible (lossy reconstruction when $k < p$)
\end{itemize}
\bottomnote{PCA: one of the most fundamental tools in data science}
\end{frame}

\begin{frame}[t]{Mathematical Foundation}
\textbf{Covariance Matrix:}
\[
\Sigma = \frac{1}{n-1} X_c^T X_c \quad \text{where } X_c = X - \bar{X} \text{ (mean-centered)}
\]

\textbf{Eigendecomposition:}
\[
\Sigma v = \lambda v
\]
where $v$ = eigenvector (principal direction), $\lambda$ = eigenvalue (variance)

\textbf{Projection:}
\[
Z = X_c W_k \quad \text{where } X_c \text{ is centered, } W_k = [v_1, \ldots, v_k]
\]
\bottomnote{Eigenvalues tell us how much variance each component captures}
\end{frame}

\begin{frame}[t]{Why Eigenvectors? Optimality Proof}
\textbf{Find direction $w$ maximizing variance:}
\[
\max_{w} w^T \Sigma w \quad \text{subject to } \|w\| = 1
\]

\textbf{Lagrangian:}
\[
L = w^T \Sigma w - \lambda(w^T w - 1)
\]

\textbf{First-order condition:}
\[
\nabla_w L = 2\Sigma w - 2\lambda w = 0
\]

\textbf{Result:}
\[
\Sigma w = \lambda w \quad \text{(eigenvalue equation)}
\]

\textbf{Maximum variance = largest eigenvalue $\lambda_1$}
\bottomnote{The constrained optimization proof shows eigenvectors are optimal}
\end{frame}

\begin{frame}{SVD--PCA Equivalence: The Computational Connection}
\textbf{Theorem}: The principal components of $X_c$ are the right singular vectors of $X_c$.

\textbf{Proof sketch}:
\begin{enumerate}
\item The covariance matrix: $C = \frac{1}{n-1}X_c^\top X_c$
\item SVD of $X_c$: $X_c = U\Sigma V^\top$
\item Then: $X_c^\top X_c = V\Sigma^\top U^\top U\Sigma V^\top = V\Sigma^2 V^\top$
\item Therefore: $C = \frac{1}{n-1}V\Sigma^2 V^\top$
\item Since $C V = V \cdot \frac{\Sigma^2}{n-1}$, columns of $V$ are eigenvectors of $C$
\end{enumerate}

\medskip
\textbf{Eigenvalues}: $\lambda_k = \sigma_k^2/(n-1)$ where $\sigma_k$ are singular values

\medskip
\textbf{Computational advantage}: SVD is numerically more stable than eigendecomposition of $X_c^\top X_c$ (avoids squaring condition number)

\bottomnote{Practical implementations (scikit-learn, R) use SVD internally, not eigendecomposition.}
\end{frame}

\begin{frame}[t]{Variance Explained}
\textbf{Proportion of Variance:}
\[
\text{Explained Variance Ratio}_i = \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}
\]

\textbf{Cumulative Variance:}
\[
\text{Cumulative}_k = \sum_{i=1}^{k} \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}
\]
\vspace{0.5em}
\textbf{Rules of thumb for choosing k:}
\begin{itemize}
\item Keep 80-95\% of total variance
\item Use scree plot ``elbow'' method
\item Kaiser criterion: $\lambda > 1$ (valid for correlation matrix / standardized data only)
\end{itemize}
\bottomnote{Balance dimensionality reduction with information preservation}
\end{frame}

\section{Method}

\begin{frame}[t]{Scree Plot}
\begin{center}
\includegraphics[width=0.65\textwidth]{01_scree_plot/chart.pdf}
\end{center}
\bottomnote{Look for the ``elbow'' where variance explained drops off}
\end{frame}

\begin{frame}[t]{Principal Components Visualization}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{02_principal_components/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{PC1 captures the dominant trend, PC2 the residual variation}
\end{frame}

\begin{frame}[t]{Reconstruction}
\textbf{From k components back to original space:}
\[
\hat{X} = Z W_k^T + \bar{X} \quad \text{(add mean back for original scale)}
\]

\textbf{Reconstruction Error:}
\[
\text{Error} = ||X - \hat{X}||_F^2 = \sum_{i=k+1}^{p} \lambda_i
\]
\bottomnote{Reconstruction error = sum of discarded eigenvalues}
\end{frame}

\begin{frame}[t]{Reconstruction Error vs Components}
\begin{center}
\includegraphics[width=0.65\textwidth]{03_reconstruction/chart.pdf}
\end{center}
\bottomnote{Adding more components always reduces error (but diminishing returns)}
\end{frame}

\begin{frame}[t]{Statistical Inference for PCA}
\textbf{Bootstrap Confidence Intervals:}
\begin{itemize}
\item Resample data, recompute PCA, track loading stability
\end{itemize}

\textbf{Parallel Analysis:}
\begin{itemize}
\item Compare eigenvalues to random data
\item Keep components where $\lambda_{data} > \lambda_{random}$
\end{itemize}

\textbf{Cross-Validation for k:}
\begin{itemize}
\item Split data, train PCA, test reconstruction on holdout
\end{itemize}

\textbf{For t-SNE:}
\begin{itemize}
\item Run multiple times with different seeds
\item Unstable clusters may be artifacts
\end{itemize}
\bottomnote{MSc-level: always quantify uncertainty in dimensionality reduction}
\end{frame}

%% PART 2: PCA APPLICATIONS
\begin{frame}[t]{Part 2: PCA in Finance}
\textbf{Portfolio Risk Decomposition:}
\begin{itemize}
\item PC1 often represents ``market factor''
\item PC2-3 may capture sector/size factors
\item Higher PCs: idiosyncratic risk
\end{itemize}

\vspace{0.3em}
\textbf{Example: 10-Stock Portfolio}
\begin{itemize}
\item PC1 loadings $\approx [0.31, 0.32, 0.30, \ldots]$ --- uniform (market factor)
\item PC2: tech stocks positive, banks negative (sector rotation)
\item First 3 PCs explain $\sim$75\% of portfolio variance
\end{itemize}

\vspace{0.5em}
\textbf{Applications:}
\begin{itemize}
\item Risk factor modeling
\item Dimensionality reduction for trading signals
\item Noise reduction in time series
\item Feature extraction for ML models
\end{itemize}
\bottomnote{PCA reveals latent structure in financial data}
\end{frame}

\begin{frame}[t]{Finance Application: Yield Curve PCA}
\textbf{The Canonical Finance Example}
\begin{itemize}
\item Yield curves decompose into $\sim$3 principal components
\item PC1 = \textbf{Level} (parallel shift): explains $\sim$85\% variance
\item PC2 = \textbf{Slope} (steepening/flattening): explains $\sim$10\% variance
\item PC3 = \textbf{Curvature} (butterfly): explains $\sim$3\% variance
\end{itemize}
\vspace{0.3em}
\textbf{Together explain 98\%+ of yield curve movements}
\begin{itemize}
\item Used daily in every bank's risk management system
\item Foundation for interest rate hedging strategies
\end{itemize}
\bottomnote{Level/slope/curvature: industry-standard yield curve decomposition}
\end{frame}

\begin{frame}[t]{Yield Curve Factor Interpretation}
\textbf{Typical Loadings Pattern:}
\begin{itemize}
\item \textbf{PC1 (Level):} Flat loadings across maturities---all rates move together
\item \textbf{PC2 (Slope):} Negative short end, positive long end---curve steepens/flattens
\item \textbf{PC3 (Curvature):} Negative at ends, positive in middle---butterfly trades
\end{itemize}
\vspace{0.3em}
\textbf{Trading Applications:}
\begin{itemize}
\item Duration-neutral hedging (hedge PC1 exposure)
\item Curve trades (exploit PC2 movements)
\item Butterfly spreads (exploit PC3 movements)
\end{itemize}
\bottomnote{PCA on interest rates is foundational fixed-income knowledge}
\end{frame}

\begin{frame}[t]{PCA Limitations}
\textbf{When PCA Falls Short:}
\begin{itemize}
\item Non-linear relationships (curved manifolds)
\item Cluster structure not aligned with variance
\item Discrete or categorical data
\item Outliers heavily influence results
\end{itemize}

\vspace{0.5em}
\textbf{Solutions:}
\begin{itemize}
\item Kernel PCA (implicit non-linear mapping via kernel trick)
\item Robust PCA (outlier-resistant)
\item t-SNE/UMAP (for visualization)
\end{itemize}
\bottomnote{PCA assumes linear structure; Gaussian NOT required but PCA is optimal for Gaussian data}
\end{frame}

\section{Solution}

%% PART 3: t-SNE
\begin{frame}[t]{Part 3: t-SNE Introduction}
\textbf{t-Distributed Stochastic Neighbor Embedding}
\begin{itemize}
\item Non-linear dimensionality reduction
\item Optimized for visualization (2D/3D)
\item Preserves local neighborhood structure
\end{itemize}

\vspace{0.5em}
\textbf{Key Idea:}
\begin{itemize}
\item Convert distances to probabilities
\item In high-D: Gaussian similarities
\item In low-D: t-distribution similarities
\item Minimize KL divergence between distributions
\end{itemize}
\bottomnote{t-SNE: visualization method, NOT for preprocessing}
\end{frame}

\begin{frame}[t]{t-SNE: Mathematical Formulation}
\textbf{High-dimensional similarity:}
\[
p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}
\]

\textbf{Symmetrize:} $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$ (joint probability)

\textbf{Low-dimensional similarity (t-distribution):}
\[
q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}
\]

\textbf{Objective: Minimize KL divergence}
\[
KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
\]

Note: KL is asymmetric---penalizes large $p_{ij}$ with small $q_{ij}$ more than reverse. This preserves local structure (nearby points stay nearby).

\bottomnote{t-distribution solves the \textbf{crowding problem}: in high-D, moderate distances become small in low-D, causing points to collapse. Heavy tails allow dissimilar points to spread apart.}
\end{frame}

\begin{frame}[t]{Perplexity: Low (Local Focus)}
\begin{center}
\includegraphics[width=0.55\textwidth]{04a_tsne_perplexity_5/chart.pdf}
\end{center}
\bottomnote{Low perplexity (5): tight clusters, focus on nearest neighbors only}
\end{frame}

\begin{frame}[t]{Perplexity: Default (Balanced)}
\begin{center}
\includegraphics[width=0.55\textwidth]{04b_tsne_perplexity_30/chart.pdf}
\end{center}
\bottomnote{Default perplexity (30): balanced local and global structure}
\end{frame}

\begin{frame}[t]{Perplexity: High (Global Focus)}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{04c_tsne_perplexity_100/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{High perplexity (100): more spread, clusters may merge}
\end{frame}

\begin{frame}[t]{Perplexity Guidelines}
\textbf{Perplexity} controls the balance between local and global structure:

Perplexity $= 2^H$ where $H$ is entropy; roughly the effective number of neighbors considered.

\begin{itemize}
\item Low perplexity (5-10): Focus on very local structure
\item Medium perplexity (30-50): Balanced (default)
\item High perplexity (100+): More global structure
\end{itemize}

\vspace{0.5em}
\textbf{Guidelines:}
\begin{itemize}
\item Should be smaller than number of points
\item Larger datasets can use higher perplexity
\item Run multiple perplexities to validate findings
\end{itemize}
\bottomnote{Results can vary significantly with perplexity choice}
\end{frame}

\begin{frame}[t]{t-SNE Caveats}
\textbf{Important Limitations:}
\begin{itemize}
\item Non-deterministic (run multiple times)
\item Cluster sizes are not meaningful
\item Distances between clusters are not meaningful
\item Slow for large datasets (O($n^2$); Barnes-Hut approximation gives O($n \log n$))
\end{itemize}

\vspace{0.5em}
\textbf{Best Practices:}
\begin{itemize}
\item Use PCA first to reduce to 30-50 dims
\item Run multiple times with different seeds
\item Don't over-interpret cluster sizes/distances
\item Use for exploration, not final conclusions
\end{itemize}
\bottomnote{t-SNE shows IF clusters exist, not HOW they relate}
\end{frame}

%% PART 4: COMPARISON
\begin{frame}[t]{Part 4: PCA on Non-linear Manifolds}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{05a_pca_swiss_roll/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{PCA (linear) cannot unroll the Swiss roll - structure overlaps}
\end{frame}

\begin{frame}[t]{t-SNE on Non-linear Manifolds}
\begin{center}
\includegraphics[width=0.55\textwidth]{05b_tsne_swiss_roll/chart.pdf}
\end{center}
\bottomnote{t-SNE (non-linear) successfully unrolls the manifold structure}
\end{frame}

\begin{frame}[t]{Comparison Table}
\begin{center}
\small
\begin{tabular}{l|cc}
\toprule
\textbf{Aspect} & \textbf{PCA} & \textbf{t-SNE} \\
\midrule
Type & Linear & Non-linear \\
Speed & Fast $O(np^2)$ & Slow $O(n^2)$ \\
Deterministic & Yes & No \\
Preserves & Global variance & Local neighbors \\
Reversible & Lossy if $k<p$ & No \\
Use for ML & Yes (preprocessing) & No \\
Visualization & Okay & Excellent \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{Use PCA for preprocessing, t-SNE for visualization only}
\end{frame}

\begin{frame}[t]{Cluster Preservation: Original Data}
\begin{center}
\includegraphics[width=0.55\textwidth]{06a_original_clusters/chart.pdf}
\end{center}
\bottomnote{MNIST digits (64 dims): classes overlap when viewed in raw pixel space}
\end{frame}

\begin{frame}[t]{Cluster Preservation: PCA Projection}
\begin{center}
\includegraphics[width=0.55\textwidth]{06b_pca_cluster_projection/chart.pdf}
\end{center}
\bottomnote{PCA finds max-variance directions -- partial digit separation}
\end{frame}

\begin{frame}[t]{Cluster Preservation: t-SNE Projection}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.50\textwidth]{06c_tsne_cluster_projection/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{t-SNE preserves local neighborhoods -- clear digit cluster separation}
\end{frame}

\begin{frame}[t]{When to Use Which}
\textbf{Use PCA When:}
\begin{itemize}
\item Preprocessing for ML (reduce features)
\item Linear relationships expected
\item Need reversibility (reconstruction)
\item Speed matters
\end{itemize}

\vspace{0.5em}
\textbf{Use t-SNE When:}
\begin{itemize}
\item Visualizing high-dimensional data
\item Looking for cluster structure
\item Non-linear manifolds expected
\item Exploratory analysis
\end{itemize}
\bottomnote{Often use both: PCA first to 30-50 dims, then t-SNE for visualization}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Apply PCA to high-dimensional finance data
    \item Exercise 2: Visualize clusters with t-SNE
    \item Exercise 3: Compare PCA vs t-SNE for different datasets
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/} See course materials
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{Decision Framework}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{Consider purpose: preprocessing (PCA) vs visualization (t-SNE)}
\end{frame}

\section{Implementation}

%% PART 5: IMPLEMENTATION
\begin{frame}[t]{Part 5: Implementation}
\textbf{PCA in scikit-learn:}
\small
\begin{itemize}
\item \texttt{PCA(n\_components=k)}: Keep k components
\item \texttt{PCA(n\_components=0.95)}: Keep 95\% variance
\item \texttt{pca.explained\_variance\_ratio\_}: Variance per component
\item \texttt{pca.inverse\_transform()}: Reconstruct original
\item Note: sklearn uses truncated SVD internally (more stable than eigendecomposition)
\end{itemize}
\normalsize

\vspace{0.5em}
\textbf{t-SNE in scikit-learn:}
\small
\begin{itemize}
\item \texttt{TSNE(n\_components=2, perplexity=30)}
\item Always normalize data first
\item Consider PCA preprocessing for speed
\end{itemize}
\bottomnote{Standardize data before PCA; normalize before t-SNE}
\end{frame}

\begin{frame}[t]{UMAP: Modern Alternative}
\textbf{Uniform Manifold Approximation and Projection}
\begin{itemize}
\item Faster than t-SNE
\item Better preserves global structure
\item Can embed new points (unlike t-SNE)
\item Hyperparameters: n\_neighbors, min\_dist
\end{itemize}

\vspace{0.5em}
\textbf{When to use UMAP:}
\begin{itemize}
\item Large datasets (faster than t-SNE)
\item Need to embed new data points
\item Want more preserved global structure
\end{itemize}
\bottomnote{UMAP increasingly popular; see McInnes et al. (2018) for comparison}
\end{frame}

\section{Summary}

\begin{frame}[t]{Summary}
\textbf{PCA:}
\begin{itemize}
\item Linear, fast, reversible
\item Use for preprocessing and feature extraction
\item Choose k by variance explained or elbow
\end{itemize}

\textbf{t-SNE:}
\begin{itemize}
\item Non-linear, slow, visualization-only
\item Excellent for exploring cluster structure
\item Don't interpret distances or sizes literally
\end{itemize}

\textbf{Common Pipeline:} Standardize $\rightarrow$ PCA (30-50) $\rightarrow$ t-SNE (2D)
\bottomnote{Next: Embeddings and Reinforcement Learning}
\end{frame}

\begin{frame}[t]{References}
\small
\textbf{Textbooks:}
\begin{itemize}
\item James et al. (2021). \textit{ISLR}, Chapter 12: Unsupervised Learning
\item Hastie et al. (2009). \textit{ESL}, Chapter 14: Unsupervised Learning
\end{itemize}

\textbf{Original Papers:}
\begin{itemize}
\item Pearson (1901). On Lines and Planes of Closest Fit
\item van der Maaten \& Hinton (2008). Visualizing Data using t-SNE
\item McInnes et al. (2018). UMAP
\end{itemize}

\textbf{Documentation:}
\begin{itemize}
\item scikit-learn: \texttt{sklearn.decomposition.PCA}
\item scikit-learn: \texttt{sklearn.manifold.TSNE}
\end{itemize}
\bottomnote{t-SNE paper: one of the most influential visualization papers}
\end{frame}

\end{document}
