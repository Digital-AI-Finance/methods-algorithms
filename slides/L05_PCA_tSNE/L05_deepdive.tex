\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L05: PCA \& t-SNE Deep Dive]{L05: PCA \& t-SNE}
\subtitle{Deep Dive: Theory, Implementation, and Applications}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%% PART 1: PCA FOUNDATIONS
\section{Problem}

\begin{frame}[t]{Part 1: PCA Foundations}
\textbf{Principal Component Analysis (PCA)}
\begin{itemize}
\item Find orthogonal directions of maximum variance
\item Project data onto these directions
\item Reduce dimensions while preserving information
\end{itemize}
\vspace{0.5em}
\textbf{Key Properties:}
\begin{itemize}
\item Linear transformation
\item Components are uncorrelated
\item Reversible (can reconstruct original data)
\end{itemize}
\bottomnote{PCA: one of the most fundamental tools in data science}
\end{frame}

\begin{frame}[t]{Mathematical Foundation}
\textbf{Covariance Matrix:}
\[
\Sigma = \frac{1}{n-1} X^T X \quad \text{(centered data)}
\]

\textbf{Eigendecomposition:}
\[
\Sigma v = \lambda v
\]
where $v$ = eigenvector (principal direction), $\lambda$ = eigenvalue (variance)

\textbf{Projection:}
\[
Z = X W_k \quad \text{where } W_k = [v_1, v_2, \ldots, v_k]
\]
\bottomnote{Eigenvalues tell us how much variance each component captures}
\end{frame}

\begin{frame}[t]{Variance Explained}
\textbf{Proportion of Variance:}
\[
\text{Explained Variance Ratio}_i = \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}
\]

\textbf{Cumulative Variance:}
\[
\text{Cumulative}_k = \sum_{i=1}^{k} \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}
\]
\vspace{0.5em}
\textbf{Rules of thumb for choosing k:}
\begin{itemize}
\item Keep 80-95\% of total variance
\item Use scree plot ``elbow'' method
\item Kaiser criterion: keep components with $\lambda > 1$
\end{itemize}
\bottomnote{Balance dimensionality reduction with information preservation}
\end{frame}

\section{Method}

\begin{frame}[t]{Scree Plot}
\begin{center}
\includegraphics[width=0.65\textwidth]{01_scree_plot/chart.pdf}
\end{center}
\bottomnote{Look for the ``elbow'' where variance explained drops off}
\end{frame}

\begin{frame}[t]{Principal Components Visualization}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{02_principal_components/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{PC1 captures the dominant trend, PC2 the residual variation}
\end{frame}

\begin{frame}[t]{Reconstruction}
\textbf{From k components back to original space:}
\[
\hat{X} = Z W_k^T = X W_k W_k^T
\]

\textbf{Reconstruction Error:}
\[
\text{Error} = ||X - \hat{X}||_F^2 = \sum_{i=k+1}^{p} \lambda_i
\]
\bottomnote{Reconstruction error = sum of discarded eigenvalues}
\end{frame}

\begin{frame}[t]{Reconstruction Error vs Components}
\begin{center}
\includegraphics[width=0.65\textwidth]{03_reconstruction/chart.pdf}
\end{center}
\bottomnote{Adding more components always reduces error (but diminishing returns)}
\end{frame}

%% PART 2: PCA APPLICATIONS
\begin{frame}[t]{Part 2: PCA in Finance}
\textbf{Portfolio Risk Decomposition:}
\begin{itemize}
\item PC1 often represents ``market factor''
\item PC2-3 may capture sector/size factors
\item Higher PCs: idiosyncratic risk
\end{itemize}

\vspace{0.5em}
\textbf{Applications:}
\begin{itemize}
\item Risk factor modeling
\item Dimensionality reduction for trading signals
\item Noise reduction in time series
\item Feature extraction for ML models
\end{itemize}
\bottomnote{PCA reveals latent structure in financial data}
\end{frame}

\begin{frame}[t]{PCA Limitations}
\textbf{When PCA Falls Short:}
\begin{itemize}
\item Non-linear relationships (curved manifolds)
\item Cluster structure not aligned with variance
\item Discrete or categorical data
\item Outliers heavily influence results
\end{itemize}

\vspace{0.5em}
\textbf{Solutions:}
\begin{itemize}
\item Kernel PCA (non-linear)
\item Robust PCA (outlier-resistant)
\item t-SNE/UMAP (for visualization)
\end{itemize}
\bottomnote{PCA assumes linear structure and Gaussian-like distributions}
\end{frame}

\section{Solution}

%% PART 3: t-SNE
\begin{frame}[t]{Part 3: t-SNE Introduction}
\textbf{t-Distributed Stochastic Neighbor Embedding}
\begin{itemize}
\item Non-linear dimensionality reduction
\item Optimized for visualization (2D/3D)
\item Preserves local neighborhood structure
\end{itemize}

\vspace{0.5em}
\textbf{Key Idea:}
\begin{itemize}
\item Convert distances to probabilities
\item In high-D: Gaussian similarities
\item In low-D: t-distribution similarities
\item Minimize KL divergence between distributions
\end{itemize}
\bottomnote{t-SNE: visualization method, NOT for preprocessing}
\end{frame}

\begin{frame}[t]{t-SNE: Mathematical Formulation}
\textbf{High-dimensional similarity:}
\[
p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}
\]

\textbf{Low-dimensional similarity (t-distribution):}
\[
q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}
\]

\textbf{Objective: Minimize KL divergence}
\[
KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
\]
\bottomnote{t-distribution has heavier tails, allowing better separation in low-D}
\end{frame}

\begin{frame}[t]{Perplexity: Low (Local Focus)}
\begin{center}
\includegraphics[width=0.55\textwidth]{04a_tsne_perplexity_5/chart.pdf}
\end{center}
\bottomnote{Low perplexity (5): tight clusters, focus on nearest neighbors only}
\end{frame}

\begin{frame}[t]{Perplexity: Default (Balanced)}
\begin{center}
\includegraphics[width=0.55\textwidth]{04b_tsne_perplexity_30/chart.pdf}
\end{center}
\bottomnote{Default perplexity (30): balanced local and global structure}
\end{frame}

\begin{frame}[t]{Perplexity: High (Global Focus)}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{04c_tsne_perplexity_100/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{High perplexity (100): more spread, clusters may merge}
\end{frame}

\begin{frame}[t]{Perplexity Guidelines}
\textbf{Perplexity} controls the balance between local and global structure:
\begin{itemize}
\item Low perplexity (5-10): Focus on very local structure
\item Medium perplexity (30-50): Balanced (default)
\item High perplexity (100+): More global structure
\end{itemize}

\vspace{0.5em}
\textbf{Guidelines:}
\begin{itemize}
\item Should be smaller than number of points
\item Larger datasets can use higher perplexity
\item Run multiple perplexities to validate findings
\end{itemize}
\bottomnote{Results can vary significantly with perplexity choice}
\end{frame}

\begin{frame}[t]{t-SNE Caveats}
\textbf{Important Limitations:}
\begin{itemize}
\item Non-deterministic (run multiple times)
\item Cluster sizes are not meaningful
\item Distances between clusters are not meaningful
\item Slow for large datasets (O($n^2$))
\end{itemize}

\vspace{0.5em}
\textbf{Best Practices:}
\begin{itemize}
\item Use PCA first to reduce to 30-50 dims
\item Run multiple times with different seeds
\item Don't over-interpret cluster sizes/distances
\item Use for exploration, not final conclusions
\end{itemize}
\bottomnote{t-SNE shows IF clusters exist, not HOW they relate}
\end{frame}

%% PART 4: COMPARISON
\begin{frame}[t]{Part 4: PCA on Non-linear Manifolds}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{05a_pca_swiss_roll/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{PCA (linear) cannot unroll the Swiss roll - structure overlaps}
\end{frame}

\begin{frame}[t]{t-SNE on Non-linear Manifolds}
\begin{center}
\includegraphics[width=0.55\textwidth]{05b_tsne_swiss_roll/chart.pdf}
\end{center}
\bottomnote{t-SNE (non-linear) successfully unrolls the manifold structure}
\end{frame}

\begin{frame}[t]{Comparison Table}
\begin{center}
\small
\begin{tabular}{l|cc}
\toprule
\textbf{Aspect} & \textbf{PCA} & \textbf{t-SNE} \\
\midrule
Type & Linear & Non-linear \\
Speed & Fast $O(np^2)$ & Slow $O(n^2)$ \\
Deterministic & Yes & No \\
Preserves & Global variance & Local neighbors \\
Reversible & Yes & No \\
Use for ML & Yes (preprocessing) & No \\
Visualization & Okay & Excellent \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{Use PCA for preprocessing, t-SNE for visualization only}
\end{frame}

\begin{frame}[t]{Cluster Preservation: Original Data}
\begin{center}
\includegraphics[width=0.55\textwidth]{06a_original_clusters/chart.pdf}
\end{center}
\bottomnote{High-dimensional data (50 dims): clusters overlap when viewed in 2D}
\end{frame}

\begin{frame}[t]{Cluster Preservation: PCA Projection}
\begin{center}
\includegraphics[width=0.55\textwidth]{06b_pca_cluster_projection/chart.pdf}
\end{center}
\bottomnote{PCA finds directions of max variance - some cluster separation}
\end{frame}

\begin{frame}[t]{Cluster Preservation: t-SNE Projection}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.50\textwidth]{06c_tsne_cluster_projection/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{t-SNE preserves local structure - clear cluster separation}
\end{frame}

\begin{frame}[t]{When to Use Which}
\textbf{Use PCA When:}
\begin{itemize}
\item Preprocessing for ML (reduce features)
\item Linear relationships expected
\item Need reversibility (reconstruction)
\item Speed matters
\end{itemize}

\vspace{0.5em}
\textbf{Use t-SNE When:}
\begin{itemize}
\item Visualizing high-dimensional data
\item Looking for cluster structure
\item Non-linear manifolds expected
\item Exploratory analysis
\end{itemize}
\bottomnote{Often use both: PCA first to 30-50 dims, then t-SNE for visualization}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Apply PCA to high-dimensional finance data
    \item Exercise 2: Visualize clusters with t-SNE
    \item Exercise 3: Compare PCA vs t-SNE for different datasets
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/} [TBD]
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{Decision Framework}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\vspace{-0.3em}
\bottomnote{Consider purpose: preprocessing (PCA) vs visualization (t-SNE)}
\end{frame}

\section{Implementation}

%% PART 5: IMPLEMENTATION
\begin{frame}[t]{Part 5: Implementation}
\textbf{PCA in scikit-learn:}
\small
\begin{itemize}
\item \texttt{PCA(n\_components=k)}: Keep k components
\item \texttt{PCA(n\_components=0.95)}: Keep 95\% variance
\item \texttt{pca.explained\_variance\_ratio\_}: Variance per component
\item \texttt{pca.inverse\_transform()}: Reconstruct original
\end{itemize}
\normalsize

\vspace{0.5em}
\textbf{t-SNE in scikit-learn:}
\small
\begin{itemize}
\item \texttt{TSNE(n\_components=2, perplexity=30)}
\item Always normalize data first
\item Consider PCA preprocessing for speed
\end{itemize}
\bottomnote{Standardize data before PCA; normalize before t-SNE}
\end{frame}

\begin{frame}[t]{UMAP: Modern Alternative}
\textbf{Uniform Manifold Approximation and Projection}
\begin{itemize}
\item Faster than t-SNE
\item Better preserves global structure
\item Can embed new points (unlike t-SNE)
\item Hyperparameters: n\_neighbors, min\_dist
\end{itemize}

\vspace{0.5em}
\textbf{When to use UMAP:}
\begin{itemize}
\item Large datasets (faster than t-SNE)
\item Need to embed new data points
\item Want more preserved global structure
\end{itemize}
\bottomnote{UMAP often preferred over t-SNE in modern practice}
\end{frame}

\section{Summary}

\begin{frame}[t]{Summary}
\textbf{PCA:}
\begin{itemize}
\item Linear, fast, reversible
\item Use for preprocessing and feature extraction
\item Choose k by variance explained or elbow
\end{itemize}

\textbf{t-SNE:}
\begin{itemize}
\item Non-linear, slow, visualization-only
\item Excellent for exploring cluster structure
\item Don't interpret distances or sizes literally
\end{itemize}

\textbf{Common Pipeline:} Standardize $\rightarrow$ PCA (30-50) $\rightarrow$ t-SNE (2D)
\bottomnote{Next: Embeddings and Reinforcement Learning}
\end{frame}

\begin{frame}[t]{References}
\small
\textbf{Textbooks:}
\begin{itemize}
\item James et al. (2021). \textit{ISLR}, Chapter 12: Unsupervised Learning
\item Hastie et al. (2009). \textit{ESL}, Chapter 14: Unsupervised Learning
\end{itemize}

\textbf{Original Papers:}
\begin{itemize}
\item Pearson (1901). On Lines and Planes of Closest Fit
\item van der Maaten \& Hinton (2008). Visualizing Data using t-SNE
\item McInnes et al. (2018). UMAP
\end{itemize}

\textbf{Documentation:}
\begin{itemize}
\item scikit-learn: \texttt{sklearn.decomposition.PCA}
\item scikit-learn: \texttt{sklearn.manifold.TSNE}
\end{itemize}
\bottomnote{t-SNE paper: one of the most influential visualization papers}
\end{frame}

\end{document}
