\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L03: KNN \& K-Means]{L03: K-Nearest Neighbors \& K-Means}
\subtitle{Classification and Clustering with Distance}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Problem}

\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item Analyze the bias-variance tradeoff in KNN as a function of $K$ and derive the consistency bound
\item Prove K-Means convergence and evaluate initialization strategies (K-Means++)
\item Evaluate cluster validity using silhouette analysis, Hopkins statistic, and Gap statistic
\item Compare distance metrics and analyze their impact on algorithm performance in high dimensions
\end{enumerate}
\vspace{0.5em}
\textbf{Finance Applications:} Customer segmentation, fraud detection
\bottomnote{Bloom's Level 4--5: Analyze, Evaluate, Prove, Compare}
\end{frame}

\begin{frame}[t]{The Business Problem}
\textbf{Two Distinct Problems}

\textbf{1. Classification (Supervised)}
\begin{itemize}
\item Given labeled examples: is this transaction fraudulent?
\item ``Show me similar past transactions and their outcomes''
\end{itemize}
\vspace{0.5em}
\textbf{2. Clustering (Unsupervised)}
\begin{itemize}
\item No labels: what natural customer segments exist?
\item ``Group customers by behavior for targeted marketing''
\end{itemize}
\bottomnote{KNN = classification with labels, K-Means = clustering without labels}
\end{frame}

\section{Method}

\begin{frame}[t]{KNN: Decision Boundaries}
\begin{center}
\includegraphics[width=0.65\textwidth]{01_knn_boundaries/chart.pdf}
\end{center}
\bottomnote{KNN creates non-linear, flexible decision boundaries based on local data}
\end{frame}

\begin{frame}[t]{Distance Metrics}
\begin{center}
\includegraphics[width=0.50\textwidth]{02_distance_metrics/chart.pdf}
\end{center}
\bottomnote{Choice of metric affects which points are considered ``nearest''}
\end{frame}

\begin{frame}[t]{Key Equations}
\textbf{Euclidean distance}:
\begin{equation}
d(\mathbf{x}, \mathbf{x}') = \sqrt{\sum_{j=1}^p (x_j - x'_j)^2}
\end{equation}

\textbf{KNN classification} (majority vote among $k$ nearest neighbors):
\begin{equation}
\hat{y} = \text{majority vote among } k \text{ nearest neighbors}
\end{equation}

\textbf{K-Means objective}:
\begin{equation}
\min_{\mu_1,\ldots,\mu_K} \sum_{k=1}^K \sum_{\mathbf{x}_i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2
\end{equation}

\textbf{Silhouette}: $s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} \in [-1, 1]$

\bottomnote{$a(i)$ = mean intra-cluster distance; $b(i)$ = mean nearest-cluster distance}
\end{frame}

\begin{frame}[t]{K-Means: The Algorithm}
\begin{center}
\includegraphics[width=0.65\textwidth]{03_kmeans_iteration/chart.pdf}
\end{center}
\bottomnote{Iteratively assign points and update centroids until convergence}
\end{frame}

\begin{frame}[t]{Choosing K: Elbow Method}
\begin{center}
\includegraphics[width=0.65\textwidth]{04_elbow_method/chart.pdf}
\end{center}
\bottomnote{Look for the ``elbow'' where adding clusters gives diminishing returns}
\end{frame}

\section{Solution}

\begin{frame}[t]{Cluster Quality: Silhouette Analysis}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_silhouette/chart.pdf}
\end{center}
\bottomnote{Silhouette score measures how similar points are to their own cluster}
\end{frame}

\begin{frame}[t]{K-Means Decision Regions}
\begin{center}
\includegraphics[width=0.65\textwidth]{06_voronoi/chart.pdf}
\end{center}
\bottomnote{Each region contains all points closest to one centroid}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Implement KNN classifier from scratch
    \item Exercise 2: Apply K-Means to customer segmentation data
    \item Exercise 3: Compare distance metrics and k values
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} See course materials for Colab notebook
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{Decision Framework}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\bottomnote{KNN for labeled data classification, K-Means for unlabeled clustering}
\end{frame}

\section{Summary}

\begin{frame}[t]{Key Takeaways}
  \textbf{Remember}
  \begin{itemize}
    \item KNN: supervised classification using nearest neighbors
    \item K-Means: unsupervised clustering with iterative centroids
    \item Distance metrics and K selection are critical choices
    \item Finance use cases: fraud detection, customer segmentation
  \end{itemize}
  \bottomnote{Next lecture: L04 Random Forests}
\end{frame}

\end{document}
