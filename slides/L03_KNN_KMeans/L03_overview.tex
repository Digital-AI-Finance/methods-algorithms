\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}

\definecolor{mlpurple}{HTML}{3333B2}
\definecolor{mlblue}{HTML}{0066CC}
\definecolor{mlgreen}{HTML}{2CA02C}
\definecolor{mlred}{HTML}{D62728}
\definecolor{mlorange}{HTML}{FF7F0E}

\newcommand{\bottomnote}[1]{\vfill\footnotesize\textcolor{gray}{#1}}

\title[L03: KNN \& K-Means]{L03: K-Nearest Neighbors \& K-Means}
\subtitle{Classification and Clustering with Distance}
\author{Methods and Algorithms -- MSc Data Science}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item Apply KNN for classification with appropriate K selection
\item Implement K-Means clustering and evaluate cluster quality
\item Compare distance metrics and their effects on results
\item Distinguish between supervised (KNN) and unsupervised (K-Means)
\end{enumerate}
\vspace{1em}
\textbf{Finance Applications:} Customer segmentation, fraud detection
\bottomnote{From parametric models (regression) to instance-based methods}
\end{frame}

\begin{frame}[t]{The Business Problem}
\textbf{Two Distinct Problems}

\textbf{1. Classification (Supervised)}
\begin{itemize}
\item Given labeled examples: is this transaction fraudulent?
\item ``Show me similar past transactions and their outcomes''
\end{itemize}
\vspace{0.5em}
\textbf{2. Clustering (Unsupervised)}
\begin{itemize}
\item No labels: what natural customer segments exist?
\item ``Group customers by behavior for targeted marketing''
\end{itemize}
\bottomnote{KNN = classification with labels, K-Means = clustering without labels}
\end{frame}

\begin{frame}[t]{KNN: Decision Boundaries}
\begin{center}
\includegraphics[width=0.6\textwidth]{01_knn_boundaries/chart.pdf}
\end{center}
\bottomnote{KNN creates non-linear, flexible decision boundaries based on local data}
\end{frame}

\begin{frame}[t]{Distance Metrics}
\begin{center}
\includegraphics[width=0.6\textwidth]{02_distance_metrics/chart.pdf}
\end{center}
\bottomnote{Choice of metric affects which points are considered ``nearest''}
\end{frame}

\begin{frame}[t]{K-Means: The Algorithm}
\begin{center}
\includegraphics[width=0.6\textwidth]{03_kmeans_iteration/chart.pdf}
\end{center}
\bottomnote{Iteratively assign points and update centroids until convergence}
\end{frame}

\begin{frame}[t]{Choosing K: Elbow Method}
\begin{center}
\includegraphics[width=0.6\textwidth]{04_elbow_method/chart.pdf}
\end{center}
\bottomnote{Look for the ``elbow'' where adding clusters gives diminishing returns}
\end{frame}

\begin{frame}[t]{Cluster Quality: Silhouette Analysis}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_silhouette/chart.pdf}
\end{center}
\bottomnote{Silhouette score measures how similar points are to their own cluster}
\end{frame}

\begin{frame}[t]{K-Means Decision Regions}
\begin{center}
\includegraphics[width=0.6\textwidth]{06_voronoi/chart.pdf}
\end{center}
\bottomnote{Each region contains all points closest to one centroid}
\end{frame}

\begin{frame}[t]{Decision Framework}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\bottomnote{KNN for labeled data classification, K-Means for unlabeled clustering}
\end{frame}

\end{document}
