\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L03: KNN \& K-Means Deep Dive]{L03: KNN \& K-Means}
\subtitle{Mathematical Foundations and Implementation}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item Analyze the bias-variance tradeoff in KNN and derive its asymptotic error bounds
\item Evaluate clustering validity using statistical tests (Hopkins, Gap statistic, silhouette)
\item Prove K-Means convergence and analyze computational complexity
\item Compare distance metrics and assess their suitability for high-dimensional finance data
\end{enumerate}
\vspace{0.5em}
\textbf{Finance Applications:} Customer segmentation, fraud detection
\bottomnote{Bloom's Level 4--5: Analyze, Evaluate, Prove, Compare}
\end{frame}

%% PART 1: KNN (Slides 2-12)
\section{K-Nearest Neighbors}

\begin{frame}[t]{KNN: The Lazy Learner}
\textbf{Key Insight}
\begin{itemize}
\item No explicit model training (store all data)
\item Classification by majority vote of K nearest neighbors
\item ``Lazy'' because work is done at prediction time
\end{itemize}
\vspace{0.5em}
\textbf{The Algorithm}
\begin{enumerate}
\item Store all training examples
\item For new query $\mathbf{x}$: find K nearest training points
\item Return majority class among neighbors
\end{enumerate}
\bottomnote{Instance-based learning: the training data IS the model}
\end{frame}

\begin{frame}[t]{Distance Metrics}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{02_distance_metrics/chart.pdf}
\end{center}
\vspace{-0.5em}
\small
\textbf{Common}: Euclidean $\sqrt{\sum(x_i - y_i)^2}$, Manhattan $\sum|x_i - y_i|$
\end{frame}

\begin{frame}[t]{Distance Metrics: Minkowski Family}
\textbf{Minkowski Distance}
$$d_p(\mathbf{x}, \mathbf{y}) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}$$
\vspace{0.3em}
\begin{itemize}
\item $p=1$: Manhattan (L1)
\item $p=2$: Euclidean (L2)
\item $p=\infty$: Chebyshev (max absolute difference)
\item Note: $p < 1$ violates triangle inequality (not a true metric)
\end{itemize}
\vspace{0.5em}
\textbf{Choosing p}
\begin{itemize}
\item $p=2$: Default, works well in most cases
\item $p=1$: More robust to outliers
\item Higher $p$: Sensitive to large single differences
\end{itemize}
\bottomnote{In high dimensions, all distances become similar (curse of dimensionality)}
\end{frame}

\begin{frame}[t]{Beyond Minkowski: Cosine \& Mahalanobis}
\textbf{Cosine Similarity} (critical for text/embeddings)
$$\cos(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}$$
\begin{itemize}
\item Measures angle, not magnitude
\item Range: $[-1, 1]$ where 1 = identical direction
\item Use for: document similarity, word embeddings, high-dimensional sparse data
\end{itemize}
\vspace{0.5em}
\textbf{Mahalanobis Distance} (accounts for correlation)
$$d_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x}-\mathbf{y})^T \Sigma^{-1} (\mathbf{x}-\mathbf{y})}$$
\begin{itemize}
\item Considers feature covariance $\Sigma$
\item Unit-less, scale-invariant
\item Detects outliers accounting for correlation structure
\end{itemize}
\bottomnote{Choose metric based on data type: Euclidean for dense, Cosine for sparse/text}
\end{frame}

\begin{frame}[t]{KNN Decision Boundaries}
\begin{center}
\includegraphics[width=0.55\textwidth]{01_knn_boundaries/chart.pdf}
\end{center}
\textbf{Boundary Properties}
\begin{itemize}
\item Non-linear, locally adaptive
\item Small K: complex boundary, may overfit
\item Large K: smoother boundary, may underfit
\end{itemize}
\end{frame}

\begin{frame}[t]{Choosing K}
\textbf{The Bias-Variance Trade-off}
\begin{itemize}
\item $K=1$: High variance, low bias (very flexible)
\item $K=n$: High bias, low variance (always predicts majority class)
\end{itemize}
\vspace{0.5em}
\textbf{Practical Guidelines}
\begin{itemize}
\item Start with $K = \sqrt{n}$ where $n$ is training size
\item Use odd K for binary classification (avoid ties)
\item Cross-validation to find optimal K
\end{itemize}
\vspace{0.5em}
\textbf{Common Choices}: $K \in \{3, 5, 7, 11\}$
\bottomnote{Small K for complex patterns, larger K for noisy data}
\end{frame}

\begin{frame}[t]{K Selection via Cross-Validation}
\textbf{GridSearchCV for Optimal K}
\begin{itemize}
\item \texttt{from sklearn.model\_selection import GridSearchCV}
\item \texttt{param\_grid = \{'n\_neighbors': range(1, 21, 2)\}}
\item \texttt{grid = GridSearchCV(KNeighborsClassifier(), param\_grid, cv=5)}
\item \texttt{grid.fit(X\_train, y\_train)}
\item \texttt{best\_k = grid.best\_params\_['n\_neighbors']}
\end{itemize}
\vspace{0.3em}
\textbf{Validation Curve}
\begin{itemize}
\item Plot accuracy vs K for training and validation sets
\item Choose K where validation accuracy peaks (before overfitting)
\end{itemize}
\bottomnote{Cross-validation provides objective K selection}
\end{frame}

\begin{frame}[t]{Weighted KNN}
\textbf{Problem with Equal Voting}
\begin{itemize}
\item All K neighbors have equal influence
\item A distant neighbor counts as much as closest neighbor
\end{itemize}
\vspace{0.5em}
\textbf{Solution: Inverse-Distance Weighting}
$$\hat{y} = \frac{\sum_{i \in N_k(\mathbf{x})} w_i y_i}{\sum_{i \in N_k(\mathbf{x})} w_i} \quad \text{where } w_i = \frac{1}{d(\mathbf{x}, \mathbf{x}_i)}$$
\begin{itemize}
\item Closer neighbors get higher weight
\item Reduces sensitivity to K choice
\item For $d=0$ (exact match): return that point's class directly
\end{itemize}
\vspace{0.3em}
\textbf{In scikit-learn}
\begin{itemize}
\item \texttt{weights='uniform'}: equal weights (default)
\item \texttt{weights='distance'}: inverse distance weighting
\end{itemize}
\bottomnote{Distance weighting often improves performance}
\end{frame}

\begin{frame}[t]{Feature Scaling for KNN}
\textbf{Why Scaling Matters}

Without scaling:
\begin{itemize}
\item Income: ranges 20,000--200,000
\item Age: ranges 20--80
\item Distance dominated by income (larger scale)!
\end{itemize}
\vspace{0.5em}
\textbf{Scaling Methods}
\begin{itemize}
\item \textbf{Standardization}: $z = \frac{x - \mu}{\sigma}$ (mean=0, std=1)
\item \textbf{Min-Max}: $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$ (range [0,1])
\end{itemize}
\vspace{0.2em}
\textbf{Edge Cases}
\begin{itemize}
\item If $\sigma = 0$ (constant feature): drop feature or set $z=0$
\item If $x_{max} = x_{min}$ (constant): drop feature or set $x'=0$
\end{itemize}
\vspace{0.3em}
\textbf{Rule}: Always scale features for distance-based methods!
\bottomnote{StandardScaler for Gaussian-like features, MinMaxScaler for bounded}
\end{frame}

\begin{frame}[t]{KNN: Curse of Dimensionality}
\textbf{The Problem}

In high dimensions:
\begin{itemize}
\item All points become approximately equidistant (Beyer et al., 1999)
\item ``Nearest neighbor'' becomes meaningless
\item Exponentially more data needed
\end{itemize}
\vspace{0.5em}
\textbf{Solutions}
\begin{itemize}
\item \textbf{Dimensionality reduction}: PCA before KNN
\item \textbf{Feature selection}: keep only relevant features
\item \textbf{Use domain knowledge}: select meaningful features
\end{itemize}
\bottomnote{KNN works best with moderate number of features (typically $d < 15-20$, problem-dependent)}
\end{frame}

\begin{frame}[t]{KNN: Computational Considerations}
\textbf{Time Complexity}
\begin{itemize}
\item Training: $O(1)$ -- just store data!
\item Prediction: $O(nd)$ for brute force ($n$ samples, $d$ features)
\end{itemize}
\vspace{0.5em}
\textbf{Acceleration Techniques}
\begin{itemize}
\item \textbf{KD-Tree}: $O(d \log n)$ average for $d < 15$ (degrades to $O(n)$ in high dimensions)
\item \textbf{Ball Tree}: Works better in higher dimensions
\item \textbf{Approximate NN}: Trade accuracy for speed
\end{itemize}
\vspace{0.3em}
\textbf{In scikit-learn}
\begin{itemize}
\item \texttt{algorithm='auto'}: automatically chooses best
\item \texttt{algorithm='brute'}: force brute force
\end{itemize}
\bottomnote{For large datasets: consider approximate methods or trees}
\end{frame}

\begin{frame}[t]{KNN: scikit-learn Implementation}
\textbf{Classification}
\begin{itemize}
\item \texttt{from sklearn.neighbors import KNeighborsClassifier}
\item \texttt{knn = KNeighborsClassifier(n\_neighbors=5)}
\item \texttt{knn.fit(X\_train, y\_train)}
\item \texttt{y\_pred = knn.predict(X\_test)}
\end{itemize}
\vspace{0.5em}
\textbf{Key Parameters}
\begin{itemize}
\item \texttt{n\_neighbors}: K value
\item \texttt{weights}: 'uniform' or 'distance'
\item \texttt{metric}: 'euclidean', 'manhattan', etc.
\item \texttt{algorithm}: 'auto', 'ball\_tree', 'kd\_tree', 'brute'
\end{itemize}
\bottomnote{Also available: KNeighborsRegressor for regression tasks}
\end{frame}

\begin{frame}[t]{KNN: Theoretical Foundations}
\textbf{Cover \& Hart (1967) Consistency Theorem}
\begin{itemize}
\item As $n \to \infty$: 1-NN error rate $\leq 2 \times$ Bayes error
\item KNN is \textbf{universally consistent}: converges to optimal
\item Requires: $K \to \infty$ but $K/n \to 0$
\end{itemize}
\vspace{0.5em}
\textbf{VC Dimension}
\begin{itemize}
\item 1-NN has infinite VC dimension in general
\item Shattering depends on geometry, not sample size
\item Implications: prone to overfitting without regularization (large K)
\end{itemize}
\vspace{0.3em}
\textbf{Practical Implication}: KNN's asymptotic optimality requires LARGE datasets
\bottomnote{The 2Ã— Bayes error bound makes KNN a strong baseline}
\end{frame}

\begin{frame}{Cover \& Hart (1967): Proof Sketch}
\textbf{Theorem}: As $n \to \infty$, $K \to \infty$, $K/n \to 0$, the KNN error rate $R^*_{\text{KNN}} \to R^*$ (Bayes rate).

\textbf{Proof idea} (1-NN case, binary classification):

\begin{enumerate}
\item As $n \to \infty$, the nearest neighbor $\mathbf{x}_{(1)} \to \mathbf{x}$ (converges to query point)
\item The 1-NN error at $\mathbf{x}$ is:
\end{enumerate}
\begin{equation}
R_{1\text{-NN}}(\mathbf{x}) = \eta(\mathbf{x})(1 - \eta(\mathbf{x})) + (1 - \eta(\mathbf{x}))\eta(\mathbf{x}) = 2\eta(\mathbf{x})(1 - \eta(\mathbf{x}))
\end{equation}
where $\eta(\mathbf{x}) = P(Y=1|\mathbf{x})$.

Since $R^*(\mathbf{x}) = \min(\eta, 1-\eta)$ and $2\eta(1-\eta) \leq 2R^*(1-R^*) \leq 2R^*$:
\begin{equation}
R^* \leq R_{1\text{-NN}} \leq 2R^*(1 - R^*)
\end{equation}

\bottomnote{The 1-NN error is at most twice the Bayes rate. For $K$-NN with $K/n \to 0$: $R_{K\text{-NN}} \to R^*$ exactly.}
\end{frame}

\begin{frame}[t]{Bias-Variance Decomposition for KNN}
For KNN regression, the expected prediction error at $\mathbf{x}$ decomposes as:

\begin{equation}
E[(\hat{f}_K(\mathbf{x}) - Y)^2] = \underbrace{\text{Bias}^2(\hat{f}_K(\mathbf{x}))}_{\text{increases with } K} + \underbrace{\text{Var}(\hat{f}_K(\mathbf{x}))}_{\text{decreases with } K} + \sigma^2
\end{equation}

For the KNN estimator $\hat{f}_K(\mathbf{x}) = \frac{1}{K}\sum_{i \in N_K(\mathbf{x})} y_i$:

\begin{itemize}
\item \textbf{Variance}: $\text{Var}(\hat{f}_K) = \frac{\sigma^2}{K}$ (averaging $K$ neighbors reduces variance)
\item \textbf{Bias}: $\text{Bias}(\hat{f}_K) \approx f(\mathbf{x}) - \frac{1}{K}\sum_{i \in N_K} f(\mathbf{x}_i)$ (larger $K$ $\Rightarrow$ neighbors farther away $\Rightarrow$ more bias)
\item \textbf{Optimal $K$}: balances this tradeoff; found via cross-validation
\end{itemize}

\bottomnote{At $K=1$: zero bias, variance $= \sigma^2$. At $K=n$: high bias (predicts global mean), variance $= \sigma^2/n$.}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Implement weighted KNN with distance weighting
    \item Exercise 2: Compare Gap statistic vs Elbow for K selection
    \item Exercise 3: Apply SMOTE for imbalanced fraud detection
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} See course materials for Colab notebook
\end{frame}

%% PART 2: K-MEANS (Slides 13-24)
\section{K-Means Clustering}

\begin{frame}[t]{K-Means: The Idea}
\textbf{Goal}: Partition $n$ points into $K$ clusters
\vspace{0.5em}

\textbf{Objective}: Minimize within-cluster sum of squares (WCSS)
$$\sum_{k=1}^{K}\sum_{\mathbf{x} \in C_k}\|\mathbf{x} - \mu_k\|^2$$
where $\mu_k$ is the centroid of cluster $C_k$
\vspace{0.5em}

\textbf{Key Insight}
\begin{itemize}
\item Each point assigned to nearest centroid
\item Centroids are cluster means
\item Iterative refinement until convergence
\end{itemize}
\bottomnote{K-Means finds locally optimal solution (not guaranteed global)}
\end{frame}

\begin{frame}[t]{K-Means Algorithm}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Data $\mathbf{X}$, number of clusters $K$
\STATE Initialize $K$ centroids randomly
\REPEAT
\STATE \textbf{Assignment}: assign each point to nearest centroid
\STATE \textbf{Update}: recompute centroids as cluster means
\STATE \textbf{Handle empty}: if cluster empty, reinitialize from farthest point
\UNTIL{centroid change $< \epsilon$ (e.g., $10^{-4}$) or max iterations}
\STATE \textbf{return} cluster assignments, centroids
\end{algorithmic}
\vspace{0.5em}
\textbf{Convergence}
\begin{itemize}
\item Guaranteed to converge (WCSS decreases each iteration)
\item May converge to local optimum
\end{itemize}
\bottomnote{Each iteration: $O(nKd)$ where $n$ = samples, $K$ = clusters, $d$ = features}
\end{frame}

\begin{frame}[t]{K-Means Convergence Proof}
\textbf{Theorem}: Lloyd's algorithm converges in a finite number of iterations.

\textbf{Proof}: The WCSS objective $J = \sum_{k=1}^{K}\sum_{\mathbf{x}_i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2$ is:

\begin{enumerate}
\item \textbf{Assignment step} (fix $\boldsymbol{\mu}$, optimize $C$): Assigning each point to its nearest centroid minimizes $J$ $\Rightarrow$ $J$ decreases or stays equal
\item \textbf{Update step} (fix $C$, optimize $\boldsymbol{\mu}$): The mean $\boldsymbol{\mu}_k = \frac{1}{|C_k|}\sum_{\mathbf{x}_i \in C_k} \mathbf{x}_i$ minimizes within-cluster sum of squares $\Rightarrow$ $J$ decreases or stays equal
\end{enumerate}

Since $J \geq 0$ and strictly decreasing at each step, and there are finitely many partitions of $n$ points into $K$ clusters: \textbf{convergence is guaranteed}. $\square$

\textbf{NP-hardness}: Finding the globally optimal K-Means solution is NP-hard (Aloise et al., 2009). Lloyd's algorithm finds a local optimum; K-Means++ initialization provides an $O(\log k)$-competitive approximation guarantee.

\bottomnote{This is a coordinate descent argument: alternating optimization of two blocks ($C$ and $\boldsymbol{\mu}$) on a bounded objective.}
\end{frame}

\begin{frame}[t]{K-Means as EM Special Case}
\textbf{Connection to Expectation-Maximization}
\begin{itemize}
\item K-Means = ``Hard EM'' for Gaussian Mixture Models
\item Assumes: spherical Gaussians with equal variance
\item E-step: assign points to nearest centroid (hard assignment)
\item M-step: update centroids as cluster means
\end{itemize}
\vspace{0.5em}
\textbf{Why This Matters}
\begin{itemize}
\item Explains WHY convergence is guaranteed (EM always converges)
\item Explains WHY K-Means assumes spherical clusters
\item Opens door to soft clustering via GMM (next course)
\end{itemize}
\bottomnote{Understanding EM connection deepens theoretical understanding}
\end{frame}

\begin{frame}[t]{K-Means: Visualization}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_kmeans_iteration/chart.pdf}
\end{center}
\bottomnote{Final state after convergence: points colored by cluster, X marks centroids}
\end{frame}

\begin{frame}[t]{Initialization Strategies}
\textbf{Random Initialization}
\begin{itemize}
\item Pick K random points as initial centroids
\item Sensitive to choice, may get poor solution
\item Run multiple times, keep best result
\end{itemize}
\vspace{0.5em}
\textbf{K-Means++ (Default in scikit-learn)}
\begin{itemize}
\item Smart initialization: spread out initial centroids
\item First centroid: random
\item Next centroids: probability proportional to squared distance
\end{itemize}
\vspace{0.3em}
\textbf{Result}: Much better starting point, fewer iterations
\bottomnote{K-Means++ achieves $O(\log k)$-competitive: expected cost $\leq 8(\ln k + 2) \times$ optimal (Arthur \& Vassilvitskii, 2007)}
\end{frame}

\begin{frame}[t]{Choosing K: Elbow Method}
\begin{center}
\includegraphics[width=0.55\textwidth]{04_elbow_method/chart.pdf}
\end{center}
\textbf{Interpretation}: Look for the ``elbow'' where WCSS stops dropping sharply
\end{frame}

\begin{frame}[t]{Choosing K: Silhouette Score}
\textbf{For each point $i$}
\begin{itemize}
\item $a(i)$: average distance to points in same cluster
\item $b(i)$: average distance to points in nearest other cluster
\end{itemize}
\vspace{0.3em}
\textbf{Silhouette score}
$$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$
\begin{itemize}
\item Range: $[-1, 1]$
\item $s \approx 1$: point is well-matched to cluster
\item $s \approx 0$: point is on boundary
\item $s < 0$: point may be in wrong cluster
\item Singleton cluster: $s(i) = 0$ by convention
\end{itemize}
\bottomnote{Average silhouette score summarizes overall clustering quality}
\end{frame}

\begin{frame}[t]{Silhouette Plot}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_silhouette/chart.pdf}
\end{center}
\bottomnote{All clusters should have similar width and scores above average line}
\end{frame}

\begin{frame}[t]{Should We Cluster? Hopkins Statistic}
\textbf{Before Clustering: Test for Cluster Tendency}
$$H = \frac{\sum u_i}{\sum u_i + \sum w_i}$$
\begin{itemize}
\item $u_i$ = distances from random points to nearest data point
\item $w_i$ = distances from data points to nearest neighbor
\item $H \approx 0.5$: uniform distribution (no clusters)
\item $H > 0.75$: significant clustering tendency
\end{itemize}
\vspace{0.3em}
\textbf{Use Case}: Before running K-Means, verify data HAS cluster structure
\bottomnote{Don't cluster uniform data - Hopkins test catches this}
\end{frame}

\begin{frame}[t]{Gap Statistic for K Selection}
\textbf{The Problem with Elbow Method}
\begin{itemize}
\item Elbow is subjective - where exactly IS the elbow?
\item No formal statistical test
\end{itemize}
\vspace{0.5em}
\textbf{Gap Statistic (Tibshirani et al., 2001)}
$$\text{Gap}_n(k) = E_n^*[\log W_k] - \log W_k$$
where $W_k = \sum_{r=1}^k \frac{1}{2|C_r|}\sum_{i,j \in C_r} \|x_i - x_j\|^2$ and $E_n^*$ is computed over reference distributions.

\begin{itemize}
\item Choose smallest K where: $\text{Gap}(k) \geq \text{Gap}(k+1) - s_{k+1}$
\item $s_{k+1}$ = standard error from bootstrap reference samples
\end{itemize}
\bottomnote{Gap statistic provides statistical justification for K selection}
\end{frame}

\begin{frame}[t]{K-Means: Decision Regions}
\begin{center}
\includegraphics[width=0.55\textwidth]{06_voronoi/chart.pdf}
\end{center}
\bottomnote{K-Means creates Voronoi tessellation: regions where all points are closest to one centroid}
\end{frame}

\begin{frame}[t]{K-Means Assumptions}
\textbf{What K-Means Assumes}
\begin{itemize}
\item Clusters are spherical (isotropic)
\item Clusters have similar sizes
\item Clusters have similar densities
\end{itemize}
\vspace{0.5em}
\textbf{When K-Means Fails}
\begin{itemize}
\item Non-convex shapes (e.g., crescents)
\item Very different cluster sizes
\item Different densities
\item Outliers (pull centroids away)
\end{itemize}
\bottomnote{Consider DBSCAN or Gaussian Mixture Models for these cases}
\end{frame}

\begin{frame}[t]{K-Means Variants}
\textbf{Mini-Batch K-Means}
\begin{itemize}
\item Uses random subsets for updates
\item Much faster for large datasets
\item Slightly worse results
\end{itemize}
\vspace{0.5em}
\textbf{K-Medoids}
\begin{itemize}
\item Centroids must be actual data points
\item More robust to outliers
\item Slower than K-Means
\end{itemize}
\vspace{0.5em}
\textbf{K-Means for Mixed Data}
\begin{itemize}
\item K-Modes: for categorical data
\item K-Prototypes: mixed continuous and categorical
\end{itemize}
\bottomnote{Mini-Batch K-Means: good for >10k samples}
\end{frame}

\begin{frame}[t]{K-Means: scikit-learn}
\textbf{Basic Usage}
\begin{itemize}
\item \texttt{from sklearn.cluster import KMeans}
\item \texttt{kmeans = KMeans(n\_clusters=3, random\_state=42)}
\item \texttt{labels = kmeans.fit\_predict(X)}
\item \texttt{centroids = kmeans.cluster\_centers\_}
\end{itemize}
\vspace{0.5em}
\textbf{Key Parameters}
\begin{itemize}
\item \texttt{n\_clusters}: K (required)
\item \texttt{init}: 'k-means++' (default) or 'random'
\item \texttt{n\_init}: number of runs (default 10)
\item \texttt{max\_iter}: max iterations per run
\end{itemize}
\bottomnote{inertia\_ attribute gives WCSS after fitting}
\end{frame}

%% PART 3: COMPARISON AND APPLICATIONS (Slides 25-32)
\section{Comparison and Applications}

\begin{frame}[t]{KNN vs K-Means: Key Differences}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{KNN} & \textbf{K-Means} \\
\midrule
Task & Classification/Regression & Clustering \\
Learning & Supervised (needs labels) & Unsupervised \\
K meaning & Number of neighbors & Number of clusters \\
Training & None (lazy) & Iterative optimization \\
Prediction & Compute distances & Assign to centroid \\
Output & Class label & Cluster ID \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{The ``K'' in KNN and K-Means mean completely different things!}
\end{frame}

\begin{frame}[t]{Finance Application: Customer Segmentation}
\textbf{RFM Analysis} (Industry Standard)
\begin{itemize}
\item \textbf{R}ecency: Days since last transaction
\item \textbf{F}requency: Number of transactions in period
\item \textbf{M}onetary: Total/average transaction value
\end{itemize}
\vspace{0.5em}
\textbf{K-Means on RFM Features}
\begin{itemize}
\item Standardize each RFM dimension (different scales!)
\item Cluster into segments (typically K=4-6)
\item Profile: ``Champions'' (high R,F,M) vs ``At Risk'' (low R)
\end{itemize}
\vspace{0.3em}
\textbf{Business Value}
\begin{itemize}
\item Customer Lifetime Value (CLV) prediction per segment
\item Targeted retention campaigns by segment risk
\end{itemize}
\bottomnote{RFM segmentation is foundational for CRM and marketing analytics}
\end{frame}

\begin{frame}[t]{Finance Application: Fraud Detection}
\textbf{CRITICAL: Class Imbalance Problem}
\begin{itemize}
\item Fraud is typically $<$1\% of transactions (100:1 ratio)
\item Naive KNN majority vote: ALWAYS predicts non-fraud
\item 99\% accuracy but detects ZERO fraud!
\end{itemize}
\vspace{0.5em}
\textbf{Solutions for Imbalanced Data}
\begin{itemize}
\item \textbf{SMOTE}: Synthetic Minority Oversampling TEchnique
\item \textbf{Weighted KNN}: Higher weight for minority class neighbors
\item \textbf{Cost-sensitive}: False Negative costs $>>$ False Positive
\item \textbf{Anomaly score}: Use distance to K-th neighbor, not vote
\item Use \textbf{Precision-Recall AUC}, NOT accuracy!
\end{itemize}
\bottomnote{Always check class distribution before applying majority vote!}
\end{frame}

\begin{frame}[t]{When to Use What}
\textbf{Use KNN When}
\begin{itemize}
\item You have labeled training data
\item Local patterns matter (non-linear boundaries)
\item Interpretability: ``similar to these examples''
\item Moderate dataset size
\end{itemize}
\vspace{0.5em}
\textbf{Use K-Means When}
\begin{itemize}
\item No labels available
\item Looking for natural groupings
\item Clusters are roughly spherical
\item Need fast clustering of large data
\end{itemize}
\bottomnote{K-Means often used as preprocessing before supervised learning}
\end{frame}

\begin{frame}[t]{Decision Framework}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\bottomnote{Start with KNN for classification, K-Means for clustering}
\end{frame}

\begin{frame}[t]{Alternative Clustering: DBSCAN and Hierarchical}
\textbf{DBSCAN} (Density-Based Spatial Clustering):
\begin{itemize}
\item \textbf{Core point}: has $\geq$ minPts neighbors within radius $\varepsilon$
\item \textbf{Border point}: within $\varepsilon$ of a core point but not itself core
\item \textbf{Noise}: neither core nor border $\Rightarrow$ automatically detected as outliers
\item Advantage: discovers non-spherical clusters; does not require $K$
\end{itemize}

\textbf{Hierarchical (Agglomerative)} Clustering:
\begin{itemize}
\item Bottom-up: each point starts as its own cluster, iteratively merge closest pairs
\item Linkage criteria: single (min), complete (max), Ward (minimize variance)
\item Produces a \textbf{dendrogram}: cut at desired height to get $K$ clusters
\end{itemize}

\bottomnote{Use DBSCAN when clusters have irregular shapes. Use hierarchical when you want to explore multiple $K$ values via the dendrogram.}
\end{frame}

\begin{frame}[t]{Alternatives to Consider}
\textbf{Instead of KNN}
\begin{itemize}
\item Large data: use ball tree or approximate NN
\item Need probability: logistic regression
\item Many features: random forest
\end{itemize}
\vspace{0.5em}
\textbf{Instead of K-Means}
\begin{itemize}
\item Soft assignments: Gaussian Mixture Models
\item Non-spherical: spectral clustering
\item Streaming data: Mini-Batch K-Means
\end{itemize}
\bottomnote{Choose algorithm based on data characteristics and problem requirements}
\end{frame}

%% SUMMARY
\section{Summary}

\begin{frame}[t]{Key Takeaways}
\textbf{K-Nearest Neighbors}
\begin{itemize}
\item Instance-based, lazy learner
\item Scale features, choose K via cross-validation
\item Works best with moderate features, moderate data size
\end{itemize}
\vspace{0.5em}
\textbf{K-Means}
\begin{itemize}
\item Iterative: assign points, update centroids
\item K-Means++ for initialization, elbow/silhouette for K
\item Assumes spherical, similar-size clusters
\end{itemize}
\vspace{0.5em}
\textbf{Common Considerations}
\begin{itemize}
\item Feature scaling is critical for both
\item ``K'' means different things in each algorithm
\end{itemize}
\bottomnote{Both are foundational algorithms: simple, interpretable, widely used}
\end{frame}

\begin{frame}[t]{References}
\textbf{Textbooks}
\begin{itemize}
\item James et al. (2021). \textit{ISLR}, Chapters 2, 12
\item Hastie et al. (2009). \textit{ESL}, Chapters 13, 14
\end{itemize}
\vspace{0.5em}
\textbf{Key Papers}
\begin{itemize}
\item Arthur \& Vassilvitskii (2007). K-Means++
\item Cover \& Hart (1967). Nearest Neighbor Pattern Classification
\end{itemize}
\vspace{0.5em}
\textbf{Next Lecture}
\begin{itemize}
\item L04: Random Forests
\item Ensemble methods and feature importance
\end{itemize}
\end{frame}

\end{document}
