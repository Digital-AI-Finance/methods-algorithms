\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Compact list for dense slides
\newenvironment{compactlist}{%
  \begin{itemize}%
    \setlength{\itemsep}{2pt}%
    \setlength{\parskip}{0pt}%
    \setlength{\parsep}{0pt}%
}{%
  \end{itemize}%
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L03: KNN \& K-Means Deep Dive]{L03: KNN \& K-Means}
\subtitle{Deep Dive: Mathematical Foundations and Implementation}
\author{Methods and Algorithms}
\institute{MSc Data Science}
\date{Spring 2026}

\begin{document}

% ============================================================
% SLIDE 1: Title Page
% ============================================================
\begin{frame}
\titlepage
\end{frame}

% ============================================================
% SLIDE 2: Outline
% ============================================================
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% ============================================================
% SLIDE 3: Opening Comic -- XKCD #1838
% ============================================================
\begin{frame}[t]{The Math Behind ``Similar Things Behave Similarly''}
\begin{columns}[T]
\column{0.55\textwidth}
\small
\textbf{Today's Deep Dive}

\vspace{0.3em}
In the overview, we saw that KNN classifies by neighbors and K-Means discovers clusters. Now we go deeper:

\begin{compactlist}
  \item How do we \textbf{formalize} distance and prove KNN's theoretical guarantees?
  \item How do we \textbf{prove} K-Means convergence and understand its connection to EM?
  \item How do we \textbf{validate} clusters statistically before trusting them?
\end{compactlist}

\column{0.42\textwidth}
\begin{center}
\includegraphics[width=0.85\textwidth]{images/1838_machine_learning.png}
\end{center}
\end{columns}

\bottomnote{XKCD \#1838 by Randall Munroe (CC BY-NC 2.5) -- The math behind the ``pile of linear algebra''}
\end{frame}

% ============================================================
% SLIDE 4: Learning Objectives
% ============================================================
\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item \textbf{Analyze} the bias-variance tradeoff in KNN and derive its asymptotic error bounds
\item \textbf{Evaluate} clustering validity using statistical tests (Hopkins, Gap statistic, silhouette)
\item \textbf{Prove} K-Means convergence and analyze computational complexity
\item \textbf{Compare} distance metrics and assess their suitability for high-dimensional finance data
\end{enumerate}
\vspace{0.5em}
\textbf{Finance Applications:} Customer segmentation (RFM), fraud detection
\bottomnote{Bloom's Level 4--5: Analyze, Evaluate, Prove, Compare}
\end{frame}

% ============================================================
% SECTION 1: KNN FOUNDATIONS (Slides 5-12)
% ============================================================
\section{KNN Foundations}

% ============================================================
% SLIDE 5: KNN: The Lazy Learner
% ============================================================
\begin{frame}[t]{Why Is KNN Called a Lazy Learner?}
\textbf{Key Insight}
\begin{compactlist}
\item No explicit model training -- store all training data
\item Classification by majority vote of $K$ nearest neighbors
\item ``Lazy'' because all computation happens at prediction time
\end{compactlist}
\vspace{0.5em}
\textbf{The Algorithm in Words}
\begin{enumerate}
\item Store all training examples $(\mathbf{x}_i, y_i)$
\item For new query $\mathbf{x}$: find $K$ nearest training points
\item Return majority class among those $K$ neighbors
\end{enumerate}
\bottomnote{Instance-based learning: the training data IS the model -- no parameters to estimate}
\end{frame}

% ============================================================
% SLIDE 6: KNN Algorithm (Pseudocode)
% ============================================================
\begin{frame}[t]{How Does the KNN Algorithm Work?}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Training set $\mathcal{D}$, query point $\mathbf{x}$, number of neighbors $K$
\STATE Compute $d(\mathbf{x}, \mathbf{x}_i)$ for all $\mathbf{x}_i \in \mathcal{D}$
\STATE Select $K$ points with smallest distances: $N_K(\mathbf{x})$
\STATE Count votes: $v_c = |\{i \in N_K(\mathbf{x}) : y_i = c\}|$ for each class $c$
\IF{unique maximum class exists}
\STATE $\hat{y} = \arg\max_c v_c$
\ELSE
\STATE \textbf{Tie-breaking}: $\hat{y} = \arg\max_c \sum_{i: y_i = c} \frac{1}{d(\mathbf{x}, \mathbf{x}_i)}$
\ENDIF
\STATE \textbf{return} $\hat{y}$
\end{algorithmic}
\vspace{0.5em}
\textbf{Complexity}: $O(nd)$ per query (brute force over $n$ samples, $d$ features)
\bottomnote{Tie-breaking via distance-weighted vote prevents non-determinism on decision boundaries}
\end{frame}

% SLIDE: KNN Boundaries Visualization
\begin{frame}[t]{How Do KNN Boundaries Change with K?}
\begin{center}
\includegraphics[width=0.55\textwidth]{01_knn_boundaries/chart.pdf}
\end{center}

\begin{compactlist}
  \item Small K creates jagged, complex boundaries that follow noise
  \item Large K creates smooth boundaries that may miss local patterns
  \item The optimal K balances flexibility with stability
\end{compactlist}

\bottomnote{Decision boundaries are non-parametric -- they adapt to the local data structure}
\end{frame}

% ============================================================
% SLIDE 7: Distance Metrics Visualization
% ============================================================
\begin{frame}[t]{What Do Different Distance Metrics Look Like?}
\begin{center}
\includegraphics[width=0.55\textwidth]{02_distance_metrics/chart.pdf}
\end{center}
\vspace{-0.3em}
\textbf{Key Insight}: Different metrics define different ``neighborhoods'' -- the choice of metric shapes the decision boundary and determines which points count as nearest neighbors.
\bottomnote{Euclidean (circular), Manhattan (diamond), Chebyshev (square) unit balls in 2D}
\end{frame}

% ============================================================
% SLIDE 8: Distance Metrics: Minkowski Family
% ============================================================
\begin{frame}[t]{How Do We Generalize Distance?}
\textbf{Minkowski Distance}
$$d_p(\mathbf{x}, \mathbf{y}) = \left(\sum_{i=1}^{d}|x_i - y_i|^p\right)^{1/p}$$
\vspace{0.3em}
\begin{itemize}
\item $p=1$: Manhattan (L1) -- robust to outliers, sparse feature differences
\item $p=2$: Euclidean (L2) -- default, geometrically intuitive
\item $p=\infty$: Chebyshev -- maximum absolute difference along any axis
\end{itemize}
\vspace{0.5em}
\textbf{Triangle Inequality}: For $p \geq 1$, $d_p(\mathbf{x}, \mathbf{z}) \leq d_p(\mathbf{x}, \mathbf{y}) + d_p(\mathbf{y}, \mathbf{z})$
\begin{itemize}
\item Enables pruning in KD-Trees and Ball Trees
\item For $p < 1$: violates triangle inequality (not a true metric)
\end{itemize}
\bottomnote{Higher $p$ amplifies large single-feature differences; $p=2$ is the default for most applications}
\end{frame}

% ============================================================
% SLIDE 9: Beyond Minkowski: Cosine and Mahalanobis
% ============================================================
\begin{frame}[t]{When Do We Need Non-Euclidean Distance?}
\textbf{Cosine Similarity} (for text and embeddings)
$$\cos(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}, \quad \text{range: } [-1, 1]$$
\begin{itemize}
\item Measures angle, not magnitude -- ideal for sparse, high-dimensional data
\item Use for: document similarity, word embeddings, TF-IDF vectors
\end{itemize}
\vspace{0.5em}
\textbf{Mahalanobis Distance} (accounts for correlation)
$$d_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x}-\mathbf{y})^T \Sigma^{-1} (\mathbf{x}-\mathbf{y})}$$
\begin{itemize}
\item Uses feature covariance $\Sigma$ -- unit-less, scale-invariant
\item Detects outliers accounting for correlation structure
\end{itemize}
\bottomnote{Choose metric based on data type: Euclidean for dense, Cosine for sparse/text, Mahalanobis for correlated features}
\end{frame}

% ============================================================
% SLIDE 10: Choosing K: Bias-Variance Tradeoff
% ============================================================
\begin{frame}[t]{How Does K Control the Bias-Variance Tradeoff?}
\textbf{The Fundamental Tradeoff}
\begin{itemize}
\item $K=1$: High variance, low bias -- very flexible, memorizes noise
\item $K=n$: High bias, low variance -- always predicts majority class
\end{itemize}
\vspace{0.5em}
\textbf{Practical Guidelines}
\begin{itemize}
\item Start with $K = \sqrt{n}$ where $n$ is training size
\item Use odd $K$ for binary classification (avoids ties)
\item Always validate with cross-validation
\end{itemize}
\vspace{0.3em}
\textbf{Common Choices}: $K \in \{3, 5, 7, 11\}$
\bottomnote{Small K for complex patterns with clean data; larger K for noisy data requiring smoothing}
\end{frame}

% SLIDE: K-Means Iteration Visualization
\begin{frame}[t]{How Do Centroids Move During Iteration?}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_kmeans_iteration/chart.pdf}
\end{center}

\begin{compactlist}
  \item Each iteration: assign points to nearest centroid, then recompute centroids
  \item Centroids migrate toward cluster centers until stable
  \item Convergence typically occurs within 10--50 iterations
\end{compactlist}

\bottomnote{Visualizing iterations helps verify that K-Means is finding meaningful clusters}
\end{frame}

% ============================================================
% SLIDE 11: Weighted KNN
% ============================================================
\begin{frame}[t]{Why Weight Neighbors by Distance?}
\textbf{Problem}: All $K$ neighbors have equal influence -- a distant neighbor counts as much as the closest one.
\vspace{0.5em}

\textbf{Inverse-Distance Weighting}
$$\hat{y} = \arg\max_c \sum_{i \in N_K(\mathbf{x})} w_i \cdot \mathbf{1}[y_i = c] \quad \text{where } w_i = \frac{1}{d(\mathbf{x}, \mathbf{x}_i)}$$
\begin{itemize}
\item Closer neighbors receive exponentially higher weight
\item Reduces sensitivity to the choice of $K$
\item For $d=0$ (exact match): return that point's class directly
\end{itemize}
\vspace{0.3em}
\textbf{In scikit-learn}: \texttt{weights='uniform'} (default) vs \texttt{weights='distance'}
\bottomnote{Distance weighting often improves performance, especially when K is not perfectly tuned}
\end{frame}

% ============================================================
% SLIDE 12: K Selection via Cross-Validation
% ============================================================
\begin{frame}[t]{How Do We Select K Objectively?}
\textbf{GridSearchCV for Optimal K}
\begin{itemize}
\item \texttt{param\_grid = \{'n\_neighbors': range(1, 21, 2)\}}
\item \texttt{grid = GridSearchCV(KNeighborsClassifier(), param\_grid, cv=5)}
\item \texttt{grid.fit(X\_train, y\_train)}
\item \texttt{best\_k = grid.best\_params\_['n\_neighbors']}
\end{itemize}
\vspace{0.5em}
\textbf{Validation Curve Interpretation}
\begin{itemize}
\item Plot accuracy vs K for both training and validation sets
\item Choose K where validation accuracy peaks (before overfitting gap widens)
\item Large gap between train/val curves signals overfitting
\end{itemize}
\bottomnote{Cross-validation provides objective, data-driven K selection -- never choose K by ``eyeballing''}
\end{frame}

% ============================================================
% SECTION 2: KNN THEORY (Slides 13-17)
% ============================================================
\section{KNN Theory}

% ============================================================
% SLIDE 13: Feature Scaling for KNN
% ============================================================
\begin{frame}[t]{Why Must We Scale Features for KNN?}
\textbf{Why Scaling Matters} -- Without scaling:
\begin{itemize}
\item Income: ranges 20,000--200,000
\item Age: ranges 20--80
\item Distance dominated by income (180,000$\times$ larger scale!)
\end{itemize}
\vspace{0.5em}
\textbf{Scaling Methods}
\begin{compactlist}
\item \textbf{Standardization}: $z = \frac{x - \mu}{\sigma}$ \quad (mean=0, std=1)
\item \textbf{Min-Max}: $x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$ \quad (range [0,1])
\end{compactlist}
\vspace{0.3em}
\textbf{Edge Cases}
\begin{itemize}
\item If $\sigma = 0$ or $x_{\max} = x_{\min}$ (constant feature): drop feature or set to 0
\end{itemize}
\bottomnote{Rule: ALWAYS scale features for distance-based methods. StandardScaler for Gaussian-like, MinMaxScaler for bounded.}
\end{frame}

% ============================================================
% SLIDE 14: Curse of Dimensionality
% ============================================================
\begin{frame}[t]{Why Does KNN Struggle in High Dimensions?}
\textbf{The Problem} (Beyer et al., 1999)

In high dimensions, for any query point $\mathbf{x}$:
$$\lim_{d \to \infty} \frac{d_{\max} - d_{\min}}{d_{\min}} \to 0$$
\begin{itemize}
\item All points become approximately equidistant
\item ``Nearest neighbor'' becomes meaningless
\item Volume of unit hypersphere $\to 0$ as $d \to \infty$
\end{itemize}
\vspace{0.5em}
\textbf{Solutions}
\begin{itemize}
\item \textbf{Dimensionality reduction}: PCA before KNN (see L05)
\item \textbf{Feature selection}: keep only relevant features
\item \textbf{Domain knowledge}: select meaningful features from subject matter
\end{itemize}
\bottomnote{KNN works best with moderate dimensionality ($d < 15$--$20$, problem-dependent)}
\end{frame}

% ============================================================
% SLIDE 15: Cover & Hart (1967) Consistency Theorem
% ============================================================
\begin{frame}[t]{How Good Can KNN Theoretically Be?}
\textbf{Theorem}: As $n \to \infty$, the 1-NN error rate satisfies:
$$R^* \leq R_{1\text{-NN}} \leq 2R^*(1 - R^*)$$
where $R^*$ is the Bayes optimal error rate.
\vspace{0.5em}

\textbf{Key Results}
\begin{itemize}
\item 1-NN error is at most $2\times$ Bayes error -- remarkably strong for such a simple method
\item KNN is \textbf{universally consistent}: $R_{K\text{-NN}} \to R^*$ exactly
\item Requires: $K \to \infty$ and $K/n \to 0$ simultaneously
\end{itemize}
\vspace{0.3em}
\textbf{Practical Implication}: KNN is a strong baseline -- if it performs badly, the features may be poor rather than the algorithm.
\bottomnote{Universal consistency holds under mild conditions; see appendix for full proof}
\end{frame}

% ============================================================
% SLIDE 16: Cover & Hart: Proof Sketch
% ============================================================
\begin{frame}[t]{How Do We Prove the 1-NN Error Bound?}
\textbf{Proof idea} (1-NN case, binary classification):

\begin{enumerate}
\item As $n \to \infty$, the nearest neighbor $\mathbf{x}_{(1)} \to \mathbf{x}$ (converges to query)
\item Let $\eta(\mathbf{x}) = P(Y=1|\mathbf{x})$. The 1-NN error at $\mathbf{x}$:
\end{enumerate}
$$R_{1\text{-NN}}(\mathbf{x}) = \eta(\mathbf{x})(1-\eta(\mathbf{x})) + (1-\eta(\mathbf{x}))\eta(\mathbf{x}) = 2\eta(\mathbf{x})(1-\eta(\mathbf{x}))$$
\vspace{0.3em}
Since $R^*(\mathbf{x}) = \min(\eta, 1-\eta)$ and $2\eta(1-\eta) \leq 2\min(\eta,1-\eta)$:
$$R^* \leq R_{1\text{-NN}} \leq 2R^*(1 - R^*)$$
\vspace{0.3em}
For $K$-NN with $K \to \infty$, $K/n \to 0$: majority vote over $K$ neighbors converges to $\mathbf{1}[\eta(\mathbf{x}) > 0.5]$, so $R_{K\text{-NN}} \to R^*$ exactly.
\bottomnote{The 2$\times$ bound is tight: equality when $\eta(\mathbf{x}) = 0.5$ everywhere (maximum ambiguity)}
\end{frame}

% ============================================================
% SLIDE 17: Bias-Variance Decomposition
% ============================================================
\begin{frame}[t]{How Does K Affect Bias and Variance?}
For KNN regression, $\hat{f}_K(\mathbf{x}) = \frac{1}{K}\sum_{i \in N_K(\mathbf{x})} y_i$, the expected error decomposes as:

$$E[(\hat{f}_K(\mathbf{x}) - Y)^2] = \underbrace{\text{Bias}^2(\hat{f}_K(\mathbf{x}))}_{\text{increases with } K} + \underbrace{\text{Var}(\hat{f}_K(\mathbf{x}))}_{\text{decreases with } K} + \sigma^2$$

\begin{itemize}
\item \textbf{Variance}: $\text{Var}(\hat{f}_K) = \frac{\sigma^2}{K}$ \quad (averaging $K$ neighbors reduces variance)
\item \textbf{Bias}: $\text{Bias}(\hat{f}_K) \approx f(\mathbf{x}) - \frac{1}{K}\sum_{i \in N_K} f(\mathbf{x}_i)$ \quad (larger $K$ = farther neighbors = more bias)
\item \textbf{Optimal $K$}: balances this tradeoff; found via cross-validation
\end{itemize}
\bottomnote{$K=1$: zero bias, variance $= \sigma^2$. \quad $K=n$: high bias (global mean), variance $= \sigma^2/n$.}
\end{frame}

% ============================================================
% SECTION 3: K-MEANS ALGORITHM (Slides 18-24)
% ============================================================
\section{K-Means Algorithm}

% ============================================================
% SLIDE 18: K-Means: The Idea
% ============================================================
\begin{frame}[t]{What Is K-Means Trying to Optimize?}
\textbf{Goal}: Partition $n$ points into $K$ clusters by minimizing within-cluster sum of squares (WCSS):
$$J = \sum_{k=1}^{K}\sum_{\mathbf{x}_i \in C_k}\|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2$$
where $\boldsymbol{\mu}_k = \frac{1}{|C_k|}\sum_{\mathbf{x}_i \in C_k} \mathbf{x}_i$ is the centroid of cluster $C_k$.
\vspace{0.5em}

\textbf{Key Properties}
\begin{itemize}
\item Each point assigned to its nearest centroid
\item Iterative refinement: assign $\to$ update $\to$ repeat
\item Converges to local (not global) optimum
\end{itemize}
\bottomnote{Finding the globally optimal K-Means solution is NP-hard (Aloise et al., 2009)}
\end{frame}

% ============================================================
% SLIDE 19: K-Means Algorithm (Pseudocode)
% ============================================================
\begin{frame}[t]{How Does Lloyd's Algorithm Work?}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Data $\mathbf{X}$, number of clusters $K$
\STATE Initialize $K$ centroids (K-Means++ or random)
\REPEAT
\STATE \textbf{Assignment}: $C_k = \{\mathbf{x}_i : \|\mathbf{x}_i - \boldsymbol{\mu}_k\| \leq \|\mathbf{x}_i - \boldsymbol{\mu}_j\| \; \forall j\}$
\STATE \textbf{Update}: $\boldsymbol{\mu}_k = \frac{1}{|C_k|}\sum_{\mathbf{x}_i \in C_k} \mathbf{x}_i$
\STATE \textbf{Handle empty}: if $|C_k| = 0$, reinitialize $\boldsymbol{\mu}_k$ from farthest point
\UNTIL{centroid change $< \epsilon$ or max iterations reached}
\STATE \textbf{return} cluster assignments, centroids
\end{algorithmic}
\vspace{0.5em}
\textbf{Convergence}: Guaranteed -- WCSS decreases monotonically each iteration. May converge to local optimum; run multiple restarts.
\bottomnote{Each iteration costs $O(nKd)$ where $n$ = samples, $K$ = clusters, $d$ = features}
\end{frame}

% ============================================================
% SLIDE 20: K-Means++: Smart Initialization
% ============================================================
\begin{frame}[t]{Why Does Smart Initialization Matter?}
\textbf{Problem}: Random initialization is sensitive to starting positions and often converges to poor local optima.
\vspace{0.5em}

\textbf{K-Means++ Algorithm} (Arthur \& Vassilvitskii, 2007)
\begin{enumerate}
\item Choose first centroid uniformly at random from data
\item For each subsequent centroid: select point $\mathbf{x}$ with probability $\propto d(\mathbf{x})^2$ (squared distance to nearest existing centroid)
\item Repeat until $K$ centroids chosen
\end{enumerate}
\vspace{0.5em}
\textbf{Guarantee}: Expected cost $\leq 8(\ln K + 2) \times$ optimal cost -- an $O(\log K)$-competitive approximation.
\bottomnote{K-Means++ is the default in scikit-learn (\texttt{init='k-means++'}); typically converges in fewer iterations}
\end{frame}

% ============================================================
% SLIDE 21: Silhouette Score
% ============================================================
\begin{frame}[t]{How Do We Measure Cluster Quality?}
\textbf{For each point $i$}:
\begin{itemize}
\item $a(i)$: average distance to all other points in \textbf{same} cluster (cohesion)
\item $b(i)$: average distance to points in \textbf{nearest other} cluster (separation)
\end{itemize}
\vspace{0.3em}
\textbf{Silhouette coefficient}:
$$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}, \quad s(i) \in [-1, 1]$$
\vspace{0.3em}
\textbf{Interpretation}:
\begin{itemize}
\item $s \approx 1$: point is well-clustered (far from neighboring clusters)
\item $s \approx 0$: point lies on the boundary between clusters
\item $s < 0$: point is likely assigned to the wrong cluster
\end{itemize}
\bottomnote{Average silhouette score across all points summarizes overall clustering quality. Singleton: $s(i) = 0$ by convention.}
\end{frame}

% ============================================================
% SLIDE 22: Silhouette Plot
% ============================================================
\begin{frame}[t]{What Does a Silhouette Plot Reveal?}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_silhouette/chart.pdf}
\end{center}
\textbf{Reading the Plot}: Each cluster should have similar width (balanced sizes) and scores above the average line. Thin or negative clusters indicate poor assignments.
\bottomnote{Compare silhouette plots across different K values to find the optimal number of clusters}
\end{frame}

% ============================================================
% SLIDE 23: K-Means Decision Regions
% ============================================================
\begin{frame}[t]{What Shape Are K-Means Clusters?}
\begin{center}
\includegraphics[width=0.55\textwidth]{06_voronoi/chart.pdf}
\end{center}
\textbf{Voronoi Tessellation}: K-Means partitions the feature space into convex polygonal regions. Every point within a region is closest to that region's centroid.
\bottomnote{Voronoi cells are always convex -- this is why K-Means cannot discover non-convex cluster shapes}
\end{frame}

% ============================================================
% SLIDE 24: K-Means Assumptions
% ============================================================
\begin{frame}[t]{When Does K-Means Fail?}
\textbf{What K-Means Assumes}
\begin{itemize}
\item Clusters are \textbf{spherical} (isotropic variance in all directions)
\item Clusters have \textbf{similar sizes} (roughly equal number of points)
\item Clusters have \textbf{similar densities}
\end{itemize}
\vspace{0.5em}
\textbf{When K-Means Fails}
\begin{itemize}
\item Non-convex shapes (e.g., crescents, rings) -- use DBSCAN
\item Very different cluster sizes or densities -- use GMM
\item Strong outliers -- use K-Medoids (centroids must be data points)
\end{itemize}
\bottomnote{Always visualize clusters in 2D (via PCA/t-SNE) to check if assumptions hold}
\end{frame}

% ============================================================
% SECTION 4: K-MEANS THEORY (Slides 25-28)
% ============================================================
\section{K-Means Theory}

% ============================================================
% SLIDE 25: K-Means Convergence Proof
% ============================================================
\begin{frame}[t]{Why Must K-Means Converge?}
\textbf{Theorem}: Lloyd's algorithm converges in a finite number of iterations.
\vspace{0.3em}

\textbf{Proof} (coordinate descent argument):

The WCSS objective $J = \sum_{k=1}^{K}\sum_{\mathbf{x}_i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2$ satisfies:
\begin{enumerate}
\item \textbf{Assignment step} (fix $\boldsymbol{\mu}$, optimize $C$): Reassigning each point to its nearest centroid can only decrease $J$ \quad $\Rightarrow$ $J$ non-increasing
\item \textbf{Update step} (fix $C$, optimize $\boldsymbol{\mu}$): The mean minimizes within-cluster SSE \quad $\Rightarrow$ $J$ non-increasing
\end{enumerate}
\vspace{0.3em}
Since $J \geq 0$ and non-increasing, and there are finitely many partitions of $n$ points into $K$ clusters: the algorithm must terminate. \hfill $\square$
\vspace{0.3em}

\textbf{NP-hardness}: Global optimum is NP-hard; K-Means++ provides $O(\log K)$ approximation.
\bottomnote{Block coordinate descent: alternating optimization of two blocks ($C$ and $\boldsymbol{\mu}$) on a bounded objective}
\end{frame}

% ============================================================
% SLIDE 26: K-Means as EM Special Case
% ============================================================
\begin{frame}[t]{How Does K-Means Relate to EM?}
\textbf{Connection to Expectation-Maximization}
\begin{itemize}
\item K-Means = ``Hard EM'' for Gaussian Mixture Models (GMM)
\item Assumes: spherical Gaussians with equal variance $\sigma^2 I$
\item \textbf{E-step}: assign points to nearest centroid (hard assignment, $\gamma_{ik} \in \{0,1\}$)
\item \textbf{M-step}: update centroids as cluster means
\end{itemize}
\vspace{0.5em}
\textbf{Why This Matters}
\begin{itemize}
\item Explains WHY convergence is guaranteed (EM always converges)
\item Explains WHY K-Means assumes spherical clusters
\item Opens door to \textbf{soft clustering} via full GMM (probabilistic assignments)
\end{itemize}
\bottomnote{Soft EM: $\gamma_{ik} \in [0,1]$ gives probability of membership -- more flexible but computationally heavier}
\end{frame}

% ============================================================
% SLIDE 27: K-Means Variants
% ============================================================
\begin{frame}[t]{What Alternatives Exist to Standard K-Means?}
\textbf{Mini-Batch K-Means}
\begin{itemize}
\item Uses random subsets (mini-batches) for centroid updates
\item $10$--$100\times$ faster for large datasets; slightly worse results
\end{itemize}
\vspace{0.3em}
\textbf{K-Medoids (PAM)}
\begin{itemize}
\item Centroids must be actual data points (medoids)
\item More robust to outliers; works with any distance metric
\end{itemize}
\vspace{0.3em}
\textbf{K-Modes / K-Prototypes}
\begin{itemize}
\item K-Modes: for categorical data (uses mode instead of mean)
\item K-Prototypes: mixed continuous and categorical features
\end{itemize}
\bottomnote{Mini-Batch K-Means is recommended for $n > 10{,}000$ samples; K-Medoids for non-Euclidean distances}
\end{frame}

% ============================================================
% SLIDE 28: Computational Considerations
% ============================================================
\begin{frame}[t]{How Fast Are These Algorithms?}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Training} & \textbf{Prediction} \\
\midrule
KNN (brute force) & $O(1)$ & $O(nd)$ \\
KNN (KD-Tree) & $O(nd \log n)$ & $O(d \log n)$ \\
KNN (Ball Tree) & $O(nd \log n)$ & $O(d \log n)$ \\
K-Means (Lloyd's) & $O(nKdT)$ & $O(Kd)$ \\
Mini-Batch K-Means & $O(bKdT)$ & $O(Kd)$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{0.3em}
$n$ = samples, $d$ = features, $K$ = neighbors/clusters, $T$ = iterations, $b$ = batch size.
\begin{itemize}
\item KD-Tree degrades to $O(nd)$ when $d > 15$--$20$
\item For very large $n$: approximate nearest neighbors (LSH, FAISS)
\end{itemize}
\bottomnote{scikit-learn \texttt{algorithm='auto'} selects the best method based on $n$ and $d$}
\end{frame}

% ============================================================
% SECTION 5: CLUSTER VALIDATION (Slides 29-31)
% ============================================================
\section{Cluster Validation}

% ============================================================
% SLIDE 29: Hopkins Statistic
% ============================================================
\begin{frame}[t]{Should We Cluster This Data at All?}
\textbf{Before Clustering}: Test whether data has cluster tendency at all.
\vspace{0.3em}

$$H = \frac{\sum_{i=1}^m u_i^d}{\sum_{i=1}^m u_i^d + \sum_{i=1}^m w_i^d}$$
\begin{itemize}
\item $u_i$ = distance from random point (uniform in data range) to nearest data point
\item $w_i$ = distance from randomly sampled data point to its nearest neighbor
\end{itemize}
\vspace{0.3em}
\textbf{Interpretation}:
\begin{itemize}
\item $H \approx 0.5$: uniform distribution (no clusters) -- do not cluster
\item $H > 0.75$: significant clustering tendency -- proceed with clustering
\end{itemize}
\bottomnote{Always run Hopkins test before K-Means -- clustering uniform data produces meaningless results}
\end{frame}

% ============================================================
% SLIDE 30: Gap Statistic
% ============================================================
\begin{frame}[t]{How Does the Gap Statistic Choose K?}
\textbf{Gap Statistic} (Tibshirani et al., 2001)
$$\text{Gap}_n(k) = E_n^*[\log W_k] - \log W_k$$
where $W_k = \sum_{r=1}^k \frac{1}{2|C_r|}\sum_{i,j \in C_r} \|x_i - x_j\|^2$ and $E_n^*$ is the expectation under a reference (uniform) distribution.
\vspace{0.3em}

\textbf{Selection Rule}: Choose smallest $K$ such that:
$$\text{Gap}(k) \geq \text{Gap}(k+1) - s_{k+1}$$
where $s_{k+1}$ is the standard error from $B$ bootstrap reference samples.
\vspace{0.3em}

\textbf{Advantage over Elbow}: Provides a statistical criterion rather than subjective visual inspection.
\bottomnote{Tibshirani et al. recommend $B = 20$--$50$ bootstrap samples for stable estimates}
\end{frame}

% ============================================================
% SLIDE 31: Comparison of K Selection Methods
% ============================================================
\begin{frame}[t]{Which K Selection Method Should We Use?}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Objectivity} & \textbf{Speed} & \textbf{Best For} \\
\midrule
Elbow (WCSS) & Low (subjective) & Fast & Quick exploration \\
Silhouette & High & Moderate & Cluster quality \\
Gap Statistic & High (statistical) & Slow & Rigorous analysis \\
\bottomrule
\end{tabular}
\end{center}
\vspace{0.5em}
\textbf{Practical Recommendation}:
\begin{itemize}
\item Start with elbow plot for quick overview
\item Validate with silhouette score and silhouette plots
\item Use Gap statistic when formal justification is needed (e.g., publications)
\end{itemize}
\bottomnote{No single method is perfect -- use multiple methods and check if they agree}
\end{frame}

% SLIDE: Elbow Method Visualization
\begin{frame}[t]{Where Is the Elbow in the WCSS Curve?}
\begin{center}
\includegraphics[width=0.55\textwidth]{04_elbow_method/chart.pdf}
\end{center}

\begin{compactlist}
  \item WCSS always decreases with more clusters -- the question is \textit{how fast}
  \item The ``elbow'' marks the point of diminishing returns
  \item Combine with silhouette analysis for a more objective decision
\end{compactlist}

\bottomnote{The elbow is often ambiguous -- that is why we also use silhouette and Gap statistics}
\end{frame}

% ============================================================
% SECTION 6: COMPARISON AND APPLICATIONS (Slides 32-36)
% ============================================================
\section{Comparison and Applications}

% ============================================================
% SLIDE 32: KNN vs K-Means: Key Differences
% ============================================================
\begin{frame}[t]{How Do KNN and K-Means Differ Fundamentally?}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{KNN} & \textbf{K-Means} \\
\midrule
Task & Classification / Regression & Clustering \\
Learning Type & Supervised (needs labels) & Unsupervised (no labels) \\
K meaning & Number of neighbors & Number of clusters \\
Training & None (lazy learner) & Iterative optimization \\
Prediction & Compute distances to all & Assign to nearest centroid \\
Output & Class label / value & Cluster assignment \\
Scalability & $O(nd)$ per query & $O(nKdT)$ total \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{The ``K'' in KNN and K-Means mean completely different things -- a common source of confusion!}
\end{frame}

% ============================================================
% SLIDE 33: Finance: RFM Customer Segmentation
% ============================================================
\begin{frame}[t]{How Do Banks Segment Customers with RFM?}
\textbf{RFM Analysis} (Industry Standard)
\begin{itemize}
\item \textbf{R}ecency: Days since last transaction
\item \textbf{F}requency: Number of transactions in period
\item \textbf{M}onetary: Total / average transaction value
\end{itemize}
\vspace{0.3em}
\textbf{Pipeline}: Standardize RFM $\to$ K-Means ($K$=4--6) $\to$ Profile segments
\vspace{0.3em}
\textbf{Typical Segments}:
\begin{itemize}
\item ``Champions'' (high R, F, M) $\to$ loyalty rewards
\item ``At Risk'' (low R, declining F) $\to$ retention campaigns
\item ``New Customers'' (high R, low F) $\to$ onboarding programs
\end{itemize}
\vspace{0.3em}
\textbf{Business Value}: Customer Lifetime Value (CLV) prediction per segment
\bottomnote{RFM segmentation is foundational for CRM, marketing analytics, and credit risk profiling}
\end{frame}

% ============================================================
% SLIDE 34: Finance: Fraud Detection
% ============================================================
\begin{frame}[t]{How Does KNN Detect Financial Fraud?}
\textbf{CRITICAL: Class Imbalance Problem}
\begin{itemize}
\item Fraud is typically $<$1\% of transactions (100:1 ratio)
\item Naive KNN majority vote: ALWAYS predicts non-fraud (99\% accuracy, zero fraud detected!)
\end{itemize}
\vspace{0.5em}
\textbf{Solutions for Imbalanced Data}
\begin{itemize}
\item \textbf{SMOTE}: Synthetic Minority Oversampling TEchnique
\item \textbf{Weighted KNN}: Higher weight for minority class neighbors
\item \textbf{Anomaly score}: Use distance to $K$-th neighbor instead of majority vote
\end{itemize}
\vspace{0.3em}
\textbf{Evaluation}: Use \textbf{Precision-Recall AUC}, NOT accuracy!
\bottomnote{Always check class distribution before applying majority vote -- accuracy is misleading for imbalanced data}
\end{frame}

% ============================================================
% SLIDE 35: Alternative Clustering: DBSCAN and Hierarchical
% ============================================================
\begin{frame}[t]{What If Clusters Aren't Spherical?}
\textbf{DBSCAN} (Density-Based Spatial Clustering):
\begin{itemize}
\item \textbf{Core point}: $\geq$ minPts neighbors within radius $\varepsilon$
\item \textbf{Border point}: within $\varepsilon$ of a core point but not itself core
\item \textbf{Noise}: neither core nor border $\Rightarrow$ automatic outlier detection
\item Does not require $K$; discovers non-spherical clusters
\end{itemize}
\vspace{0.3em}
\textbf{Hierarchical (Agglomerative)} Clustering:
\begin{itemize}
\item Bottom-up: each point starts as its own cluster, iteratively merge closest pairs
\item Linkage: single (min), complete (max), Ward (minimize variance)
\item \textbf{Dendrogram}: cut at desired height to get $K$ clusters
\end{itemize}
\bottomnote{DBSCAN for irregular shapes with outliers; hierarchical when exploring multiple K values via dendrogram}
\end{frame}

% ============================================================
% SLIDE 36: When to Use What
% ============================================================
\begin{frame}[t]{Which Algorithm Should You Choose?}
\textbf{Use KNN When}
\begin{itemize}
\item You have labeled training data (supervised)
\item Local patterns matter (non-linear boundaries)
\item Interpretability: ``classified because similar to these examples''
\end{itemize}
\vspace{0.3em}
\textbf{Use K-Means When}
\begin{itemize}
\item No labels available (unsupervised)
\item Looking for natural groupings in data
\item Clusters are roughly spherical and similar in size
\end{itemize}
\vspace{0.3em}
\textbf{Use Alternatives When}
\begin{itemize}
\item Non-spherical clusters $\to$ DBSCAN or spectral clustering
\item Soft assignments needed $\to$ Gaussian Mixture Models
\item Very large $n$ $\to$ Mini-Batch K-Means or approximate NN
\end{itemize}
\bottomnote{K-Means is often used as preprocessing (cluster features) before supervised learning}
\end{frame}

% SLIDE: Decision Flowchart
\begin{frame}[t]{Decision Flowchart: KNN vs K-Means vs Alternatives}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}

\begin{compactlist}
  \item Labels available $\to$ supervised methods (KNN, logistic regression, random forests)
  \item No labels, spherical clusters $\to$ K-Means with K-Means++ initialization
  \item Non-spherical or unknown K $\to$ DBSCAN or hierarchical clustering
\end{compactlist}

\bottomnote{Start simple (K-Means) and add complexity only when validation metrics demand it}
\end{frame}

% ============================================================
% SECTION 7: IMPLEMENTATION (Slides 37-39)
% ============================================================
\section{Implementation}

% ============================================================
% SLIDE 37: KNN in scikit-learn
% ============================================================
\begin{frame}[t]{How Do We Implement KNN in Python?}
\textbf{Classification}
\begin{itemize}
\item \texttt{from sklearn.neighbors import KNeighborsClassifier}
\item \texttt{knn = KNeighborsClassifier(n\_neighbors=5, weights='distance')}
\item \texttt{knn.fit(X\_train, y\_train)}
\item \texttt{y\_pred = knn.predict(X\_test)}
\end{itemize}
\vspace{0.5em}
\textbf{Key Parameters}
\begin{itemize}
\item \texttt{n\_neighbors}: K value (default 5)
\item \texttt{weights}: \texttt{'uniform'} or \texttt{'distance'}
\item \texttt{metric}: \texttt{'euclidean'}, \texttt{'manhattan'}, \texttt{'minkowski'}
\item \texttt{algorithm}: \texttt{'auto'}, \texttt{'ball\_tree'}, \texttt{'kd\_tree'}, \texttt{'brute'}
\end{itemize}
\bottomnote{Also available: \texttt{KNeighborsRegressor} for regression; \texttt{RadiusNeighborsClassifier} for radius-based}
\end{frame}

% ============================================================
% SLIDE 38: K-Means in scikit-learn
% ============================================================
\begin{frame}[t]{How Do We Implement K-Means in Python?}
\textbf{Basic Usage}
\begin{itemize}
\item \texttt{from sklearn.cluster import KMeans}
\item \texttt{km = KMeans(n\_clusters=3, init='k-means++', random\_state=42)}
\item \texttt{labels = km.fit\_predict(X)}
\item \texttt{centroids = km.cluster\_centers\_}
\end{itemize}
\vspace{0.5em}
\textbf{Key Parameters}
\begin{itemize}
\item \texttt{n\_clusters}: $K$ (required, no default)
\item \texttt{init}: \texttt{'k-means++'} (default) or \texttt{'random'}
\item \texttt{n\_init}: number of restarts (default 10; keeps best)
\item \texttt{max\_iter}: max iterations per run (default 300)
\end{itemize}
\bottomnote{\texttt{km.inertia\_} gives WCSS after fitting; \texttt{km.n\_iter\_} gives iterations until convergence}
\end{frame}

% ============================================================
% SLIDE 39: Pipeline: Scaling + Algorithm
% ============================================================
\begin{frame}[t]{Why Must Scaling Be Inside the Pipeline?}
\textbf{Why Pipelines?} Prevent data leakage -- scaling must be fit on training data only.
\vspace{0.3em}
\begin{itemize}
\item \texttt{from sklearn.pipeline import Pipeline}
\item \texttt{from sklearn.preprocessing import StandardScaler}
\item \texttt{pipe = Pipeline([('scaler', StandardScaler()),}
\item \texttt{\phantom{pipe = Pipeline([}('knn', KNeighborsClassifier())])}
\item \texttt{pipe.fit(X\_train, y\_train)}
\end{itemize}
\vspace{0.3em}
\textbf{GridSearchCV with Pipeline}
\begin{itemize}
\item \texttt{params = \{'knn\_\_n\_neighbors': [3,5,7,11],}
\item \texttt{\phantom{params = \{}'knn\_\_weights': ['uniform','distance']\}}
\item \texttt{grid = GridSearchCV(pipe, params, cv=5, scoring='f1')}
\end{itemize}
\bottomnote{Pipelines ensure scaling is part of cross-validation -- fitting scaler on full data before CV causes leakage}
\end{frame}

% ============================================================
% SECTION 8: PRACTICE + SUMMARY + CLOSING (Slides 40-42)
% ============================================================
\section{Practice}

% ============================================================
% SLIDE 40: Hands-On Exercise
% ============================================================
\begin{frame}[t]{Hands-On Exercise}
\textbf{Open the Colab Notebook}
\begin{itemize}
\item \textbf{Exercise 1}: Implement weighted KNN with distance weighting; compare uniform vs distance on fraud data
\item \textbf{Exercise 2}: Compare Gap statistic vs Elbow vs Silhouette for K selection on customer data
\item \textbf{Exercise 3}: Apply SMOTE + KNN pipeline for imbalanced fraud detection; evaluate with PR AUC
\end{itemize}
\vspace{1em}
\textbf{Link}: See course materials for Colab notebook
\bottomnote{All exercises use real-world-inspired financial datasets with class imbalance and mixed scales}
\end{frame}

\section{Summary}

% ============================================================
% SLIDE 41: Key Takeaways
% ============================================================
\begin{frame}[t]{What Should You Remember?}
\textbf{K-Nearest Neighbors}
\begin{itemize}
\item Instance-based lazy learner; Cover \& Hart bound: $R_{1\text{-NN}} \leq 2R^*$
\item Scale features, choose K via cross-validation, consider weighted voting
\end{itemize}
\vspace{0.3em}
\textbf{K-Means}
\begin{itemize}
\item Iterative assign-update with guaranteed convergence (local optimum)
\item K-Means++ for initialization; silhouette/Gap for K selection
\end{itemize}
\vspace{0.3em}
\textbf{Common Considerations}
\begin{itemize}
\item Feature scaling is critical for both methods
\item ``K'' means completely different things in each algorithm
\end{itemize}
\vspace{0.3em}
\textbf{Finance}: RFM segmentation (K-Means), fraud detection with SMOTE (KNN)
\bottomnote{Both are foundational algorithms: simple, interpretable, widely used across industries}
\end{frame}

% ============================================================
% SLIDE 42: Closing Comic
% ============================================================
\begin{frame}[t]{Until Next Time...}
\begin{center}
\textit{``Even K-Means would struggle to cluster the ways students misuse K-Means.''}\\[0.5em]
With KNN and K-Means, you can now classify the known and discover the unknown.
\end{center}
\vspace{1em}
\textbf{Next Session:} L04 -- Random Forests (from distance-based to tree-based methods)
\bottomnote{XKCD \#2731 callback -- clustering is easy, knowing when to cluster is the hard part}
\end{frame}

% ============================================================
% APPENDIX (8 slides: A1-A8)
% ============================================================
\appendix
\section*{Advanced Topics}

% ============================================================
% SLIDE A1: Appendix Divider
% ============================================================
\begin{frame}{}
\vfill
\begin{center}
\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
\usebeamerfont{title}Appendix: Advanced Topics and Proofs
\end{beamercolorbox}
\end{center}
\vfill
\end{frame}

% ============================================================
% SLIDE A2: Distance Metric Properties -- Triangle Inequality
% ============================================================
\begin{frame}[t]{Distance Metric Properties: Triangle Inequality}
\textbf{Statement}: For any metric $d$ and points $\mathbf{x}, \mathbf{y}, \mathbf{z}$:
$$d(\mathbf{x}, \mathbf{z}) \leq d(\mathbf{x}, \mathbf{y}) + d(\mathbf{y}, \mathbf{z})$$

\textbf{Proof for Euclidean} (via Cauchy-Schwarz):
\begin{enumerate}
\item $\|\mathbf{x} - \mathbf{z}\| = \|(\mathbf{x} - \mathbf{y}) + (\mathbf{y} - \mathbf{z})\|$
\item $\leq \|\mathbf{x} - \mathbf{y}\| + \|\mathbf{y} - \mathbf{z}\|$ \quad (by Minkowski inequality, itself a consequence of Cauchy-Schwarz)
\end{enumerate}
\vspace{0.3em}
\textbf{Why It Enables KD-Tree Pruning}:
\begin{itemize}
\item If current best distance is $d^*$ and node center is $\mathbf{c}$:
\item Prune subtree if $d(\mathbf{x}, \mathbf{c}) - r > d^*$ (where $r$ = node radius)
\item Triangle inequality guarantees no closer point can exist in that subtree
\end{itemize}
\bottomnote{Without triangle inequality ($p < 1$), spatial indexing structures cannot prune and brute force is required}
\end{frame}

% ============================================================
% SLIDE A3: K-Means Convergence: Formal Proof
% ============================================================
\begin{frame}[t]{K-Means Convergence: Formal Proof}
\textbf{Objective}: $J(C, \boldsymbol{\mu}) = \sum_{k=1}^{K}\sum_{\mathbf{x}_i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2$

\textbf{Block Coordinate Descent}:
\begin{enumerate}
\item \textbf{Fix $\boldsymbol{\mu}$, optimize $C$}: For each $\mathbf{x}_i$, $\frac{\partial J}{\partial C}$ is minimized by $C_k^* = \{\mathbf{x}_i : k = \arg\min_j \|\mathbf{x}_i - \boldsymbol{\mu}_j\|\}$ \quad $\Rightarrow J^{(t+1)} \leq J^{(t)}$
\item \textbf{Fix $C$, optimize $\boldsymbol{\mu}$}: $\frac{\partial J}{\partial \boldsymbol{\mu}_k} = -2\sum_{\mathbf{x}_i \in C_k}(\mathbf{x}_i - \boldsymbol{\mu}_k) = 0$ gives $\boldsymbol{\mu}_k^* = \frac{1}{|C_k|}\sum_{\mathbf{x}_i \in C_k}\mathbf{x}_i$ \quad $\Rightarrow J^{(t+1)} \leq J^{(t)}$
\end{enumerate}
\vspace{0.3em}
\textbf{Termination}: $J \geq 0$ and strictly non-increasing. The number of distinct partitions of $n$ points into $K$ groups is $\leq K^n$ (finite). Each partition visited at most once $\Rightarrow$ convergence in $\leq K^n$ steps (in practice, much fewer).
\bottomnote{Worst-case $K^n$ iterations is exponential but never observed in practice; typical convergence in 10--50 iterations}
\end{frame}

% ============================================================
% SLIDE A4: Empty Cluster Handling
% ============================================================
\begin{frame}[t]{Empty Cluster Handling Strategies}
\textbf{Problem}: During K-Means iteration, a cluster may lose all its points.

\textbf{Strategy 1: Farthest Point Reinitialization}
\begin{itemize}
\item Find the point farthest from its assigned centroid
\item Use it as the new centroid for the empty cluster
\item Rationale: poorly fit points are good candidates for new clusters
\end{itemize}
\vspace{0.3em}
\textbf{Strategy 2: Split Largest Cluster}
\begin{itemize}
\item Find cluster with highest WCSS, split it into two
\item Maintains total number of clusters at $K$
\end{itemize}
\vspace{0.3em}
\textbf{Strategy 3: Random Reinitialization}
\begin{itemize}
\item Randomly select a new data point as centroid
\item Simplest but least principled approach
\end{itemize}
\bottomnote{scikit-learn uses a reassignment strategy; K-Means++ initialization makes empty clusters rare}
\end{frame}

% ============================================================
% SLIDE A5: Cover & Hart: Full Proof
% ============================================================
\begin{frame}[t]{Cover \& Hart: Full Proof Details}
\textbf{Setup}: Binary classification, $\eta(\mathbf{x}) = P(Y=1|\mathbf{x})$, Bayes rule: $g^*(\mathbf{x}) = \mathbf{1}[\eta(\mathbf{x}) > 0.5]$.

\textbf{1-NN Error Rate Derivation}:
\begin{itemize}
\item As $n \to \infty$, nearest neighbor $\mathbf{x}_{(1)} \to \mathbf{x}$, so $\eta(\mathbf{x}_{(1)}) \to \eta(\mathbf{x})$
\item $P(\text{error} | \mathbf{x}) = P(Y \neq Y_{(1)} | \mathbf{x})$
\item $= \eta(\mathbf{x})(1-\eta(\mathbf{x})) + (1-\eta(\mathbf{x}))\eta(\mathbf{x}) = 2\eta(\mathbf{x})(1-\eta(\mathbf{x}))$
\end{itemize}
\vspace{0.3em}
\textbf{Bounding}:
$$R^*(\mathbf{x}) = \min(\eta, 1-\eta) \leq 2\eta(1-\eta) \leq 2\min(\eta,1-\eta) = 2R^*(\mathbf{x})$$
Integrating over $\mathbf{x}$: $R^* \leq R_{1\text{-NN}} \leq 2R^*(1-R^*)$
\vspace{0.3em}

\textbf{$K$-NN Extension}: With $K \to \infty$, $K/n \to 0$, majority vote converges to $\mathbf{1}[\eta > 0.5]$ by law of large numbers $\Rightarrow R_{K\text{-NN}} \to R^*$.
\bottomnote{The tighter bound $2R^*(1-R^*) \leq 2R^*$ shows 1-NN is at most 2$\times$ suboptimal}
\end{frame}

% ============================================================
% SLIDE A6: KNN Computational Complexity
% ============================================================
\begin{frame}[t]{KNN Computational Complexity}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Build} & \textbf{Query (avg)} & \textbf{Query (worst)} \\
\midrule
Brute Force & $O(1)$ & $O(nd)$ & $O(nd)$ \\
KD-Tree & $O(nd \log n)$ & $O(d \log n)$ & $O(nd)$ \\
Ball Tree & $O(nd \log n)$ & $O(d \log n)$ & $O(nd)$ \\
LSH (approx.) & $O(nd)$ & $O(d)$ & $O(nd)$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{0.3em}
\textbf{When Each Breaks Down}:
\begin{itemize}
\item KD-Tree: degrades to brute force for $d > 15$--$20$ (curse of dimensionality)
\item Ball Tree: better than KD-Tree in moderate-high $d$ but still degrades
\item LSH: approximate; tunable accuracy-speed tradeoff via hash functions
\end{itemize}
\bottomnote{Locality-Sensitive Hashing (LSH) is used by FAISS, Annoy, and other large-scale NN libraries}
\end{frame}

% ============================================================
% SLIDE A7: Gap Statistic: Mathematical Details
% ============================================================
\begin{frame}[t]{Gap Statistic: Mathematical Details}
\textbf{Reference Distribution}: Generate $B$ datasets from uniform distribution over bounding box of data (or from PCA-aligned box for better null model).

\textbf{Bootstrap Procedure}:
\begin{enumerate}
\item For each $k = 1, \ldots, K_{\max}$: compute $\log W_k$ on real data
\item For $b = 1, \ldots, B$: generate reference data, compute $\log W_k^{*(b)}$
\item $\text{Gap}(k) = \frac{1}{B}\sum_{b=1}^B \log W_k^{*(b)} - \log W_k$
\item $s_k = \text{sd}(\log W_k^{*(b)}) \cdot \sqrt{1 + 1/B}$
\end{enumerate}
\vspace{0.3em}
\textbf{Formal Selection Rule}: Choose smallest $k$ such that:
$$\text{Gap}(k) \geq \text{Gap}(k+1) - s_{k+1}$$

\textbf{Intuition}: Gap measures how much better the clustering is compared to clustering uniform (structureless) data.
\bottomnote{Tibshirani et al. (2001) recommend $B = 20$--$50$; the $\sqrt{1+1/B}$ correction accounts for simulation error}
\end{frame}

% ============================================================
% SLIDE A8: References
% ============================================================
\begin{frame}[t]{References}
\begin{multicols}{2}
\textbf{Textbooks}
\begin{itemize}
\item James et al. (2021). \textit{ISLR}, Ch.\ 2 (KNN), Ch.\ 12 (Clustering)
\item Hastie et al. (2009). \textit{ESL}, Ch.\ 13 (Prototypes/NN), Ch.\ 14 (Unsupervised)
\end{itemize}

\columnbreak

\textbf{Key Papers}
\begin{itemize}
\item Arthur \& Vassilvitskii (2007). K-Means++: The Advantages of Careful Seeding
\item Cover \& Hart (1967). Nearest Neighbor Pattern Classification
\item Tibshirani et al. (2001). Estimating the Number of Clusters via the Gap Statistic
\item Beyer et al. (1999). When Is ``Nearest Neighbor'' Meaningful?
\end{itemize}
\end{multicols}
\vspace{0.5em}
\textbf{Next Lecture}: L04 -- Random Forests (from distance-based to tree-based ensemble methods)
\bottomnote{All papers are available via university library access or open-access preprints}
\end{frame}

\end{document}
