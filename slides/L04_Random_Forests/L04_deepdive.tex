\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L04: Random Forests Deep Dive]{L04: Random Forests}
\subtitle{Deep Dive: Theory, Implementation, and Applications}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%% PART 1: DECISION TREES FOUNDATIONS
\section{Decision Tree Foundations}
\begin{frame}[t]{Part 1: Decision Tree Foundations}
\textbf{From Rules to Trees}
\begin{itemize}
\item Decision trees encode if-then-else rules
\item Each node splits data based on a feature threshold
\item Leaves contain predictions (class or value)
\end{itemize}
\vspace{0.5em}
\textbf{Key Questions}
\begin{itemize}
\item How to choose the best split?
\item When to stop splitting?
\item How to make predictions?
\end{itemize}
\bottomnote{Decision trees: the building blocks of Random Forests}
\end{frame}

\begin{frame}[t]{Splitting Criteria: Gini Impurity}
\textbf{Gini Impurity} measures class mixture at a node:
\[
G = 1 - \sum_{k=1}^{K} p_k^2
\]
where $p_k$ is the proportion of class $k$ samples.

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item $G = 0$: pure node (all samples same class)
\item $G = 0.5$: maximum for binary; general: $G_{max} = 1 - 1/K$ for $K$ classes
\item Lower Gini = better split
\end{itemize}
\bottomnote{Gini impurity: probability of misclassifying a random sample}
\end{frame}

\begin{frame}[t]{Splitting Criteria: Information Gain}
\textbf{Entropy} measures disorder:
\[
H = -\sum_{k=1}^{K} p_k \log_2(p_k)
\]
\textbf{Note}: By convention, $0 \cdot \log_2(0) = 0$ (via L'H\^opital's rule)

\textbf{Information Gain}:
\[
IG = H(\text{parent}) - \sum_{j} \frac{n_j}{n} H(\text{child}_j)
\]

\vspace{0.5em}
\textbf{Comparison:}
\begin{itemize}
\item Gini: faster to compute, tends to isolate most frequent class
\item Entropy: more balanced trees, slightly slower
\item In practice: similar performance
\end{itemize}
\bottomnote{Both criteria aim to create pure child nodes}
\end{frame}

\begin{frame}[t]{Regression Trees: MSE Criterion}
\textbf{For regression}, use Mean Squared Error:
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2
\]

\textbf{Split quality}:
\[
\text{Reduction} = \text{MSE}(\text{parent}) - \sum_{j} \frac{n_j}{n} \text{MSE}(\text{child}_j)
\]

\textbf{Leaf prediction}: mean of samples in leaf
\bottomnote{Trees can handle both classification and regression tasks}
\end{frame}

\begin{frame}[t]{Decision Tree Algorithm}
\textbf{Recursive Partitioning:}
\begin{enumerate}
\item Start with all samples at root
\item For each feature and threshold:
    \begin{itemize}
    \item Calculate impurity reduction
    \item Select split with maximum reduction
    \end{itemize}
\item Create child nodes with split samples
\item Recurse until stopping criterion met
\end{enumerate}

\textbf{Stopping Criteria:}
\begin{itemize}
\item Maximum depth reached
\item Minimum samples per leaf
\item No improvement in impurity
\end{itemize}
\bottomnote{Greedy is necessary: finding the globally optimal tree is NP-complete}
\end{frame}

\begin{frame}{CART: Recursive Partitioning Algorithm}
\textbf{Classification and Regression Trees} (Breiman et al., 1984):

\medskip
\begin{algorithmic}[1]
\REQUIRE node, dataset $\mathcal{D}$, depth
\IF{stopping criterion met}
  \RETURN leaf with prediction $\hat{y} = \text{majority}(\mathcal{D})$ or $\bar{y}$
\ENDIF
\FOR{each feature $j$ and threshold $s$}
  \STATE Compute impurity reduction: $\Delta I = I(\mathcal{D}) - \frac{|\mathcal{D}_L|}{|\mathcal{D}|} I(\mathcal{D}_L) - \frac{|\mathcal{D}_R|}{|\mathcal{D}|} I(\mathcal{D}_R)$
\ENDFOR
\STATE $(j^*, s^*) = \arg\max_{j,s} \Delta I$
\STATE Split $\mathcal{D}$ into $\mathcal{D}_L = \{x : x_j \leq s^*\}$ and $\mathcal{D}_R = \{x : x_j > s^*\}$
\STATE Recurse on $\mathcal{D}_L$ (left child, depth+1) and $\mathcal{D}_R$ (right child, depth+1)
\end{algorithmic}

\medskip
Impurity measures: Gini $= 1 - \sum_k p_k^2$, \; Entropy $= -\sum_k p_k \log_2 p_k$

\bottomnote{Breiman et al.\ (1984). \textit{Classification and Regression Trees}. Wadsworth.}
\end{frame}

\begin{frame}[t]{Decision Tree: Fraud Detection Example}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.65\textwidth]{01_decision_tree/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Each path through the tree represents a fraud detection rule}
\end{frame}

\begin{frame}[t]{CRITICAL: Class Imbalance in Fraud Detection}
\textbf{The Reality of Fraud Data}
\begin{itemize}
\item Fraud is typically $<$1\% of transactions (100:1 ratio)
\item A model predicting ``not fraud'' for ALL transactions: 99\% accuracy!
\item \textbf{Accuracy is the WRONG metric} for imbalanced classification
\end{itemize}
\vspace{0.5em}
\textbf{Correct Evaluation Metrics}
\begin{itemize}
\item \textbf{Precision}: Of flagged transactions, how many are fraud?
\item \textbf{Recall}: Of actual fraud, how many did we catch?
\item \textbf{F1 Score}: Harmonic mean of precision and recall
\item \textbf{AUC-PR}: Area under Precision-Recall curve (preferred over ROC)
\end{itemize}
\bottomnote{NEVER use accuracy for imbalanced classification!}
\end{frame}

\begin{frame}[t]{Handling Class Imbalance in Random Forests}
\textbf{1. Class Weighting} (scikit-learn)
\begin{itemize}
\item \texttt{class\_weight='balanced'}: auto-adjust by class frequency
\item \texttt{class\_weight='balanced\_subsample'}: per-tree balancing
\end{itemize}
\vspace{0.3em}
\textbf{2. Resampling Techniques}
\begin{itemize}
\item \textbf{SMOTE}: Synthetic Minority Oversampling TEchnique
\item \textbf{Undersampling}: Reduce majority class
\item \textbf{Combination}: SMOTE + Tomek links
\end{itemize}
\vspace{0.3em}
\textbf{3. Threshold Tuning}
\begin{itemize}
\item Default threshold 0.5 rarely optimal for imbalanced data
\item Choose threshold based on business cost: FN cost $>>$ FP cost for fraud
\end{itemize}
\bottomnote{Always check class distribution BEFORE training any classifier!}
\end{frame}

%% PART 2: ENSEMBLE METHODS
\section{Ensemble Methods}
\begin{frame}[t]{Part 2: Why Ensembles?}
\textbf{Problem with Single Trees:}
\begin{itemize}
\item High variance: small data changes $\rightarrow$ very different trees
\item Prone to overfitting
\item Unstable predictions
\end{itemize}

\vspace{0.5em}
\textbf{Ensemble Solution:}
\begin{itemize}
\item Train multiple diverse models
\item Combine predictions
\item Reduce variance while maintaining low bias
\end{itemize}
\bottomnote{``Wisdom of crowds'': aggregate many weak learners into strong learner}
\end{frame}

\begin{frame}[t]{Bootstrap Sampling}
\textbf{Bootstrap}: sample with replacement from original data

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item Each sample: same size as original ($n$ observations)
\item Expected unique samples: $\approx 63.2\%$ as $n \to \infty$ (converges to $1 - 1/e$)
\item Remaining $\sim 37\%$: out-of-bag (OOB) samples
\end{itemize}

\vspace{0.5em}
\textbf{Effect:}
\begin{itemize}
\item Each tree sees different data subset
\item Creates diversity among trees
\item OOB samples provide validation
\end{itemize}
\bottomnote{Bootstrap: key ingredient for reducing variance through aggregation}
\end{frame}

\begin{frame}[t]{Bagging Visualization}
\vspace{-1.3em}
\begin{center}
\includegraphics[width=0.60\textwidth]{03_bootstrap/chart.pdf}
\end{center}
\vspace{-1.0em}
\bottomnote{Bootstrap Aggregating: train on random samples, aggregate predictions}
\end{frame}

\begin{frame}[t]{Bagging: Mathematical Foundation}
\textbf{Variance Reduction by Averaging}

For $B$ independent predictions with variance $\sigma^2$:
\[
\text{Var}\left(\frac{1}{B}\sum_{b=1}^{B} \hat{f}_b(x)\right) = \frac{\sigma^2}{B}
\]

\textbf{With correlation $\rho$}:
\[
\text{Var} = \rho \sigma^2 + \frac{1-\rho}{B}\sigma^2
\]

\textbf{Derivation}: For correlated predictions with $\text{Cov}(f_i, f_j) = \rho\sigma^2$:
\[
\text{Var}(\bar{f}) = \frac{1}{B^2}\left[B\sigma^2 + B(B-1)\rho\sigma^2\right] = \rho\sigma^2 + \frac{(1-\rho)\sigma^2}{B}
\]

\textbf{Key insight}: Reduce correlation between trees to maximize variance reduction
\bottomnote{Lower correlation between trees = greater ensemble benefit}
\end{frame}

%% PART 3: RANDOM FORESTS
\section{Random Forests Algorithm}
\begin{frame}[t]{Part 3: Random Forests Algorithm}
\textbf{Two Sources of Randomness:}
\begin{enumerate}
\item \textbf{Bootstrap sampling}: each tree trained on random sample
\item \textbf{Feature randomization}: each split considers random subset
\end{enumerate}

\vspace{0.5em}
\textbf{Feature Subset Size} (at each split):
\begin{itemize}
\item Classification: $\sqrt{p}$ features (default)
\item Regression: $p/3$ features (default)
\item Decorrelates trees more than bagging alone
\end{itemize}
\bottomnote{Feature randomization: combines Ho's (1995) random subspace method with Breiman's bagging}
\end{frame}

\begin{frame}[t]{Random Forest Algorithm}
\small
\textbf{Training:}
\begin{enumerate}
\item For $b = 1$ to $B$ trees:
    \begin{itemize}
    \item Draw bootstrap sample of size $n$
    \item Grow tree:
        \begin{itemize}
        \item At each node, select $m$ features randomly (without replacement)
        \item Find best split among $m$ features
        \item Split until stopping criterion
        \end{itemize}
    \end{itemize}
\end{enumerate}

\textbf{Prediction:}
\begin{itemize}
\item Classification: majority vote across trees
\item Ties: broken randomly or by lowest class index
\item Regression: average predictions
\end{itemize}
\bottomnote{Typical: 100-500 trees; more trees improves accuracy (but increases memory/latency)}
\end{frame}

\begin{frame}[t]{Feature Importance}
\begin{center}
\includegraphics[width=0.65\textwidth]{02_feature_importance/chart.pdf}
\end{center}
\bottomnote{Mean Decrease in Impurity: sum of impurity reductions from splits on feature}
\end{frame}

\begin{frame}[t]{Feature Importance Methods}
\textbf{1. Mean Decrease in Impurity (MDI):}
\[
\text{MDI}(j) = \frac{1}{T}\sum_{t=1}^T \sum_{\text{node } v \text{ splits on } j} p(v) \cdot \Delta I(v)
\]
where $p(v)$ = fraction of samples reaching node $v$, $T$ = number of trees.
\begin{itemize}
\item Fast to compute (comes free from training)
\item Bias toward high-cardinality features
\end{itemize}

\vspace{0.5em}
\textbf{2. Permutation Importance:}
\begin{itemize}
\item Permute feature values, measure accuracy drop
\item More reliable, less biased
\item Slower (requires re-evaluation)
\end{itemize}
\bottomnote{Permutation importance preferred for final feature selection}
\end{frame}

\begin{frame}[t]{Statistical Inference for Feature Importance}
\textbf{Problem}: How significant is a feature's importance score?
\vspace{0.3em}

\textbf{Permutation Testing (Altmann et al., 2010)}
\begin{enumerate}
\item Compute actual importance $I_j$ for feature $j$
\item Permute feature $j$ labels, recompute importance ($B$ times)
\item P-value $= \frac{1 + \#(I_j^{perm} \geq I_j)}{1 + B}$
\end{enumerate}
\vspace{0.3em}
\textbf{Confidence Intervals}
\begin{itemize}
\item Bootstrap RF training to get importance distribution
\item Report 95\% CI: $[\hat{I}_j^{2.5\%}, \hat{I}_j^{97.5\%}]$
\item Features with CI crossing zero may not be significant
\end{itemize}
\bottomnote{Always report uncertainty in feature importance rankings}
\end{frame}

\begin{frame}{SHAP Values: Game-Theoretic Feature Importance}
\textbf{SHapley Additive exPlanations} (Lundberg \& Lee, 2017):

\[
\phi_j = \sum_{S \subseteq \mathcal{F} \setminus \{j\}} \frac{|S|!(|\mathcal{F}| - |S| - 1)!}{|\mathcal{F}|!} \left[f(S \cup \{j\}) - f(S)\right]
\]

\textbf{Properties} (unique among all feature attribution methods):
\begin{itemize}
\item \textbf{Efficiency}: $\sum_{j=1}^p \phi_j = f(x) - \mathbb{E}[f(X)]$ (attributions sum to prediction)
\item \textbf{Symmetry}: Equal contributors get equal attribution
\item \textbf{Dummy}: Irrelevant features get $\phi_j = 0$
\item \textbf{Additivity}: Consistent across model ensembles
\end{itemize}

\textbf{TreeSHAP}: $O(TLD^2)$ exact algorithm for tree ensembles vs.\ $O(2^p)$ brute force

\medskip
\textbf{Finance}: SHAP provides the \textit{reason codes} required by ECOA/GDPR for adverse action notices in automated lending decisions.

\bottomnote{Lundberg \& Lee (2017). A unified approach to interpreting model predictions. \textit{NeurIPS}, 4765--4774.}
\end{frame}

\begin{frame}[t]{Out-of-Bag Error}
\vspace{-1.3em}
\begin{center}
\includegraphics[width=0.50\textwidth]{04_oob_error/chart.pdf}
\end{center}
\vspace{-1.0em}
\bottomnote{OOB error: free cross-validation using samples not in bootstrap}
\end{frame}

\begin{frame}[t]{Out-of-Bag Error: How It Works}
\textbf{For each observation $i$:}
\begin{enumerate}
\item Identify trees where $i$ was OOB (not in bootstrap sample)
\item Aggregate predictions from only those trees
\item Compare to true label
\end{enumerate}

\vspace{0.5em}
\textbf{Benefits:}
\begin{itemize}
\item No separate validation set needed
\item Uses $\sim 37\%$ of trees per sample
\item Approximately unbiased estimate of generalization error
\end{itemize}

\vspace{0.5em}
\textbf{Why OOB Works:}
\begin{itemize}
\item Each sample is OOB for $\sim 36.8\%$ of trees (never used in their training)
\item Prediction uses only trees that did not see that sample
\item Approximates the \textbf{0.632 bootstrap estimator}, not leave-one-out CV
\end{itemize}
\bottomnote{OOB error approximates the 0.632 bootstrap estimator (Efron \& Tibshirani, 1997)}
\end{frame}

\begin{frame}[t]{Ensemble Voting}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.65\textwidth]{05_ensemble_voting/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Classification: majority vote. Regression: average prediction}
\end{frame}

%% PART 4: BIAS-VARIANCE AND TUNING
\section{Bias-Variance and Tuning}
\begin{frame}[t]{Part 4: Bias-Variance Decomposition}
\textbf{Expected Prediction Error:}
\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
\]

\textbf{Single Tree:}
\begin{itemize}
\item Low bias (can fit complex patterns)
\item High variance (sensitive to training data)
\end{itemize}

\textbf{Random Forest:}
\begin{itemize}
\item Bias: similar to single tree
\item Variance: reduced by $\approx$ factor of $1/B$ (with decorrelation)
\end{itemize}
\bottomnote{Ensembles reduce variance without increasing bias}
\end{frame}

\begin{frame}[t]{Single Trees: High Variance}
\begin{center}
\includegraphics[width=0.65\textwidth]{06a_single_tree_variance/chart.pdf}
\end{center}
\bottomnote{Individual trees overfit to their bootstrap samples, producing erratic predictions}
\end{frame}

\begin{frame}[t]{Random Forest: Variance Reduction}
\begin{center}
\includegraphics[width=0.65\textwidth]{06b_random_forest_variance/chart.pdf}
\end{center}
\bottomnote{Averaging decorrelated trees dramatically reduces prediction variance}
\end{frame}

\begin{frame}[t]{Hyperparameters: Number of Trees}
\textbf{n\_estimators} (number of trees):
\begin{itemize}
\item More trees = lower variance, never overfits (but higher resource usage)
\item Diminishing returns after 100-500 trees
\item Cost: linear increase in training/prediction time
\end{itemize}

\vspace{0.5em}
\textbf{Guidelines:}
\begin{itemize}
\item Start with 100, increase if OOB error still decreasing
\item For production: balance accuracy vs. latency
\item More trees always better (if time permits)
\end{itemize}
\bottomnote{Unlike most hyperparameters, more trees cannot hurt accuracy}
\end{frame}

\begin{frame}[t]{Hyperparameters: Tree Complexity}
\textbf{max\_depth}: Maximum tree depth
\begin{itemize}
\item Deeper = more complex patterns, higher variance
\item Default: unlimited (grow full trees)
\end{itemize}

\textbf{min\_samples\_split}: Minimum samples to split
\begin{itemize}
\item Higher = simpler trees, more regularization
\item Default: 2 (full trees)
\end{itemize}

\textbf{min\_samples\_leaf}: Minimum samples in leaf
\begin{itemize}
\item Higher = smoother predictions
\item Default: 1 (full trees)
\end{itemize}
\bottomnote{Full trees (default) often work well due to bagging's variance reduction}
\end{frame}

\begin{frame}[t]{Hyperparameters: Feature Randomization}
\textbf{max\_features}: Features considered at each split
\begin{itemize}
\item Lower = more decorrelated trees, higher bias
\item Higher = less decorrelated, lower bias
\end{itemize}

\vspace{0.5em}
\textbf{Defaults:}
\begin{itemize}
\item Classification: $\sqrt{p}$ (e.g., 10 features $\rightarrow$ 3)
\item Regression: $p/3$ (e.g., 30 features $\rightarrow$ 10)
\end{itemize}

\textbf{Tuning:}
\begin{itemize}
\item Try: $\sqrt{p}$, $\log_2(p)$, $p/3$
\item Cross-validate to find optimal
\end{itemize}
\bottomnote{Feature randomization is key differentiator from bagged trees}
\end{frame}

%% PART 5: PRACTICAL CONSIDERATIONS
\section{Practical Considerations}
\begin{frame}[t]{Part 5: Practical Considerations}
\textbf{Advantages:}
\begin{itemize}
\item Excellent accuracy out-of-the-box
\item Handles mixed feature types
\item Built-in feature importance
\item Robust to outliers and missing values
\item Parallelizable (trees independent)
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item Less interpretable than single tree
\item Memory intensive (stores all trees)
\item Slower prediction than linear models
\item Cannot extrapolate beyond training range
\end{itemize}
\bottomnote{Random Forests: excellent default choice for tabular data}
\end{frame}

\begin{frame}[t]{Comparison: Random Forest vs. Others}
\begin{center}
\small
\begin{tabular}{l|ccccc}
\toprule
\textbf{Aspect} & \textbf{RF} & \textbf{Single Tree} & \textbf{Logistic} & \textbf{KNN} \\
\midrule
Accuracy & High & Medium & Medium & Medium \\
Interpretability & Medium & High & High & Low \\
Training Speed & Medium & Fast & Fast & Fast \\
Feature Importance & Yes & Yes & Yes & No \\
Non-linear & Yes & Yes & No & Yes \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{RF trades some interpretability for significant accuracy gains}
\end{frame}

\begin{frame}[t]{When to Use Random Forests}
\textbf{Use When:}
\begin{itemize}
\item Tabular data with mixed feature types
\item Non-linear relationships expected
\item Feature importance needed
\item Out-of-the-box performance matters
\end{itemize}

\vspace{0.5em}
\textbf{Consider Alternatives When:}
\begin{itemize}
\item Need fully interpretable model (use single tree)
\item Very high-dimensional sparse data (use linear models)
\item Extrapolation required (use parametric models)
\item Need fastest prediction (use linear/shallow tree)
\end{itemize}
\bottomnote{Random Forest: often the first model to try on tabular data}
\end{frame}

\begin{frame}[t]{Decision Framework}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Start with Random Forest; switch if specific constraints require it}
\end{frame}

%% PART 6: BOOSTING
\section{From Bagging to Boosting}

\begin{frame}{From Bagging to Boosting: Weak Learners to Strong}
\begin{itemize}
\item Bagging reduces \textbf{variance} by averaging independent models
\item \textbf{Boosting} reduces \textbf{bias} by sequentially correcting errors
\item Core idea: train each model on the \textit{residuals} of the previous
\end{itemize}

\textbf{AdaBoost} (Freund \& Schapire, 1997):
\begin{enumerate}
\item Initialize weights $w_i = 1/N$ for all samples
\item For $t = 1, \ldots, T$:
\begin{itemize}
\item Fit weak learner $h_t$ to weighted data
\item Compute weighted error: $\epsilon_t = \sum_{i: h_t(x_i) \neq y_i} w_i$
\item Learner weight: $\alpha_t = \frac{1}{2}\ln\!\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$
\item Update: $w_i \leftarrow w_i \cdot \exp\!\bigl(-\alpha_t y_i h_t(x_i)\bigr)$, then normalize
\end{itemize}
\item Final: $H(x) = \text{sign}\!\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$
\end{enumerate}
\bottomnote{Freund \& Schapire (1997). A decision-theoretic generalization of on-line learning. \textit{JCSS}, 55(1), 119--139.}
\end{frame}

\begin{frame}{Gradient Boosting: A Functional Gradient Descent}
\textbf{Key insight}: Boosting as gradient descent in function space (Friedman, 2001)

\begin{enumerate}
\item Initialize $F_0(x) = \arg\min_c \sum_{i=1}^N L(y_i, c)$
\item For $m = 1, \ldots, M$:
\begin{itemize}
\item Compute pseudo-residuals: $r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}$
\item Fit base learner $h_m$ to pseudo-residuals $\{(x_i, r_{im})\}$
\item Line search: $\gamma_m = \arg\min_\gamma \sum_{i=1}^N L\!\bigl(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)\bigr)$
\item Update: $F_m(x) = F_{m-1}(x) + \nu \cdot \gamma_m h_m(x)$
\end{itemize}
\end{enumerate}

\textbf{Learning rate} $\nu \in (0, 1]$ provides regularization via \textbf{shrinkage}

\medskip
\textbf{For squared loss}: $r_{im} = y_i - F_{m-1}(x_i)$ (literal residuals!)

\bottomnote{Friedman (2001). Greedy function approximation: A gradient boosting machine. \textit{Annals of Statistics}, 29(5), 1189--1232.}
\end{frame}

\begin{frame}{XGBoost, LightGBM, CatBoost: Industrial Boosting}
\textbf{XGBoost} (Chen \& Guestrin, 2016) --- regularized objective:
\[
\mathcal{L}^{(t)} = \sum_{i=1}^N \left[g_i f_t(x_i) + \tfrac{1}{2} h_i f_t^2(x_i)\right] + \Omega(f_t)
\]
where $g_i = \partial_{\hat{y}^{(t-1)}} L(y_i, \hat{y}^{(t-1)})$, \; $h_i = \partial^2_{\hat{y}^{(t-1)}} L(y_i, \hat{y}^{(t-1)})$

\medskip
Regularization: $\Omega(f) = \gamma T + \tfrac{1}{2}\lambda \sum_{j=1}^T w_j^2$

\medskip
\textbf{Optimal split gain}:
\[
\text{Gain} = \frac{1}{2}\left[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}\right] - \gamma
\]

\medskip
\begin{tabular}{lll}
\textbf{LightGBM}: & Leaf-wise growth, histogram binning & (Ke et al., 2017) \\
\textbf{CatBoost}: & Ordered boosting, native categoricals & (Prokhorenkova et al., 2018)
\end{tabular}

\bottomnote{Chen \& Guestrin (2016). XGBoost: A scalable tree boosting system. \textit{KDD}, 785--794.}
\end{frame}

\begin{frame}{Boosting in Finance: The Dominant Paradigm}
\textbf{Credit scoring} --- XGBoost/LightGBM dominate competitions (Kaggle):
\begin{itemize}
\item Home Credit Default Risk (2018): Top 50 all used gradient boosting
\item Handles missing values natively (learned split direction)
\item Built-in monotonicity constraints for regulatory compliance
\end{itemize}

\medskip
\textbf{Fraud detection}:
\begin{itemize}
\item LightGBM: Fast enough for real-time scoring ($<$5ms per transaction)
\item Handles extreme class imbalance via \texttt{scale\_pos\_weight}
\end{itemize}

\medskip
\textbf{Random Forests vs.\ Boosting --- When to choose?}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Criterion} & \textbf{RF} & \textbf{Boosting} \\
\midrule
Bias-variance & Low variance & Low bias \\
Overfitting risk & Low & Higher (tune carefully) \\
Interpretability & Feature importance & SHAP values \\
Speed (inference) & Parallelizable & Sequential \\
Kaggle/industry & Baseline & State-of-the-art \\
\bottomrule
\end{tabular}
\end{center}
\bottomnote{See Bent\'ejac et al.\ (2021) for comprehensive RF vs.\ boosting comparison.}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Train a decision tree on credit data
    \item Exercise 2: Build a random forest and analyze feature importance
    \item Exercise 3: Tune hyperparameters with cross-validation
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/} See course materials for Colab notebook
\end{frame}

\section{Implementation}

\begin{frame}[t,fragile]{Implementation: Complete Example}
\small
\begin{verbatim}
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Initialize with key hyperparameters
rf = RandomForestClassifier(
    n_estimators=200,      # Number of trees
    max_features='sqrt',   # Features per split
    max_depth=None,        # Full trees (default)
    min_samples_leaf=1,    # Minimum samples in leaf
    class_weight='balanced', # Handle imbalanced data
    oob_score=True,        # Enable OOB error
    n_jobs=-1,             # Use all CPU cores
    random_state=42        # Reproducibility
)

# Train and evaluate
rf.fit(X_train, y_train)
print(f"OOB Score: {rf.oob_score_:.3f}")
print(f"CV Score: {cross_val_score(rf, X, y, cv=5).mean():.3f}")
\end{verbatim}
\bottomnote{Key: class\_weight='balanced' for imbalanced fraud detection}
\end{frame}

\section{Summary}

\begin{frame}[t]{Summary: Random Forests}
\textbf{Core Concepts:}
\begin{itemize}
\item Ensemble of decision trees with bootstrap + feature randomization
\item Reduces variance while maintaining low bias
\item OOB error provides free cross-validation
\item \textbf{Breiman's insight}: Low tree correlation + high tree strength = best generalization
\end{itemize}

\textbf{Practical Takeaways:}
\begin{itemize}
\item Excellent default for tabular data
\item Feature importance aids interpretation
\item More trees = higher accuracy (but more memory/latency)
\item Hyperparameter tuning often optional, but recommended for imbalanced data
\end{itemize}
\bottomnote{Next: PCA and t-SNE for dimensionality reduction}
\end{frame}

\section{References}

\begin{frame}[t]{References}
\small
\textbf{Textbooks:}
\begin{itemize}
\item James et al. (2021). \textit{ISLR}, Chapter 8: Tree-Based Methods
\item Hastie et al. (2009). \textit{ESL}, Chapter 15: Random Forests
\end{itemize}

\textbf{Original Papers:}
\begin{itemize}
\item Breiman, L. (2001). Random Forests. \textit{Machine Learning}, 45(1), 5-32.
\item Breiman, L. (1996). Bagging Predictors. \textit{Machine Learning}, 24(2), 123-140.
\item Friedman, J.H. (2001). Greedy function approximation. \textit{Annals of Statistics}, 29(5).
\item Chen \& Guestrin (2016). XGBoost. \textit{KDD}, 785--794.
\item Lundberg \& Lee (2017). SHAP. \textit{NeurIPS}, 4765--4774.
\end{itemize}
\bottomnote{Breiman's 2001 paper: one of the most cited in ML}
\end{frame}

\end{document}
