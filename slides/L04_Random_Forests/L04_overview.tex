\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L04: Random Forests]{L04: Random Forests}
\subtitle{Ensemble Learning for Robust Predictions}
\author{Methods and Algorithms}
\date{Spring 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Problem}

\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item \textbf{Derive} the variance reduction formula for bagging and analyze the role of tree correlation
\item \textbf{Evaluate} Random Forest vs.\ gradient boosting for a given prediction task using bias-variance tradeoff
\item \textbf{Analyze} feature importance using permutation importance, MDI, and SHAP values
\item \textbf{Critique} ensemble methods for regulatory compliance in financial applications
\end{enumerate}
\vspace{1em}
\textbf{Finance Application:} Fraud detection with interpretable feature importance
\bottomnote{Bloom's Level 4--5: Analyze, Evaluate, Critique --- MSc-level objectives}
\end{frame}

\begin{frame}[t]{The Business Problem}
\textbf{Fraud Detection Challenge}
\begin{itemize}
\item Need high accuracy: fraudulent transactions cost millions
\item Need interpretability: explain why transaction flagged
\item Complex patterns: fraud evolves and adapts
\end{itemize}
\vspace{0.5em}
\textbf{Why Random Forests?}
\begin{itemize}
\item Combines many trees for robust predictions
\item Built-in feature importance ranking
\item Handles non-linear relationships naturally
\end{itemize}
\bottomnote{Ensemble methods: ``wisdom of crowds'' for machine learning}
\end{frame}

\section{Method}

\begin{frame}[t]{Decision Tree Structure}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.65\textwidth]{01_decision_tree/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Trees split data using simple rules at each node until reaching a prediction}
\end{frame}

\begin{frame}[t]{Feature Importance}
\begin{center}
\includegraphics[width=0.65\textwidth]{02_feature_importance/chart.pdf}
\end{center}
\bottomnote{Random Forests automatically rank which features matter most for prediction}
\end{frame}

\begin{frame}[t]{Bootstrap Aggregating (Bagging)}
\vspace{-1.3em}
\begin{center}
\includegraphics[width=0.60\textwidth]{03_bootstrap/chart.pdf}
\end{center}
\vspace{-1.0em}
\bottomnote{Each tree trains on a random sample, reducing overfitting}
\end{frame}

\begin{frame}[t]{Key Equations: Random Forest Theory}
\textbf{Gini Impurity} (split criterion):
\[
G = 1 - \sum_{k=1}^K p_k^2
\]

\textbf{Bagging Variance Reduction} (with correlated trees):
\[
\text{Var}(\bar{f}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2
\]
where $\rho$ = pairwise tree correlation, $B$ = number of trees

\medskip
\textbf{Random Forest Decorrelation}: Feature subsampling with $m \approx \sqrt{p}$ features per split reduces $\rho$, amplifying the variance reduction effect.

\medskip
\textbf{OOB Error}: Each sample is excluded from $\approx 36.8\%$ of trees ($P(\text{not selected}) = (1-1/n)^n \to 1/e$). OOB predictions aggregate only from those excluded trees.
\bottomnote{These equations formalize why ensembles of decorrelated trees outperform single models}
\end{frame}

\begin{frame}[t]{Out-of-Bag Error}
\vspace{-1.2em}
\begin{center}
\includegraphics[width=0.50\textwidth]{04_oob_error/chart.pdf}
\end{center}
\vspace{-0.9em}
\bottomnote{OOB error provides free cross-validation without held-out test set}
\end{frame}

\begin{frame}[t]{Ensemble Voting}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.65\textwidth]{05_ensemble_voting/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Final prediction combines votes from all trees (majority for classification)}
\end{frame}

\section{Solution}

\begin{frame}[t]{Single Trees: High Variance}
\begin{center}
\includegraphics[width=0.65\textwidth]{06a_single_tree_variance/chart.pdf}
\end{center}
\bottomnote{Each tree trained on different bootstrap sample produces different predictions}
\end{frame}

\begin{frame}[t]{Random Forest: Reduced Variance}
\begin{center}
\includegraphics[width=0.65\textwidth]{06b_random_forest_variance/chart.pdf}
\end{center}
\bottomnote{Averaging many high-variance trees produces low-variance ensemble}
\end{frame}

\section{Practice}

\begin{frame}[t]{Hands-on Exercise}
  \textbf{Open the Colab Notebook}
  \begin{itemize}
    \item Exercise 1: Train a decision tree on credit data
    \item Exercise 2: Build a random forest and analyze feature importance
    \item Exercise 3: Tune hyperparameters with cross-validation
  \end{itemize}
  \vspace{1em}
  \textbf{Link:} \url{https://colab.research.google.com/} See course materials for Colab notebook
\end{frame}

\section{Decision Framework}

\begin{frame}[t]{Decision Framework}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.65\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Random Forests excel when accuracy and feature importance both matter}
\end{frame}

\section{Summary}

\begin{frame}[t]{References}
  \footnotesize
  \begin{itemize}
    \item Breiman, L. (2001). \textit{Random Forests}. Machine Learning, 45(1), 5-32.
    \item James et al. (2021). \textit{Introduction to Statistical Learning}. \url{https://www.statlearning.com/}
    \item Hastie et al. (2009). \textit{Elements of Statistical Learning}. \url{https://hastie.su.domains/ElemStatLearn/}
  \end{itemize}
\end{frame}

\end{document}
