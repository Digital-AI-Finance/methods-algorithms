\documentclass[8pt,aspectratio=169]{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{hyperref}

% Custom colors (ML palette)
\definecolor{MLPurple}{RGB}{51,51,178}
\definecolor{MLBlue}{RGB}{0,102,204}
\definecolor{MLOrange}{RGB}{255,127,14}
\definecolor{MLGreen}{RGB}{44,160,44}
\definecolor{MLRed}{RGB}{214,39,40}

% Apply colors
\setbeamercolor{structure}{fg=MLPurple}
\setbeamercolor{title}{fg=MLPurple}
\setbeamercolor{frametitle}{fg=MLPurple}

% Footer customization
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Custom commands
\newcommand{\bottomnote}[1]{\vfill\footnotesize\textit{#1}}
\newcommand{\highlight}[1]{\textcolor{MLOrange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[L04: Random Forests]{L04: Random Forests}
\subtitle{Ensemble Learning for Robust Predictions}
\author{Methods and Algorithms -- MSc Data Science}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will be able to:}
\begin{enumerate}
\item Explain how decision trees partition feature space
\item Implement Random Forests using bagging and feature randomization
\item Interpret feature importance and out-of-bag error
\item Apply ensemble methods to fraud detection problems
\end{enumerate}
\vspace{1em}
\textbf{Finance Application:} Fraud detection with interpretable feature importance
\bottomnote{From single models to ensemble methods that combine many weak learners}
\end{frame}

\begin{frame}[t]{The Business Problem}
\textbf{Fraud Detection Challenge}
\begin{itemize}
\item Need high accuracy: fraudulent transactions cost millions
\item Need interpretability: explain why transaction flagged
\item Complex patterns: fraud evolves and adapts
\end{itemize}
\vspace{0.5em}
\textbf{Why Random Forests?}
\begin{itemize}
\item Combines many trees for robust predictions
\item Built-in feature importance ranking
\item Handles non-linear relationships naturally
\end{itemize}
\bottomnote{Ensemble methods: ``wisdom of crowds'' for machine learning}
\end{frame}

\begin{frame}[t]{Decision Tree Structure}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.50\textwidth]{01_decision_tree/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Trees split data using simple rules at each node until reaching a prediction}
\end{frame}

\begin{frame}[t]{Feature Importance}
\begin{center}
\includegraphics[width=0.6\textwidth]{02_feature_importance/chart.pdf}
\end{center}
\bottomnote{Random Forests automatically rank which features matter most for prediction}
\end{frame}

\begin{frame}[t]{Bootstrap Aggregating (Bagging)}
\vspace{-1.1em}
\begin{center}
\includegraphics[width=0.47\textwidth]{03_bootstrap/chart.pdf}
\end{center}
\vspace{-0.8em}
\bottomnote{Each tree trains on a random sample, reducing overfitting}
\end{frame}

\begin{frame}[t]{Out-of-Bag Error}
\begin{center}
\includegraphics[width=0.6\textwidth]{04_oob_error/chart.pdf}
\end{center}
\bottomnote{OOB error provides free cross-validation without held-out test set}
\end{frame}

\begin{frame}[t]{Ensemble Voting}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.48\textwidth]{05_ensemble_voting/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Final prediction combines votes from all trees (majority for classification)}
\end{frame}

\begin{frame}[t]{Single Trees: High Variance}
\begin{center}
\includegraphics[width=0.6\textwidth]{06a_single_tree_variance/chart.pdf}
\end{center}
\bottomnote{Each tree trained on different bootstrap sample produces different predictions}
\end{frame}

\begin{frame}[t]{Random Forest: Reduced Variance}
\begin{center}
\includegraphics[width=0.6\textwidth]{06b_random_forest_variance/chart.pdf}
\end{center}
\bottomnote{Averaging many high-variance trees produces low-variance ensemble}
\end{frame}

\begin{frame}[t]{Decision Framework}
\vspace{-0.8em}
\begin{center}
\includegraphics[width=0.50\textwidth]{07_decision_flowchart/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Random Forests excel when accuracy and feature importance both matter}
\end{frame}

\end{document}
