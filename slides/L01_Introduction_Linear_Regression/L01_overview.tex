\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Compact list for dense slides
\newenvironment{compactlist}{%
  \begin{itemize}%
    \setlength{\itemsep}{2pt}%
    \setlength{\parskip}{0pt}%
    \setlength{\parsep}{0pt}%
}{%
  \end{itemize}%
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[Linear Regression]{Introduction \& Linear Regression}
\subtitle{Overview}
\author{Methods and Algorithms}
\institute{MSc Data Science}
\date{Spring 2026}

\begin{document}

% ============================================
% SLIDE 1: Title Page
% ============================================
\begin{frame}
  \titlepage
\end{frame}

% ============================================
% SLIDE 2: Outline
% ============================================
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% === ZONE 1: INTRODUCTION (NO formulas, NO Greek letters) ===
% ============================================
% SECTION: INTRODUCTION
% ============================================
\section{Introduction}

% ============================================
% SLIDE 3: What is Machine Learning? (Layout 8 -- Mixed Media)
% ============================================
\begin{frame}[t]{Can a Computer Learn to Predict Prices?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Situation:} Banks approve millions of mortgage applications each year. Every one requires an accurate property valuation.

\vspace{0.3em}
\textbf{Complication:} Manual appraisals are slow, expensive, and inconsistent. Two appraisers can disagree by 20\% on the same property.

\vspace{0.3em}
\textbf{Question:} Can we build a model that predicts property value from its characteristics---automatically and consistently?

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.45\textwidth]{images/1725_linear_regression.png}
\end{center}
\end{columns}

\bottomnote{XKCD \#1725 by Randall Munroe (CC BY-NC 2.5) -- This is the problem we solve today}
\end{frame}

% ============================================
% SLIDE 4: Learning Objectives
% ============================================
\begin{frame}[t]{Learning Objectives}
  By the end of this session, you will be able to:

  \begin{enumerate}
    \item \textbf{Derive} the best-fit line formula and explain why it minimizes prediction errors
    \item \textbf{Analyze} how a computer iteratively improves its predictions (gradient descent)
    \item \textbf{Evaluate} model quality using diagnostic plots and metrics ($R^2$, RMSE)
    \item \textbf{Compare} Ridge vs.\ Lasso regularization and select the right one for a given problem
  \end{enumerate}

  \vspace{0.5em}
  \textbf{Finance Applications:} Property valuation, asset pricing (CAPM)

  \bottomnote{Foundation for all supervised learning methods}
\end{frame}

% ============================================
% SLIDE 5: Supervised vs Unsupervised (Layout 10 -- Comparison)
% ============================================
\begin{frame}[t]{What Are the Two Types of Learning?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Supervised Learning}

\begin{itemize}
  \item We have the answers (labels) for training data
  \item Goal: predict the answer for new data
  \item Examples: predicting house prices, classifying emails
\end{itemize}

\column{0.48\textwidth}
\textbf{Unsupervised Learning}

\begin{itemize}
  \item No answers -- just data
  \item Goal: find hidden structure or groups
  \item Examples: customer segments, anomaly detection
\end{itemize}
\end{columns}

\bottomnote{This course covers both: L01--L04 supervised, L05 unsupervised}
\end{frame}

% ============================================
% SLIDE 5: What is Regression? (Layout 7 -- Full width text)
% ============================================
\begin{frame}[t]{What is Regression?}
\textbf{Predicting a Number (Not a Category)}

\begin{itemize}
  \item Predicting a number (not a category) -- for example, a house price
  \item Example: predicting house price from its size in square meters
  \item We draw the best line through the data points
  \item The line captures the relationship between input and output
\end{itemize}

\bottomnote{Regression = predicting a continuous value. Classification = predicting a category.}
\end{frame}

% ============================================
% SLIDE 6: The Simplest Model (Layout 4 -- Two columns math)
% ============================================
\begin{frame}[t]{How Do We Model a Relationship?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Idea}

Price goes up as size increases. We write this as:
$$y = a + b \times x$$

\begin{itemize}
  \item $y$ = price (what we predict)
  \item $x$ = size (what we know)
  \item $a$ = starting price, $b$ = price per sqm
\end{itemize}

\column{0.48\textwidth}
\textbf{Example}

$$\text{Price} = 50{,}000 + 200 \times \text{Size}$$

\begin{itemize}
  \item A 100 sqm apartment: $50{,}000 + 200 \times 100 = 70{,}000$
  \item A 150 sqm apartment: $50{,}000 + 200 \times 150 = 80{,}000$
  \item Each extra square meter adds 200 to the price
\end{itemize}
\end{columns}

\bottomnote{This is the entire idea of linear regression -- the rest is details}
\end{frame}

% ============================================
% SLIDE 7: What Makes a Good Prediction? (Layout 22 -- Chart + explanations)
% ============================================
\begin{frame}[t]{What Makes a Good Prediction?}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_residual_plots/chart.pdf}
\end{center}

\begin{itemize}
  \item The red lines show prediction errors (residuals)
  \item A good model has small, random errors
  \item Patterns in errors = something the model missed
\end{itemize}

\bottomnote{We want errors to be small and random -- not systematic}
\end{frame}

% ============================================
% SLIDE 8: How Does the Computer Learn? (Layout 11 -- Step-by-step)
% ============================================
\begin{frame}[t]{How Does the Computer Learn?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Learning Process}

\vspace{0.5em}
\textbf{Step 1:} Start with a random line

\vspace{0.5em}
\textbf{Step 2:} Measure how wrong it is (errors)

\vspace{0.5em}
\textbf{Step 3:} Adjust the line to reduce errors

\column{0.48\textwidth}
\textbf{Step 4:} Repeat until errors are small

\vspace{0.5em}
\textbf{Step 5:} Use the final line to predict

\vspace{0.5em}
This process is called \textbf{optimization} -- the computer tries thousands of lines and keeps the best one
\end{columns}

\bottomnote{The computer tries thousands of lines and keeps the best one}
\end{frame}

% ============================================
% SLIDE 9: Key Vocabulary (Layout 6 -- Three-way split)
% ============================================
\begin{frame}[t]{What Terms Do You Need to Know?}
\begin{columns}[T]
\column{0.31\textwidth}
\textbf{Data}

\begin{compactlist}
  \item Features (inputs): size, bedrooms, age
  \item Target (output): price
  \item Observation: one data point (one house)
\end{compactlist}

\column{0.31\textwidth}
\textbf{Learning}

\begin{compactlist}
  \item Training: fitting the model to data
  \item Coefficients: the numbers the model learns
  \item Loss: how wrong the model is
\end{compactlist}

\column{0.31\textwidth}
\textbf{Evaluation}

\begin{compactlist}
  \item Prediction: model output for new data
  \item Error: difference between prediction and truth
  \item Overfitting: memorizing instead of learning
\end{compactlist}
\end{columns}

\bottomnote{Master these terms -- they appear in every ML method we study}
\end{frame}

% ============================================
% SLIDE 10: Road Map for Today (Layout 7 -- Full width text)
% ============================================
\begin{frame}[t]{Road Map for Today}
Now that you know the basics, we go deeper:

\begin{enumerate}
  \item A real business problem (house prices for banks)
  \item The math behind finding the best line
  \item How to tell if our model is good
  \item How to prevent our model from being too complex
\end{enumerate}

\vspace{0.5em}
\textit{Don't worry if some formulas look unfamiliar --- we build up step by step, and the intuition matters more than memorizing equations.}

\bottomnote{Everything ahead builds on the intuition from this intro}
\end{frame}

% === ZONE 2: CORE CONTENT (PMSP framework -- formulas allowed) ===
% ============================================
% SECTION: PROBLEM
% ============================================
\section{Problem}

% ============================================
% SLIDE 11: The Business Problem (Layout 3 -- Two columns text)
% ============================================
\begin{frame}[t]{Why Do Banks Need Predictions?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Finance Use Case}

\begin{itemize}
  \item Banks need accurate property valuations for mortgages
  \item Insurance companies assess property risk
  \item Investors evaluate real estate portfolios
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Linear Regression?}

\begin{itemize}
  \item Interpretable coefficients (price per square meter)
  \item Fast, well-understood method
  \item Strong baseline for comparison
\end{itemize}
\end{columns}

\bottomnote{Linear regression: the ``hello world'' of machine learning}
\end{frame}

% ============================================
% SECTION: METHOD
% ============================================
\section{Method}

% ============================================
% SLIDE 12: Linear Regression Formula (Layout 9 -- Definition-Example)
% ============================================
\begin{frame}[t]{What Does the Model Look Like?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Model}

\begin{equation}
  y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon
\end{equation}

\begin{itemize}
  \item $y$: target variable (house price)
  \item $x_1, \ldots, x_p$: features (size, bedrooms, age)
  \item $\beta_0, \ldots, \beta_p$: coefficients to learn
  \item $\varepsilon$: random error term
\end{itemize}

\column{0.48\textwidth}
\textbf{Worked Example}

$$\text{Price} = 50{,}000 + 200 \times \text{Size} + 15{,}000 \times \text{Bed} - 1{,}000 \times \text{Age}$$

\begin{itemize}
  \item Base price: \$50,000
  \item Each sqm adds \$200
  \item Each bedroom adds \$15,000
  \item Each year of age subtracts \$1,000
\end{itemize}
\end{columns}

\bottomnote{Goal: Find coefficients that minimize prediction error}
\end{frame}

% ============================================
% SLIDE 13: The Optimization Problem
% ============================================
\begin{frame}[t]{How Do We Find the Best Coefficients?}
\textbf{Objective:} Minimize the sum of squared errors (SSE)

\begin{equation}
  \min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}

\textbf{Why Squared Errors?}
\begin{itemize}
  \item Penalizes large errors more than small errors
  \item Differentiable (enables gradient-based optimization)
  \item Leads to closed-form solution
\end{itemize}

\bottomnote{This defines ``ordinary least squares'' (OLS)}
\end{frame}

% ============================================
% SLIDE 14: Normal Equation vs Gradient Descent (Layout 10 -- Comparison)
% ============================================
\begin{frame}[t]{Two Ways to Solve: Which is Better?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Normal Equation}

\begin{equation}
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
\end{equation}

\begin{itemize}
  \item Exact, closed-form solution
  \item One computation -- no iterations
  \item Slow for very large datasets
\end{itemize}

\column{0.48\textwidth}
\textbf{Gradient Descent}

\begin{equation}
  \boldsymbol{\beta}_{t+1} = \boldsymbol{\beta}_t - \alpha \nabla L(\boldsymbol{\beta}_t)
\end{equation}

\begin{itemize}
  \item Scales to big data
  \item Memory efficient
  \item Requires tuning learning rate $\alpha$
\end{itemize}
\end{columns}

\bottomnote{Both methods yield the same solution for OLS}
\end{frame}

% ============================================
% SLIDE 15: Gradient Descent in Action (Layout 21 -- Full-size chart)
% ============================================
\begin{frame}[t]{How Does Gradient Descent Move?}
\begin{center}
\includegraphics[width=0.65\textwidth]{04_gradient_descent/chart.pdf}
\end{center}

\bottomnote{Iteratively update parameters in the direction of steepest descent}
\end{frame}

% ============================================
% SECTION: SOLUTION
% ============================================
\section{Solution}

% ============================================
% SLIDE 16: Interpreting Coefficients
% ============================================
\begin{frame}[t]{What Do the Numbers Mean?}
\textbf{Example:} House price model

\vspace{0.3em}
\begin{equation}
  \text{Price} = 50{,}000 + 200 \times \text{Size} + 15{,}000 \times \text{Bedrooms} - 1{,}000 \times \text{Age}
\end{equation}

\textbf{Interpretation:}
\begin{itemize}
  \item $\beta_0 = 50{,}000$: Base price (all features = 0)
  \item $\beta_1 = 200$: Each extra sqm adds \$200
  \item $\beta_2 = 15{,}000$: Each bedroom adds \$15,000
  \item $\beta_3 = -1{,}000$: Each year of age subtracts \$1,000
\end{itemize}

\bottomnote{Coefficients show marginal effect, holding others constant}
\end{frame}

% ============================================
% SLIDE 17: Model Evaluation
% ============================================
\begin{frame}[t]{How Good Is Our Model?}
\textbf{Key Metrics:}

\begin{itemize}
  \item \textbf{$R^2$ (Coefficient of Determination):} Variance explained
  \begin{equation}
    R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
  \end{equation}

  \item \textbf{RMSE (Root Mean Squared Error):} Prediction accuracy
  \begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}
  \end{equation}
\end{itemize}

\vspace{0.3em}
\textbf{In Practice:} $R^2 = 0.75$ means model explains 75\% of price variance

\bottomnote{Always evaluate on held-out test data}
\end{frame}

% ============================================
% SLIDE 18: Learning Curves (Layout 22 -- Chart + explanations)
% ============================================
\begin{frame}[t]{Is Our Model Learning or Memorizing?}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_learning_curves/chart.pdf}
\end{center}

\begin{itemize}
  \item Gap between train and test error = overfitting
  \item Curves converging = more data won't help
\end{itemize}

\bottomnote{Learning curves diagnose underfitting vs overfitting}
\end{frame}

% ============================================
% SECTION: DECISION FRAMEWORK
% ============================================
\section{Decision Framework}

% ============================================
% SLIDE 19: When to Use Linear Regression (Layout 18 -- Pros/Cons)
% ============================================
\begin{frame}[t]{When to Use Linear Regression}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Advantages}
\begin{itemize}
  \item[\textcolor{mlgreen}{+}] Interpretable coefficients
  \item[\textcolor{mlgreen}{+}] Fast training and prediction
  \item[\textcolor{mlgreen}{+}] Well-understood theory
  \item[\textcolor{mlgreen}{+}] Works as strong baseline
\end{itemize}

\column{0.48\textwidth}
\textbf{Disadvantages}
\begin{itemize}
  \item[\textcolor{mlorange}{--}] Assumes linear relationships
  \item[\textcolor{mlorange}{--}] Sensitive to outliers
  \item[\textcolor{mlorange}{--}] Limited flexibility
  \item[\textcolor{mlorange}{--}] Needs feature engineering for non-linear patterns
\end{itemize}
\end{columns}

\bottomnote{When in doubt, start with linear regression as your baseline}
\end{frame}

% ============================================
% SLIDE 20: The Danger of Overfitting (Layout 21 -- XKCD)
% ============================================
\begin{frame}[t]{The Danger of Overfitting}
\vspace{-2.0em}
\begin{center}
\includegraphics[width=0.32\textwidth]{images/2048_curve_fitting.png}
\end{center}
\vspace{-1.5em}
\bottomnote{XKCD \#2048 -- Simpler models often generalize better (CC BY-NC 2.5)}
\end{frame}

% ============================================
% SLIDE 21: Regularization Overview (Layout 10 + chart below)
% ============================================
\begin{frame}[t]{How Do We Prevent Overfitting?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Ridge (L2)}
\begin{itemize}
  \item Adds penalty on squared coefficients
  \item Shrinks all coefficients toward zero
  \item Never removes features entirely
\end{itemize}

\column{0.48\textwidth}
\textbf{Lasso (L1)}
\begin{itemize}
  \item Adds penalty on absolute coefficients
  \item Can set coefficients exactly to zero
  \item Automatic feature selection
\end{itemize}
\end{columns}

\vspace{0.3em}
\begin{center}
\includegraphics[width=0.50\textwidth]{06_regularization_comparison/chart.pdf}
\end{center}

\bottomnote{Regularization prevents overfitting by penalizing large coefficients}
\end{frame}

% ============================================
% SLIDE 22: Bias-Variance Tradeoff (Layout 22 -- Chart + explanations)
% ============================================
\begin{frame}[t]{Why Can't We Minimize Both Errors?}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_bias_variance/chart.pdf}
\end{center}

\begin{itemize}
  \item Bias: error from wrong assumptions (too simple)
  \item Variance: error from sensitivity to training data (too complex)
  \item Sweet spot: minimize total error
\end{itemize}

\bottomnote{Model complexity controls the tradeoff between bias and variance}
\end{frame}

% === ZONE 3: WRAP-UP ===
% ============================================
% SECTION: PRACTICE
% ============================================
\section{Practice}

% ============================================
% SLIDE 23: Hands-on Exercise
% ============================================
\begin{frame}[t]{Hands-on Exercise}
\textbf{Open the Colab Notebook}

\begin{itemize}
  \item Exercise 1: Implement OLS from scratch
  \item Exercise 2: Use scikit-learn LinearRegression
  \item Exercise 3: Compare with gradient descent
\end{itemize}

\vspace{1em}
\textbf{Link:} See course materials on GitHub

\bottomnote{Start with Exercise 2 if short on time}
\end{frame}

% ============================================
% SLIDE 24: A Word of Caution (Layout 21 -- XKCD)
% ============================================
\begin{frame}[t]{A Word of Caution}
\begin{center}
\includegraphics[width=0.50\textwidth]{images/605_extrapolating.png}
\end{center}

\bottomnote{XKCD \#605 by Randall Munroe (CC BY-NC 2.5) -- Extrapolation is dangerous!}
\end{frame}

% ============================================
% SLIDE 25: Decision Framework (Layout 21 -- Full-size chart)
% ============================================
\begin{frame}[t]{Which Method Should You Choose?}
\begin{center}
\includegraphics[width=0.55\textwidth]{08_decision_flowchart/chart.pdf}
\end{center}

\bottomnote{Use this flowchart to decide when linear regression is appropriate}
\end{frame}

% ============================================
% SECTION: SUMMARY
% ============================================
\section{Summary}

% ============================================
% SLIDE 26: Key Takeaways (Layout 13 -- Summary two columns)
% ============================================
\begin{frame}[t]{Key Takeaways}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Key Concepts}
\begin{compactlist}
  \item Linear regression predicts continuous outcomes
  \item OLS minimizes sum of squared errors
  \item Solve via normal equation or gradient descent
\end{compactlist}

\column{0.48\textwidth}
\textbf{Practical Guidance}
\begin{compactlist}
  \item Coefficients are directly interpretable
  \item Evaluate with $R^2$ and RMSE on test data
  \item Use regularization to prevent overfitting
\end{compactlist}
\end{columns}

\vspace{0.8em}
\textbf{Next:} Deep dive into mathematics and implementation

\vspace{0.5em}
\textbf{References:}
\begin{itemize}
  \footnotesize
  \item ISLR Chapter 3: Linear Regression
  \item ESL Chapter 3: Linear Methods for Regression
\end{itemize}

\bottomnote{Foundation for all supervised learning methods}
\end{frame}

\end{document}
