\documentclass[8pt,aspectratio=169]{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tikz}

% Custom colors (ML palette)
\definecolor{MLPurple}{RGB}{51,51,178}
\definecolor{MLBlue}{RGB}{0,102,204}
\definecolor{MLOrange}{RGB}{255,127,14}
\definecolor{MLGreen}{RGB}{44,160,44}
\definecolor{MLRed}{RGB}{214,39,40}

% Apply colors
\setbeamercolor{structure}{fg=MLPurple}
\setbeamercolor{title}{fg=MLPurple}
\setbeamercolor{frametitle}{fg=MLPurple}

% Footer customization
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Custom commands
\newcommand{\bottomnote}[1]{\vfill\footnotesize\textit{#1}}
\newcommand{\highlight}[1]{\textcolor{MLOrange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

% Title information
\title[Linear Regression]{Introduction \& Linear Regression}
\subtitle{Overview}
\author{Methods and Algorithms}
\institute{MSc Data Science}
\date{Spring 2026}

\begin{document}

% Slide 1: Title
\begin{frame}
  \titlepage
\end{frame}

% Slide 2: Learning Objectives
\begin{frame}[t]{Learning Objectives}
  By the end of this session, you will be able to:

  \begin{enumerate}
    \item \textbf{Understand} the ordinary least squares (OLS) framework
    \item \textbf{Apply} gradient descent for parameter optimization
    \item \textbf{Interpret} regression coefficients in business contexts
  \end{enumerate}

  \vspace{1em}
  \textbf{Finance Application:} House price prediction, factor models

  \bottomnote{Foundation for all supervised learning methods}
\end{frame}

% XKCD #1725: Linear Regression Humor
\begin{frame}[t]{The Art of Fitting Lines}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.55\textwidth]{images/1725_linear_regression.png}
\end{center}
\vspace{-0.5em}
\bottomnote{XKCD \#1725 by Randall Munroe (CC BY-NC 2.5) -- Always check if your data supports a linear fit!}
\end{frame}

% Slide 3: The Business Problem
\begin{frame}[t]{The Business Problem}
  \textbf{Finance Use Case: Predicting House Prices}

  \begin{itemize}
    \item Banks need accurate property valuations for mortgages
    \item Insurance companies assess property risk
    \item Investors evaluate real estate portfolios
  \end{itemize}

  \vspace{0.5em}
  \textbf{The Question:} Given features (size, location, age), what price?

  \vspace{0.5em}
  \textbf{Why Linear Regression?}
  \begin{itemize}
    \item Interpretable coefficients (price per square meter)
    \item Fast, well-understood method
    \item Strong baseline for comparison
  \end{itemize}

  \bottomnote{Linear regression: the ``hello world'' of machine learning}
\end{frame}

% Slide 4: What is Linear Regression?
\begin{frame}[t]{What is Linear Regression?}
  \textbf{Core Idea:} Model the relationship between inputs and a continuous output

  \vspace{0.5em}
  \begin{equation}
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon
  \end{equation}

  \begin{itemize}
    \item $y$: Target variable (house price)
    \item $x_1, \ldots, x_p$: Features (size, bedrooms, age)
    \item $\beta_0, \ldots, \beta_p$: Coefficients to learn
    \item $\varepsilon$: Random error term
  \end{itemize}

  \bottomnote{Goal: Find coefficients that minimize prediction error}
\end{frame}

% Slide: Simple Regression Visualization
\begin{frame}[t]{Simple Linear Regression}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.50\textwidth]{01_simple_regression/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{The best-fit line minimizes the sum of squared distances from points}
\end{frame}

% Slide: Multiple Regression Visualization
\begin{frame}[t]{Multiple Linear Regression}
\vspace{-1.2em}
\begin{center}
\includegraphics[width=0.42\textwidth]{02_multiple_regression_3d/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{With multiple features, we fit a hyperplane to minimize squared errors}
\end{frame}

% Slide 5: The Optimization Problem
\begin{frame}[t]{The Optimization Problem}
  \textbf{Objective:} Minimize the sum of squared errors (SSE)

  \begin{equation}
    \min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \min_{\boldsymbol{\beta}} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2
  \end{equation}

  \textbf{Why Squared Errors?}
  \begin{itemize}
    \item Penalizes large errors more than small errors
    \item Differentiable (enables gradient-based optimization)
    \item Leads to closed-form solution
  \end{itemize}

  \bottomnote{This objective function defines ``ordinary least squares'' (OLS)}
\end{frame}

% Slide 6: Two Solution Approaches
\begin{frame}[t]{Two Solution Approaches}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{1. Closed-Form (Normal Equation)}
      \begin{equation}
        \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
      \end{equation}

      \textcolor{MLGreen}{Pros:}
      \begin{itemize}
        \item Exact solution
        \item One computation
      \end{itemize}

      \textcolor{MLRed}{Cons:}
      \begin{itemize}
        \item Slow for large $p$
        \item Memory intensive
      \end{itemize}
    \end{column}
    \begin{column}{0.48\textwidth}
      \textbf{2. Gradient Descent}
      \begin{equation}
        \boldsymbol{\beta}_{t+1} = \boldsymbol{\beta}_t - \alpha \nabla L(\boldsymbol{\beta}_t)
      \end{equation}

      \textcolor{MLGreen}{Pros:}
      \begin{itemize}
        \item Scales to big data
        \item Memory efficient
      \end{itemize}

      \textcolor{MLRed}{Cons:}
      \begin{itemize}
        \item Requires tuning $\alpha$
        \item Iterative process
      \end{itemize}
    \end{column}
  \end{columns}

  \bottomnote{Both methods yield the same solution for OLS}
\end{frame}

% Slide: Gradient Descent Visualization
\begin{frame}[t]{Gradient Descent in Action}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.45\textwidth]{04_gradient_descent/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Iteratively update parameters in the direction of steepest descent}
\end{frame}

% Slide 7: Interpreting Coefficients
\begin{frame}[t]{Interpreting Coefficients}
  \textbf{Example:} House price model

  \vspace{0.3em}
  \begin{equation}
    \text{Price} = 50,000 + 200 \times \text{Size} + 15,000 \times \text{Bedrooms} - 1,000 \times \text{Age}
  \end{equation}

  \textbf{Interpretation:}
  \begin{itemize}
    \item $\beta_0 = 50,000$: Base price (all features = 0)
    \item $\beta_1 = 200$: Each extra sqm adds \$200
    \item $\beta_2 = 15,000$: Each bedroom adds \$15,000
    \item $\beta_3 = -1,000$: Each year of age subtracts \$1,000
  \end{itemize}

  \bottomnote{Coefficients show marginal effect, holding others constant}
\end{frame}

% Slide 8: Model Evaluation
\begin{frame}[t]{Model Evaluation}
  \textbf{Key Metrics:}

  \begin{itemize}
    \item \textbf{$R^2$ (Coefficient of Determination):} Variance explained
    \begin{equation}
      R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
    \end{equation}

    \item \textbf{RMSE (Root Mean Squared Error):} Prediction accuracy
    \begin{equation}
      \text{RMSE} = \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}
    \end{equation}
  \end{itemize}

  \vspace{0.3em}
  \textbf{In Practice:} $R^2 = 0.75$ means model explains 75\% of price variance

  \bottomnote{Always evaluate on held-out test data}
\end{frame}

% Slide: Residual Diagnostics
\begin{frame}[t]{Residual Diagnostics}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.50\textwidth]{03_residual_plots/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Good residuals are random; patterns suggest model problems}
\end{frame}

% Slide: Learning Curves
\begin{frame}[t]{Learning Curves}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.50\textwidth]{05_learning_curves/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Learning curves help diagnose underfitting vs overfitting}
\end{frame}

% Slide 9: Decision Framework
\begin{frame}[t]{When to Use Linear Regression}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{\textcolor{MLGreen}{Use When:}}
      \begin{itemize}
        \item Target is continuous
        \item Linear relationships expected
        \item Interpretability matters
        \item Fast inference needed
      \end{itemize}
    \end{column}
    \begin{column}{0.48\textwidth}
      \textbf{\textcolor{MLRed}{Avoid When:}}
      \begin{itemize}
        \item Target is categorical
        \item Strong non-linearities
        \item High multicollinearity
        \item Many outliers present
      \end{itemize}
    \end{column}
  \end{columns}

  \vspace{0.5em}
  \begin{table}
    \centering
    \small
    \begin{tabular}{ll}
      \toprule
      \textbf{Pros} & \textbf{Cons} \\
      \midrule
      Interpretable & Assumes linearity \\
      Fast training & Sensitive to outliers \\
      Well-understood & Limited flexibility \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}

% XKCD #2048: Overfitting Warning
\begin{frame}[t]{The Danger of Overfitting}
\vspace{-2.0em}
\begin{center}
\includegraphics[width=0.32\textwidth]{images/2048_curve_fitting.png}
\end{center}
\vspace{-1.5em}
\bottomnote{XKCD \#2048 -- Simpler models often generalize better (CC BY-NC 2.5)}
\end{frame}

% Slide: Regularization Comparison
\begin{frame}[t]{Regularization: Ridge vs Lasso}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.50\textwidth]{06_regularization_comparison/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Ridge shrinks all coefficients; Lasso can set some to exactly zero}
\end{frame}

% Slide: Bias-Variance Tradeoff
\begin{frame}[t]{Bias-Variance Tradeoff}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.50\textwidth]{07_bias_variance/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Model complexity controls the tradeoff between bias and variance}
\end{frame}

% XKCD #605: Extrapolation Warning
\begin{frame}[t]{A Word of Caution}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.50\textwidth]{images/605_extrapolating.png}
\end{center}
\vspace{-0.5em}
\bottomnote{XKCD \#605 by Randall Munroe (CC BY-NC 2.5) -- Extrapolation is dangerous!}
\end{frame}

% Slide: Decision Flowchart
\begin{frame}[t]{Decision Framework}
\vspace{-1.0em}
\begin{center}
\includegraphics[width=0.45\textwidth]{08_decision_flowchart/chart.pdf}
\end{center}
\vspace{-0.5em}
\bottomnote{Use this flowchart to decide when linear regression is appropriate}
\end{frame}

% Slide 10: Key Takeaways
\begin{frame}[t]{Key Takeaways}
  \begin{enumerate}
    \item Linear regression models continuous outcomes as weighted sum of features
    \item OLS minimizes sum of squared errors
    \item Solve via normal equation (small data) or gradient descent (big data)
    \item Coefficients are directly interpretable as marginal effects
    \item Evaluate using $R^2$ (variance explained) and RMSE (prediction error)
  \end{enumerate}

  \vspace{1em}
  \textbf{Next:} Deep dive into mathematics and implementation

  \vspace{0.5em}
  \textbf{References:}
  \begin{itemize}
    \footnotesize
    \item ISLR Chapter 3: Linear Regression
    \item ESL Chapter 3: Linear Methods for Regression
  \end{itemize}
\end{frame}

\end{document}
