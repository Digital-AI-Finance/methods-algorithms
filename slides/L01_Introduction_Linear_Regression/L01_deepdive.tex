\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Backward compatibility: uppercase color names
\colorlet{MLPurple}{mlpurple}
\colorlet{MLBlue}{mlblue}
\colorlet{MLOrange}{mlorange}
\colorlet{MLGreen}{mlgreen}
\colorlet{MLRed}{mlred}
\colorlet{MLLavender}{mllavender}
\colorlet{MLGray}{mlgray}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom course footer
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
      \usebeamerfont{author in head/foot}Methods and Algorithms
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}MSc Data Science
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    \end{beamercolorbox}}%
  \vskip0pt%
}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Compact list for dense slides
\newenvironment{compactlist}{%
  \begin{itemize}%
    \setlength{\itemsep}{2pt}%
    \setlength{\parskip}{0pt}%
    \setlength{\parsep}{0pt}%
}{%
  \end{itemize}%
}

% Custom commands for course compatibility
\newcommand{\highlight}[1]{\textcolor{mlorange}{\textbf{#1}}}
\newcommand{\mathbold}[1]{\boldsymbol{#1}}

\title[Linear Regression]{Introduction \& Linear Regression}
\subtitle{Deep Dive: Mathematics and Implementation}
\author{Methods and Algorithms}
\institute{MSc Data Science}
\date{Spring 2026}

\begin{document}

% ============================================
% SLIDE 1: TITLE PAGE
% ============================================
\begin{frame}
  \titlepage
\end{frame}

% ============================================
% SLIDE 2: OUTLINE
% ============================================
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% ============================================
% SLIDE 3: Opening Comic (XKCD #1725)
% ============================================
\begin{frame}[t]{The Art of Fitting Lines}
\begin{columns}[T]
\column{0.55\textwidth}
\small
\textbf{Today's Deep Dive}

\vspace{0.3em}
In the overview, we learned that linear regression fits the best line through data. Now we go deeper:

\begin{compactlist}
  \item How do we \textbf{derive} the solution mathematically?
  \item How do we \textbf{optimize} when data is too large for a closed-form solution?
  \item How do we \textbf{know} if our model is any good?
\end{compactlist}

\column{0.42\textwidth}
\begin{center}
\includegraphics[width=0.85\textwidth]{images/1725_linear_regression.png}
\end{center}
\end{columns}

\bottomnote{XKCD \#1725 by Randall Munroe (CC BY-NC 2.5) -- The math behind the ``best line''}
\end{frame}

% ============================================
% SLIDE 4: LEARNING OBJECTIVES
% ============================================
\begin{frame}[t]{Learning Objectives}
  By the end of this session, you will be able to:

  \begin{enumerate}
    \item \textbf{Derive} the OLS estimator and prove its optimality under Gauss-Markov assumptions
    \item \textbf{Analyze} gradient descent convergence and evaluate learning rate selection
    \item \textbf{Evaluate} regression diagnostics to identify assumption violations
    \item \textbf{Compare} regularization strategies (Ridge, Lasso, Elastic Net) for different problem structures
  \end{enumerate}

  \vspace{0.5em}
  \textbf{Finance Applications:} Property valuation, asset pricing (CAPM)

  \bottomnote{Foundation for all supervised learning methods}
\end{frame}

% ============================================
% SECTION: MATHEMATICAL FOUNDATIONS (6 slides)
% ============================================
\section{Mathematical Foundations}

% ============================================
% SLIDE 4: Matrix Notation (Layout 9 -- Definition-Example)
% ============================================
\begin{frame}[t]{How Do We Write This in Matrix Form?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

The linear model in matrix form:
\begin{equation}
  \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation}

\begin{itemize}
  \item $\mathbf{y} \in \mathbb{R}^n$: Response vector
  \item $\mathbf{X} \in \mathbb{R}^{n \times (p+1)}$: Design matrix
  \item $\boldsymbol{\beta} \in \mathbb{R}^{p+1}$: Coefficient vector
  \item $\boldsymbol{\varepsilon} \in \mathbb{R}^n$: Error vector
\end{itemize}

\column{0.48\textwidth}
\textbf{Design Matrix}

\begin{equation}
  \mathbf{X} = \begin{bmatrix}
    1 & x_{11} & \cdots & x_{1p} \\
    1 & x_{21} & \cdots & x_{2p} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & \cdots & x_{np}
  \end{bmatrix}
\end{equation}

\begin{itemize}
  \item First column of 1s for intercept $\beta_0$
  \item Each row: one observation
  \item Each column (after first): one feature
\end{itemize}
\end{columns}

\bottomnote{Matrix notation enables elegant derivations and efficient computation}
\end{frame}

% ============================================
% SLIDE 5: OLS Assumptions
% ============================================
\begin{frame}[t]{What Must Be True for OLS to Work?}
  \textbf{Classical Assumptions for Valid Inference}

  \begin{enumerate}
    \item \textbf{Linearity}: $E[y|X] = X\beta$ (correct functional form)
    \item \textbf{Exogeneity}: $E[\varepsilon|X] = 0$ (no omitted variable bias)
    \item \textbf{Homoscedasticity}: $\text{Var}(\varepsilon|X) = \sigma^2 I$ (constant variance)
    \item \textbf{Full rank}: $\text{rank}(X) = p+1$ (no perfect multicollinearity)
    \item \textbf{Normality} (for inference): $\varepsilon \sim N(0, \sigma^2 I)$
  \end{enumerate}

  \bottomnote{Assumptions 1--4 for unbiased estimates; 5 for t-tests and CIs}
\end{frame}

% ============================================
% SLIDE 6: The Loss Function
% ============================================
\begin{frame}[t]{What Are We Minimizing?}
  \textbf{Sum of Squared Residuals (SSR)}

  \begin{equation}
    L(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
  \end{equation}

  \textbf{Expanding:}
  \begin{equation}
    L(\boldsymbol{\beta}) = \mathbf{y}^\top\mathbf{y} - 2\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}
  \end{equation}

  \bottomnote{Quadratic in $\boldsymbol{\beta}$ -- has unique minimum if $\mathbf{X}$ is full rank}
\end{frame}

% ============================================
% SLIDE 7: Deriving the Normal Equation
% ============================================
\begin{frame}[t]{How Do We Solve for the Best Coefficients?}
  \textbf{Taking the Derivative}

  \begin{equation}
    \frac{\partial L}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^\top\mathbf{y} + 2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}
  \end{equation}

  \textbf{Setting to Zero:}
  \begin{align}
    \mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} &= \mathbf{X}^\top\mathbf{y} \\
    \hat{\boldsymbol{\beta}} &= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}
  \end{align}

  \bottomnote{This is the closed-form OLS solution}
\end{frame}

% ============================================
% SLIDE 8: Simple Regression Visualization (Layout 21)
% ============================================
\begin{frame}[t]{What Does the Best Fit Look Like?}
  \begin{center}
    \includegraphics[width=0.65\textwidth]{01_simple_regression/chart.pdf}
  \end{center}

  \bottomnote{The fitted line minimizes vertical distances squared}
\end{frame}

% ============================================
% SLIDE 9: Multiple Regression Surface (Layout 21)
% ============================================
\begin{frame}[t]{What Happens with Two Features?}
  \vspace{-2mm}
  \begin{center}
    \includegraphics[width=0.60\textwidth]{02_multiple_regression_3d/chart.pdf}
  \end{center}

  \bottomnote{With 2 features, we fit a plane; with $p$ features, a hyperplane}
\end{frame}

% ============================================
% SECTION: OPTIMIZATION (4 slides)
% ============================================
\section{Optimization}

% ============================================
% SLIDE 10: Why Gradient Descent? (Layout 10 -- Comparison)
% ============================================
\begin{frame}[t]{Why Gradient Descent?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{\textcolor{MLRed}{Normal Equation}}
\begin{itemize}
  \item Computing $(\mathbf{X}^\top\mathbf{X})^{-1}$ is $O(p^3)$
  \item Must store $p \times p$ matrix
  \item Exact solution but slow for large $p$
\end{itemize}

\column{0.48\textwidth}
\textbf{\textcolor{MLGreen}{Gradient Descent}}
\begin{itemize}
  \item Memory efficient: process one sample at a time
  \item Scales to big data (SGD)
  \item Generalizes to non-linear models
\end{itemize}
\end{columns}

\bottomnote{For $p > 10{,}000$, gradient descent usually faster}
\end{frame}

% ============================================
% SLIDE 11: The Gradient + Update Rule (Layout 4 -- Two columns math)
% ============================================
\begin{frame}[t]{How Does Each Step Work?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Gradient of the Loss}

\begin{equation}
  \nabla L(\boldsymbol{\beta}) = -2\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
\end{equation}

\begin{itemize}
  \item Gradient points in direction of steepest ascent
  \item We move \emph{opposite} to gradient
\end{itemize}

\column{0.48\textwidth}
\textbf{Update Rule}

\begin{equation}
  \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \alpha \nabla L
\end{equation}

\textbf{Algorithm:}
\begin{enumerate}
  \item Initialize $\boldsymbol{\beta}^{(0)}$ (zeros or random)
  \item Compute gradient $\nabla L$
  \item Update $\boldsymbol{\beta}$
  \item Repeat until convergence
\end{enumerate}
\end{columns}

\bottomnote{Convergence: $\|\boldsymbol{\beta}_{t+1} - \boldsymbol{\beta}_t\| < \varepsilon$ or max iterations}
\end{frame}

% ============================================
% SLIDE 12: Learning Rate and SGD (Layout 3 -- Two columns text)
% ============================================
\begin{frame}[t]{How Do We Choose the Step Size?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Learning Rate $\alpha$}

\begin{itemize}
  \item \textbf{Too small}: Slow convergence, many iterations
  \item \textbf{Too large}: Divergence, oscillation
  \item \textbf{Practical}: Start with $\alpha = 0.01$, use adaptive methods (Adam)
\end{itemize}

\column{0.48\textwidth}
\textbf{Mini-Batch SGD}

\begin{itemize}
  \item $m = 1$: Stochastic GD (noisy but fast)
  \item $m = n$: Batch GD (stable but slow)
  \item $m \in [32, 256]$: Mini-batch (good tradeoff)
\end{itemize}
\end{columns}

\bottomnote{SGD is how we train all modern ML models, not just linear regression}
\end{frame}

% ============================================
% SLIDE 13: Feature Scaling
% ============================================
\begin{frame}[t]{Why Must We Scale Features?}
  \textbf{Standardization: Zero Mean, Unit Variance}

  \begin{equation}
    z_j = \frac{x_j - \mu_j}{\sigma_j}
  \end{equation}

  \textbf{Why Scale Features?}
  \begin{enumerate}
    \item \textbf{Coefficient comparison}: After scaling, $|\beta_j|$ reflects relative importance
    \item \textbf{Gradient descent}: Converges faster with similar feature scales
    \item \textbf{Regularization}: Fair penalty across all features
  \end{enumerate}

  \bottomnote{Always standardize for regularization; optional for OLS if only predicting}
\end{frame}

% ============================================
% SLIDE: Gradient Descent Convergence (Layout 22 -- Chart + explanations)
% ============================================
\begin{frame}[t]{How Does Gradient Descent Converge?}
\begin{center}
\includegraphics[width=0.55\textwidth]{04_gradient_descent/chart.pdf}
\end{center}

\begin{compactlist}
  \item The loss surface is a paraboloid for OLS -- gradient descent follows the steepest path
  \item Learning rate controls step size; too large causes oscillation
\end{compactlist}

\bottomnote{For OLS, gradient descent always converges to the global minimum (convexity)}
\end{frame}

% ============================================
% SECTION: EVALUATION AND INFERENCE (4 slides)
% ============================================
\section{Evaluation and Inference}

% ============================================
% SLIDE 14: R-squared and Adjusted R-squared (Layout 4 -- Two columns math)
% ============================================
\begin{frame}[t]{How Much Variance Does Our Model Explain?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{$R^2$: Coefficient of Determination}

\begin{equation}
  R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
\end{equation}

\begin{itemize}
  \item Proportion of variance explained
  \item $R^2 = 0$: no better than mean
  \item $R^2 = 1$: perfect fit
  \item Always increases with more features
\end{itemize}

\column{0.48\textwidth}
\textbf{Adjusted $R^2$}

\begin{equation}
  R^2_{\text{adj}} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
\end{equation}

\begin{itemize}
  \item Penalizes for number of predictors $p$
  \item Can decrease with irrelevant features
  \item Better for model comparison
\end{itemize}
\end{columns}

\bottomnote{Use adjusted $R^2$ when comparing models with different numbers of features}
\end{frame}

% ============================================
% SLIDE 15: RMSE and MAE
% ============================================
\begin{frame}[t]{How Far Off Are Our Predictions?}
  \textbf{Error Metrics in Original Units}

  \begin{align}
    \text{RMSE} &= \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2} \\
    \text{MAE} &= \frac{1}{n}\sum|y_i - \hat{y}_i|
  \end{align}

  \textbf{Comparison:}
  \begin{itemize}
    \item RMSE: Penalizes large errors more (sensitive to outliers)
    \item MAE: More robust, easier to interpret
    \item Units: Same as target variable (e.g., dollars)
  \end{itemize}

  \bottomnote{Report both for comprehensive evaluation}
\end{frame}

% ============================================
% SLIDE: Residual Diagnostics (Layout 22 -- Chart + explanations)
% ============================================
\begin{frame}[t]{What Do the Residuals Tell Us?}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_residual_plots/chart.pdf}
\end{center}

\begin{compactlist}
  \item Random scatter = assumptions satisfied; patterns = model misspecification
  \item Funnel shape = heteroscedasticity; use robust standard errors
  \item Curvature = missing nonlinear terms; consider polynomial features
\end{compactlist}

\bottomnote{Residual plots are the single most important diagnostic -- always plot them first}
\end{frame}

% ============================================
% SLIDE: Learning Curves (Layout 22 -- Chart + explanations)
% ============================================
\begin{frame}[t]{What Do Learning Curves Tell Us?}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_learning_curves/chart.pdf}
\end{center}

\begin{compactlist}
  \item Training error increases with more data (harder to memorize)
  \item Test error decreases then plateaus (diminishing returns from more data)
  \item The gap between curves reveals overfitting vs underfitting
\end{compactlist}

\bottomnote{Learning curves are a diagnostic tool -- they tell you whether to get more data or a better model}
\end{frame}

% ============================================
% SLIDE 16: Hypothesis Testing and CIs (Layout 4 -- Two columns math)
% ============================================
\begin{frame}[t]{Which Coefficients Are Statistically Significant?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{t-Test for Coefficient $\beta_j$}

Standard error:
\begin{equation}
  \text{SE}(\hat{\beta}_j) = \hat{\sigma}\sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}
\end{equation}

Test statistic:
\begin{equation}
  t_j = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \sim t_{n-p-1}
\end{equation}

Reject $H_0\!: \beta_j = 0$ if p-value $< 0.05$

\column{0.48\textwidth}
\textbf{95\% Confidence Interval}

\begin{equation}
  \hat{\beta}_j \pm t_{n-p-1,\, 0.975} \times \text{SE}(\hat{\beta}_j)
\end{equation}

\begin{itemize}
  \item 95\% of intervals contain true $\beta_j$
  \item CI excludes 0 $\Leftrightarrow$ significant at 5\%
  \item Prediction intervals are wider (include $\sigma^2$)
\end{itemize}
\end{columns}

\bottomnote{Always check significance before interpreting coefficients}
\end{frame}

% ============================================
% SLIDE 17: F-Test for Overall Significance
% ============================================
\begin{frame}[t]{Does the Model as a Whole Explain Anything?}
  \textbf{Null hypothesis}: $H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0$

  \begin{equation}
    F = \frac{R^2 / p}{(1 - R^2) / (n - p - 1)} \sim F_{p,\, n-p-1}
  \end{equation}

  \begin{itemize}
    \item Tests whether the model \textbf{as a whole} explains significant variance
    \item Reject $H_0$ if p-value $< \alpha$
    \item Complements t-tests: possible to have no significant t-tests but a significant F-test (multicollinearity)
  \end{itemize}

  \bottomnote{Check the F-statistic before interpreting individual coefficients}
\end{frame}

% ============================================
% SECTION: REGULARIZATION AND MODEL SELECTION (5 slides)
% ============================================
\section{Regularization and Model Selection}

% ============================================
% SLIDE 18: The Overfitting Problem
% ============================================
\begin{frame}[t]{What Happens When We Have Too Many Features?}
  \textbf{When Models Memorize Instead of Learn}

  \begin{itemize}
    \item High-dimensional data ($p \approx n$ or $p > n$)
    \item Coefficients become very large
    \item Perfect fit on training data, poor generalization
  \end{itemize}

  \vspace{0.5em}
  \textbf{Solution: Add Penalty to Loss Function}
  \begin{equation}
    L_{\text{reg}}(\boldsymbol{\beta}) = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \cdot \text{penalty}(\boldsymbol{\beta})
  \end{equation}

  \bottomnote{$\lambda$ controls strength of regularization}
\end{frame}

% ============================================
% SLIDE 19: Ridge, Lasso, and Elastic Net (Layout 6 -- Three-way split)
% ============================================
\begin{frame}[t]{Which Penalty Should We Choose?}
\begin{columns}[T]
\column{0.30\textwidth}
\textbf{Ridge (L2)}

$$L + \lambda\|\boldsymbol{\beta}\|_2^2$$

\begin{compactlist}
  \item Closed-form solution
  \item Shrinks all toward zero
  \item Never exactly zero
\end{compactlist}

\column{0.30\textwidth}
\textbf{Lasso (L1)}

$$L + \lambda\|\boldsymbol{\beta}\|_1$$

\begin{compactlist}
  \item No closed-form
  \item Sparse solutions
  \item Automatic feature selection
\end{compactlist}

\column{0.30\textwidth}
\textbf{Elastic Net}

$$L + \lambda\bigl(\alpha\|\boldsymbol{\beta}\|_1 + \tfrac{1-\alpha}{2}\|\boldsymbol{\beta}\|_2^2\bigr)$$

\begin{compactlist}
  \item Mixing parameter $\alpha$
  \item Handles correlated features
  \item Best of both worlds
\end{compactlist}
\end{columns}

\bottomnote{Ridge for many small effects, Lasso for feature selection, Elastic Net for correlated features}
\end{frame}

% ============================================
% SLIDE: Regularization Paths (Layout 21 -- Full-size chart)
% ============================================
\begin{frame}[t]{How Does Regularization Shrink Coefficients?}
\begin{center}
\includegraphics[width=0.55\textwidth]{06_regularization_comparison/chart.pdf}
\end{center}

\begin{compactlist}
  \item Ridge shrinks all coefficients toward zero but never reaches it
  \item Lasso drives some coefficients exactly to zero (feature selection)
  \item Larger $\lambda$ = more shrinkage = simpler model
\end{compactlist}

\bottomnote{The regularization path shows how coefficients change as $\lambda$ increases}
\end{frame}

% ============================================
% SLIDE 20: Choosing Lambda (Layout 11 -- Step-by-step)
% ============================================
\begin{frame}[t]{How Do We Tune the Regularization Strength?}
  \textbf{Cross-Validation for Hyperparameter Tuning}

  \begin{enumerate}
    \item \textbf{Define grid} of $\lambda$ values (e.g., $10^{-4}$ to $10^{4}$, log-spaced)
    \item \textbf{K-fold CV}: For each $\lambda$, compute average validation error
    \item \textbf{Select} $\lambda$ with lowest CV error
    \item \textbf{Refit} on full training data with chosen $\lambda$
  \end{enumerate}

  \vspace{0.5em}
  \textbf{In Practice:}
  \begin{itemize}
    \item \texttt{sklearn.linear\_model.RidgeCV}
    \item \texttt{sklearn.linear\_model.LassoCV}
  \end{itemize}

  \bottomnote{Larger $\lambda$ = more regularization = simpler model}
\end{frame}

% ============================================
% SLIDE 21: Bias-Variance Decomposition
% ============================================
\begin{frame}[t]{Why Can't We Eliminate All Error?}
  \textbf{Expected Prediction Error}

  \begin{equation}
    E[(y - \hat{f}(x))^2] = \text{Bias}^2(\hat{f}) + \text{Var}(\hat{f}) + \sigma^2
  \end{equation}

  \begin{itemize}
    \item \textbf{Bias}: Error from wrong assumptions (underfitting)
    \item \textbf{Variance}: Error from sensitivity to training data (overfitting)
    \item \textbf{$\sigma^2$}: Irreducible noise in data
  \end{itemize}

  \bottomnote{We cannot reduce irreducible error -- focus on bias and variance}
\end{frame}

% ============================================
% SLIDE: Bias-Variance Visualization (Layout 21 -- Full-size chart)
% ============================================
\begin{frame}[t]{Where Is the Sweet Spot?}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_bias_variance/chart.pdf}
\end{center}

\begin{compactlist}
  \item Total error = Bias$^2$ + Variance + Irreducible noise
  \item The minimum total error occurs at intermediate model complexity
  \item Regularization controls where we sit on this curve
\end{compactlist}

\bottomnote{The optimal model balances underfitting (high bias) and overfitting (high variance)}
\end{frame}

% ============================================
% SLIDE 22: VIF and Multicollinearity (Layout 9 -- Definition-Example)
% ============================================
\begin{frame}[t]{How Do We Detect Correlated Features?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Variance Inflation Factor}

\begin{equation}
  \text{VIF}_j = \frac{1}{1 - R^2_j}
\end{equation}

where $R^2_j$ is from regressing $x_j$ on all other features.

\begin{compactlist}
  \item VIF $= 1$: No correlation
  \item VIF $> 5$: Moderate concern
  \item VIF $> 10$: Serious multicollinearity
\end{compactlist}

\column{0.48\textwidth}
\textbf{Remedies}

\begin{compactlist}
  \item Remove highly correlated features
  \item Use Ridge regression ($\lambda > 0$ stabilizes)
  \item Apply PCA before regression
\end{compactlist}

\vspace{0.5em}
\textbf{Why it matters:}
\begin{compactlist}
  \item Inflated standard errors
  \item Unstable coefficient estimates
  \item Misleading significance tests
\end{compactlist}
\end{columns}

\bottomnote{Always check VIF before trusting coefficient estimates}
\end{frame}

% ============================================
% SECTION: GENERALIZATION (1 slide)
% ============================================
\section{Generalization}

% ============================================
% SLIDE 23: Train-Test Split and CV (Layout 10 -- Comparison)
% ============================================
\begin{frame}[t]{How Do We Know the Model Generalizes?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Simple Split}
\begin{itemize}
  \item 70--80\% train, 20--30\% test
  \item Report test set metrics
  \item Fast but high variance estimate
\end{itemize}

\column{0.48\textwidth}
\textbf{K-Fold Cross-Validation}
\begin{itemize}
  \item Split into $K$ folds (typically 5 or 10)
  \item Train on $K{-}1$ folds, validate on 1
  \item More reliable with limited data
\end{itemize}
\end{columns}

\bottomnote{Always evaluate on data the model has never seen}
\end{frame}

% ============================================
% SLIDE: Decision Flowchart (Layout 21 -- Full-size chart)
% ============================================
\begin{frame}[t]{When Should You Use Linear Regression?}
\begin{center}
\includegraphics[width=0.55\textwidth]{08_decision_flowchart/chart.pdf}
\end{center}

\begin{compactlist}
  \item Start with linear regression as baseline before trying complex models
  \item If assumptions hold and relationships are approximately linear, OLS is hard to beat
  \item If $p > n$, use regularization; if nonlinear patterns, consider tree-based methods
\end{compactlist}

\bottomnote{The simplest model that meets your accuracy threshold is usually the best choice}
\end{frame}

% ============================================
% SECTION: SUMMARY (2 slides)
% ============================================
\section{Summary}

% ============================================
% SLIDE 24: Key Equations Summary (Layout 12 -- Formula Reference)
% ============================================
\begin{frame}[t]{Key Equations Summary}
\small
\begin{columns}[T]
\column{0.30\textwidth}
\textbf{Model}

$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}$$

\column{0.30\textwidth}
\textbf{Optimization}

$$\nabla L = -2\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$
$$\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \alpha \nabla L$$

\column{0.30\textwidth}
\textbf{Regularization}

$$\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y}$$
$$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$
\end{columns}

\bottomnote{Keep this slide as a reference sheet}
\end{frame}

% ============================================
% SLIDE 25: Key Takeaways
% ============================================
\begin{frame}[t]{Key Takeaways}
  \begin{enumerate}
    \item Linear regression minimizes squared error -- closed form or gradient descent
    \item Matrix notation enables efficient computation and elegant derivations
    \item Gradient descent scales to large datasets where normal equations fail
    \item Regularization (Ridge/Lasso/Elastic Net) prevents overfitting
    \item The bias-variance tradeoff guides model complexity decisions
  \end{enumerate}

  \vspace{0.5em}
  \textbf{Next Session:} Logistic Regression for Classification

  \bottomnote{Foundation for all supervised learning -- next: Logistic Regression}
\end{frame}

% ============================================
% SLIDE 26: Closing Comic Callback
% ============================================
\begin{frame}[t]{Until Next Time...}
\begin{center}
\large
\textit{``I used to think fitting a line was trivial.}

\vspace{0.5em}
\textit{Then I learned about heteroscedasticity, multicollinearity, regularization, and the bias-variance tradeoff.}

\vspace{0.5em}
\textit{Now I think fitting a line is an art.''}

\vspace{0.8em}
\normalsize
--- Adapted from XKCD \#1725
\end{center}

\bottomnote{XKCD \#1725 callback -- Next session: Logistic Regression turns this line into a curve}
\end{frame}

% ============================================
% APPENDIX
% ============================================
\appendix

\section*{Advanced Topics}

% ============================================
% APPENDIX SLIDE 1: Section Divider
% ============================================
\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Appendix: Advanced Topics and Proofs\par
\vspace{0.3em}
\normalsize These slides are supplementary material for self-study
\end{beamercolorbox}
\vfill
\end{frame}

% ============================================
% APPENDIX SLIDE 2: CAPM
% ============================================
\begin{frame}[t]{Finance Application: The CAPM}
  \textbf{Capital Asset Pricing Model -- Linear Regression in Finance}

  \begin{equation}
    R_i - R_f = \alpha_i + \beta_i (R_m - R_f) + \varepsilon_i
  \end{equation}

  \begin{itemize}
    \item $R_i$: Return of asset $i$
    \item $R_f$: Risk-free rate (e.g., T-bill)
    \item $R_m$: Market return (e.g., S\&P 500)
    \item $\beta_i$: Systematic risk (market sensitivity)
  \end{itemize}

  \vspace{0.3em}
  \textbf{Interpretation:} $\beta = 1.2$ means 10\% market rise $\Rightarrow$ 12\% expected asset rise

  \bottomnote{CAPM: The original factor model -- basis for portfolio management}
\end{frame}

% ============================================
% APPENDIX SLIDE 3: Gauss-Markov Theorem
% ============================================
\begin{frame}[t]{The Gauss-Markov Theorem}
  \textbf{Why OLS is Special}

  \vspace{0.5em}
  Under assumptions 1--4 (linearity, exogeneity, homoscedasticity, full rank):

  \begin{center}
    \textbf{OLS is BLUE} -- Best Linear Unbiased Estimator
  \end{center}

  \begin{itemize}
    \item \textbf{Best}: Lowest variance among all linear unbiased estimators
    \item \textbf{Linear}: Estimator is a linear function of $\mathbf{y}$
    \item \textbf{Unbiased}: $E[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}$
  \end{itemize}

  \bottomnote{Gauss-Markov justifies why OLS is the default for linear regression}
\end{frame}

% ============================================
% APPENDIX SLIDE 4: Gauss-Markov Proof Sketch
% ============================================
\begin{frame}[t]{Gauss-Markov: Proof Sketch}
  \textbf{Goal}: Show OLS $\hat{\boldsymbol{\beta}}$ has minimum variance among all linear unbiased estimators.

  \textbf{Proof}: Let $\tilde{\boldsymbol{\beta}} = \mathbf{Cy}$ be any other linear unbiased estimator. Write $\mathbf{C} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' + \mathbf{D}$ for some matrix $\mathbf{D}$.

  \begin{itemize}
    \item Unbiasedness requires $E[\tilde{\boldsymbol{\beta}}] = \boldsymbol{\beta}$, which forces $\mathbf{DX} = \mathbf{0}$
    \item Then: $\text{Var}(\tilde{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}'\mathbf{X})^{-1} + \sigma^2\mathbf{DD}'$
    \item Since $\mathbf{DD}' \succeq \mathbf{0}$:
  \end{itemize}

  \begin{equation}
    \text{Var}(\tilde{\boldsymbol{\beta}}) - \text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2 \mathbf{DD}' \succeq \mathbf{0}
  \end{equation}

  Therefore OLS has the smallest variance among all linear unbiased estimators. $\square$

  \bottomnote{Any deviation from OLS adds noise without reducing bias}
\end{frame}

% ============================================
% APPENDIX SLIDE 5: OLS and Maximum Likelihood
% ============================================
\begin{frame}[t]{OLS and Maximum Likelihood Estimation}
  Under normality $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$, the likelihood is:

  \begin{equation}
    L(\boldsymbol{\beta}, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(y_i - \mathbf{x}_i'\boldsymbol{\beta})^2}{2\sigma^2}\right)
  \end{equation}

  Maximizing the log-likelihood:

  \begin{itemize}
    \item $\frac{\partial \ell}{\partial \boldsymbol{\beta}} = 0$ gives the \textbf{same normal equations as OLS}
    \item $\hat{\sigma}^2_{\text{MLE}} = \frac{\text{RSS}}{n}$ (biased; OLS uses $n - p - 1$)
    \item This connection enables likelihood ratio tests, AIC/BIC model comparison
  \end{itemize}

  \textbf{Key insight}: OLS $\equiv$ MLE under normality $\Rightarrow$ all MLE properties transfer to OLS.

  \bottomnote{OLS = MLE under normality -- bridges to Logistic Regression (L02)}
\end{frame}

% ============================================
% APPENDIX SLIDE 6: Formal Diagnostic Tests
% ============================================
\begin{frame}[t]{Formal Assumption Diagnostic Tests}
\begin{tabular}{lll}
\toprule
\textbf{Assumption} & \textbf{Test} & \textbf{$H_0$} \\
\midrule
Homoscedasticity & Breusch-Pagan & Constant variance \\
 & White test & Constant variance (robust) \\
No autocorrelation & Durbin-Watson & No serial correlation \\
Normality & Shapiro-Wilk & Residuals are normal \\
 & Jarque-Bera & Skewness=0, kurtosis=3 \\
Functional form & Ramsey RESET & No omitted nonlinearities \\
\bottomrule
\end{tabular}

\vspace{0.3cm}
\textbf{When assumptions fail:}
\begin{itemize}
  \item Heteroscedasticity $\Rightarrow$ White/HC robust standard errors
  \item Autocorrelation $\Rightarrow$ Newey-West standard errors, GLS
  \item Non-normality $\Rightarrow$ Bootstrap inference (large $n$: CLT helps)
\end{itemize}

\bottomnote{Regulators require formal test results -- ``the residuals looked fine'' is insufficient}
\end{frame}

% ============================================
% APPENDIX SLIDE 7: Hat Matrix and Cook's Distance
% ============================================
\begin{frame}[t]{Hat Matrix and Cook's Distance}
  The \textbf{hat matrix} maps observed to fitted values: $\hat{\mathbf{y}} = \mathbf{Hy}$ where
  \begin{equation}
    \mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'
  \end{equation}

  \textbf{Leverage}: $h_{ii} \in [1/n, 1]$ measures how far $\mathbf{x}_i$ is from the center of the design space. High leverage ($h_{ii} > 2p/n$): observation \emph{could} be influential.

  \textbf{Cook's Distance} combines leverage and residual:
  \begin{equation}
    D_i = \frac{e_i^2}{p \cdot \text{MSE}} \cdot \frac{h_{ii}}{(1 - h_{ii})^2}
  \end{equation}

  \begin{itemize}
    \item $D_i > 1$: observation substantially changes $\hat{\boldsymbol{\beta}}$ when removed
    \item Critical in finance: a single crisis observation can distort the entire model
  \end{itemize}

  \bottomnote{Investigate points with $D_i > 4/n$ or leverage $h_{ii} > 2(p+1)/n$}
\end{frame}

% ============================================
% APPENDIX SLIDE 8: Convergence Theory
% ============================================
\begin{frame}[t]{Convergence Theory}
  \textbf{Convergence Rates for Gradient Descent}

  \begin{itemize}
    \item \textbf{Convex}: $O(1/t)$ convergence rate (sub-linear)
    \item \textbf{Strongly convex}: $O(\rho^t)$ where $\rho < 1$ (linear rate)
  \end{itemize}

  \vspace{0.5em}
  \textbf{Learning Rate Condition:}
  \begin{equation}
    \eta < \frac{2}{L}
  \end{equation}
  where $L$ is the Lipschitz constant of $\nabla L$.

  \vspace{0.3em}
  \textbf{For OLS specifically}, the optimal learning rate is:
  \begin{equation}
    \eta^* = \frac{1}{\lambda_{\max}(\mathbf{X}^\top\mathbf{X})}
  \end{equation}

  \bottomnote{For OLS, optimal $\eta = 1/\lambda_{\max}(\mathbf{X}^\top\mathbf{X})$}
\end{frame}

% ============================================
% APPENDIX SLIDE 9: References (Layout 20 -- Two columns)
% ============================================
\begin{frame}[t]{References and Resources}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Textbooks}
\footnotesize
\begin{itemize}
  \item James, Witten, Hastie, Tibshirani (2021). \textit{Introduction to Statistical Learning}. Ch.~3.
  \item Hastie, Tibshirani, Friedman (2009). \textit{Elements of Statistical Learning}. Ch.~3.
  \item Bishop (2006). \textit{Pattern Recognition and Machine Learning}. Ch.~3.
\end{itemize}

\column{0.48\textwidth}
\textbf{Online Resources}
\footnotesize
\begin{itemize}
  \item scikit-learn: \url{https://scikit-learn.org/stable/modules/linear_model.html}
  \item Stanford CS229: \url{https://cs229.stanford.edu/}
\end{itemize}
\end{columns}

\bottomnote{See also Andrew Ng's CS229 lectures for implementation-focused treatment}
\end{frame}

\end{document}
